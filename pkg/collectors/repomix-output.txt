This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2024-12-16T02:30:39.236Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Repository Structure
================================================================
cluster.go
collector.go
cronJob.go
daemonsets.go
hpa.go
job.go
namespace.go
networking.go
node.go
persistentvolume.go
persistentvolumeclaim.go
pod.go
replicaset.go
replicationController..go
storageclass.go
workload.go

================================================================
Repository Files
================================================================

================
File: cluster.go
================
// Copyright 2024 Vega Cloud, Inc.
//
// Use of this software is governed by the Business Source License
// included in the file licenses/BSL.txt.
//
// As of the Change Date specified in that file, in accordance with
// the Business Source License, use of this software will be governed
// by the Apache License, Version 2.0, included in the file
// licenses/APL.txt.

// Package collectors hosts the collection functions
package collectors

import (
	"context"
	"fmt"

	"github.com/sirupsen/logrus"
	v1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/client-go/kubernetes"

	"github.com/vegacloud/kubernetes/metricsagent/pkg/config"
	"github.com/vegacloud/kubernetes/metricsagent/pkg/models"
	"github.com/vegacloud/kubernetes/metricsagent/pkg/utils"
)

// ClusterCollector collects cluster-wide metrics
type ClusterCollector struct {
	clientset *kubernetes.Clientset
	config    *config.Config
	k8sConfig utils.K8sClientConfig
}

// NewClusterCollector creates a new cluster collector
func NewClusterCollector(clientset *kubernetes.Clientset, cfg *config.Config, k8sConfig utils.K8sClientConfig) *ClusterCollector {
	return &ClusterCollector{
		clientset: clientset,
		config:    cfg,
		k8sConfig: k8sConfig,
	}
}

// CollectMetrics collects cluster-wide metrics
func (cc *ClusterCollector) CollectMetrics(ctx context.Context) (interface{}, error) {
	nodes, err := cc.clientset.CoreV1().Nodes().List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to list nodes: %w", err)
	}
	logrus.Debugf("Successfully listed %d nodes", len(nodes.Items))

	pods, err := cc.clientset.CoreV1().Pods("").List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to list pods: %w", err)
	}
	logrus.Debugf("Successfully listed %d pods", len(pods.Items))

	metrics := &models.ClusterMetrics{
		KubernetesVersion: cc.k8sConfig.ClusterVersion,
		NodeCount:         len(nodes.Items),
		PodCount:          len(pods.Items),
		ContainerCount:    cc.countContainers(pods.Items),
		NodeLabels:        make(map[string]map[string]string),
		PodLabels:         make(map[string]map[string]string),
	}

	metrics.TotalCapacity = cc.calculateTotalCapacity(nodes.Items)
	metrics.TotalAllocatable = cc.calculateTotalAllocatable(nodes.Items)
	metrics.TotalRequests = cc.calculateTotalRequests(pods.Items)
	metrics.TotalLimits = cc.calculateTotalLimits(pods.Items)

	// Collect labels for nodes
	for _, node := range nodes.Items {
		metrics.NodeLabels[node.Name] = node.Labels
	}

	// Collect labels for pods
	for _, pod := range pods.Items {
		podKey := fmt.Sprintf("%s/%s", pod.Namespace, pod.Name)
		metrics.PodLabels[podKey] = pod.Labels
	}

	return metrics, nil
}

func (cc *ClusterCollector) calculateTotalCapacity(nodes []v1.Node) models.ResourceMetrics {
	total := models.ResourceMetrics{}
	for _, node := range nodes {
		total.CPU += node.Status.Capacity.Cpu().MilliValue()
		total.Memory += node.Status.Capacity.Memory().Value()
		total.Storage += node.Status.Allocatable.Storage().Value()
		if ephemeral, ok := node.Status.Allocatable[v1.ResourceEphemeralStorage]; ok {
			total.EphemeralStorage += ephemeral.Value()
		}
		total.Pods += node.Status.Allocatable.Pods().Value()

		// Collect GPU metrics if available
		if gpuQuantity, ok := node.Status.Capacity["nvidia.com/gpu"]; ok {
			gpuMetric := models.GPUMetrics{
				DeviceID:    fmt.Sprintf("node-%s-gpu", node.Name),
				MemoryTotal: uint64(gpuQuantity.Value() * 1024 * 1024 * 1024), // Convert to bytes
			}
			total.GPUDevices = append(total.GPUDevices, gpuMetric)
		}
	}
	logrus.Debugf("Calculated total capacity: CPU=%d, Memory=%d, Storage=%d, EphemeralStorage=%d, Pods=%d, GPUs=%d",
		total.CPU, total.Memory, total.Storage, total.EphemeralStorage, total.Pods, len(total.GPUDevices))
	return total
}

func (cc *ClusterCollector) calculateTotalAllocatable(nodes []v1.Node) models.ResourceMetrics {
	total := models.ResourceMetrics{}
	for _, node := range nodes {
		total.CPU += node.Status.Allocatable.Cpu().MilliValue()
		total.Memory += node.Status.Allocatable.Memory().Value()
		total.Storage += node.Status.Allocatable.Storage().Value()
		if ephemeral, ok := node.Status.Allocatable[v1.ResourceEphemeralStorage]; ok {
			total.EphemeralStorage += ephemeral.Value()
		}
		total.Pods += node.Status.Allocatable.Pods().Value()

		// Collect GPU metrics if available
		if gpuQuantity, ok := node.Status.Allocatable["nvidia.com/gpu"]; ok {
			gpuMetric := models.GPUMetrics{
				DeviceID:    fmt.Sprintf("node-%s-gpu", node.Name),
				MemoryTotal: uint64(gpuQuantity.Value() * 1024 * 1024 * 1024), // Convert to bytes
			}
			total.GPUDevices = append(total.GPUDevices, gpuMetric)
		}
	}
	logrus.Debugf("Calculated total allocatable: CPU=%d, Memory=%d, Storage=%d, EphemeralStorage=%d, Pods=%d, GPUs=%d",
		total.CPU, total.Memory, total.Storage, total.EphemeralStorage, total.Pods, len(total.GPUDevices))
	return total
}

func (cc *ClusterCollector) calculateTotalRequests(pods []v1.Pod) models.ResourceMetrics {
	total := models.ResourceMetrics{}
	for _, pod := range pods {
		for _, container := range pod.Spec.Containers {
			total.CPU += container.Resources.Requests.Cpu().MilliValue()
			total.Memory += container.Resources.Requests.Memory().Value()
			if ephemeral, ok := container.Resources.Requests[v1.ResourceEphemeralStorage]; ok {
				total.EphemeralStorage += ephemeral.Value()
			}

			// Sum GPU requests if available
			if gpuQuantity, ok := container.Resources.Requests["nvidia.com/gpu"]; ok {
				gpuMetric := models.GPUMetrics{
					DeviceID:    fmt.Sprintf("pod-%s-%s-gpu", pod.Namespace, pod.Name),
					MemoryTotal: uint64(gpuQuantity.Value() * 1024 * 1024 * 1024), // Convert to bytes
				}
				total.GPUDevices = append(total.GPUDevices, gpuMetric)
			}
		}
	}
	total.Pods = int64(len(pods))
	logrus.Debugf("Calculated total requests: CPU=%d, Memory=%d, EphemeralStorage=%d, Pods=%d, GPUs=%d",
		total.CPU, total.Memory, total.EphemeralStorage, total.Pods, len(total.GPUDevices))
	return total
}

func (cc *ClusterCollector) calculateTotalLimits(pods []v1.Pod) models.ResourceMetrics {
	total := models.ResourceMetrics{}
	for _, pod := range pods {
		for _, container := range pod.Spec.Containers {
			total.CPU += container.Resources.Limits.Cpu().MilliValue()
			total.Memory += container.Resources.Limits.Memory().Value()
			if ephemeral, ok := container.Resources.Limits[v1.ResourceEphemeralStorage]; ok {
				total.EphemeralStorage += ephemeral.Value()
			}

			// Sum GPU limits if available
			if gpuQuantity, ok := container.Resources.Limits["nvidia.com/gpu"]; ok {
				gpuMetric := models.GPUMetrics{
					DeviceID:    fmt.Sprintf("pod-%s-%s-gpu", pod.Namespace, pod.Name),
					MemoryTotal: uint64(gpuQuantity.Value() * 1024 * 1024 * 1024), // Convert to bytes
				}
				total.GPUDevices = append(total.GPUDevices, gpuMetric)
			}
		}
	}
	total.Pods = int64(len(pods))
	logrus.Debugf("Calculated total limits: CPU=%d, Memory=%d, EphemeralStorage=%d, Pods=%d, GPUs=%d",
		total.CPU, total.Memory, total.EphemeralStorage, total.Pods, len(total.GPUDevices))
	return total
}

func (cc *ClusterCollector) countContainers(pods []v1.Pod) int {
	count := 0
	for _, pod := range pods {
		count += len(pod.Spec.Containers)
		count += len(pod.Spec.InitContainers)
		count += len(pod.Spec.EphemeralContainers)
	}
	return count
}

================
File: collector.go
================
// Copyright 2024 Vega Cloud, Inc.
//
// Use of this software is governed by the Business Source License
// included in the file licenses/BSL.txt.
//
// As of the Change Date specified in that file, in accordance with
// the Business Source License, use of this software will be governed
// by the Apache License, Version 2.0, included in the file
// licenses/APL.txt.
// File: pkg/collectors/collector.go

// Package collectors hosts the collection functions
package collectors

import (
	"context"
)

// Collector defines the interface for all metric collectors
type Collector interface {
	// CollectMetrics collects metrics and returns them as an interface{},
	// which can be type-asserted to the specific metrics type for each collector
	CollectMetrics(ctx context.Context) (interface{}, error)
}

================
File: cronJob.go
================
// Copyright 2024 Vega Cloud, Inc.
//
// Use of this software is governed by the Business Source License
// included in the file licenses/BSL.txt.
//
// As of the Change Date specified in that file, in accordance with
// the Business Source License, use of this software will be governed
// by the Apache License, Version 2.0, included in the file
// licenses/APL.txt.

// Package collectors hosts the collection functions
package collectors

import (
	"context"
	"fmt"
	"time"

	"github.com/robfig/cron/v3"
	"github.com/sirupsen/logrus"
	batchv1 "k8s.io/api/batch/v1"
	v1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/client-go/kubernetes"

	"github.com/vegacloud/kubernetes/metricsagent/pkg/config"
	"github.com/vegacloud/kubernetes/metricsagent/pkg/models"
)

// CronJobCollector collects metrics from Kubernetes cron jobs.
type CronJobCollector struct {
	clientset *kubernetes.Clientset
	config    *config.Config
	parser    cron.Parser
}

// NewCronJobCollector creates a new CronJobCollector.
func NewCronJobCollector(clientset *kubernetes.Clientset, cfg *config.Config) *CronJobCollector {
	return &CronJobCollector{
		clientset: clientset,
		config:    cfg,
		parser:    cron.NewParser(cron.Second | cron.Minute | cron.Hour | cron.Dom | cron.Month | cron.Dow | cron.Descriptor),
	}
}

// CollectMetrics collects metrics from Kubernetes cron jobs.
func (cjc *CronJobCollector) CollectMetrics(ctx context.Context) (interface{}, error) {
	cronJobs, err := cjc.clientset.BatchV1().CronJobs("").List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to list cron jobs: %w", err)
	}
	logrus.Debugf("Successfully listed %d cron jobs", len(cronJobs.Items))

	metrics := make([]models.CronJobMetrics, 0, len(cronJobs.Items))
	for _, cj := range cronJobs.Items {
		metric, err := cjc.parseCronJobMetrics(ctx, cj)
		if err != nil {
			logrus.Warnf("Failed to parse metrics for cronjob %s/%s: %v", cj.Namespace, cj.Name, err)
			continue
		}
		metrics = append(metrics, metric)
	}

	logrus.Debugf("Collected metrics for %d cron jobs", len(metrics))
	return metrics, nil
}

// parseCronJobMetrics parses metrics from a Kubernetes cron job.
func (cjc *CronJobCollector) parseCronJobMetrics(ctx context.Context, cj batchv1.CronJob) (models.CronJobMetrics, error) {
	metrics := models.CronJobMetrics{
		Name:      cj.Name,
		Namespace: cj.Namespace,
		Schedule:  cj.Spec.Schedule,
		Labels:    cj.Labels,
		Status: models.CronJobStatus{
			Active:             len(cj.Status.Active),
			LastScheduleTime:   cjc.convertTime(cj.Status.LastScheduleTime),
			LastSuccessfulTime: cjc.convertTime(cj.Status.LastSuccessfulTime),
		},
		Spec: models.CronJobSpec{
			Suspend:                    cjc.getBoolValue(cj.Spec.Suspend),
			Concurrency:                string(cj.Spec.ConcurrencyPolicy),
			StartingDeadlineSeconds:    cjc.getInt64Value(cj.Spec.StartingDeadlineSeconds),
			SuccessfulJobsHistoryLimit: cjc.getInt32Value(cj.Spec.SuccessfulJobsHistoryLimit),
			FailedJobsHistoryLimit:     cjc.getInt32Value(cj.Spec.FailedJobsHistoryLimit),
		},
	}

	// Calculate next scheduled time
	nextSchedule, err := cjc.calculateNextSchedule(cj)
	if err != nil {
		logrus.Warnf("Failed to calculate next schedule for cronjob %s/%s: %v", cj.Namespace, cj.Name, err)
	} else {
		metrics.Status.NextScheduledTime = nextSchedule
	}

	// Get associated jobs metrics
	jobs, err := cjc.getAssociatedJobsMetrics(ctx, cj)
	if err != nil {
		logrus.Warnf("Failed to get associated jobs for cronjob %s/%s: %v", cj.Namespace, cj.Name, err)
	} else {
		metrics.JobMetrics = jobs
		metrics.Status.SuccessRate = cjc.calculateSuccessRate(jobs)
	}

	return metrics, nil
}

// getResourceMetrics collects resource metrics from the CronJob template
func (cjc *CronJobCollector) getResourceMetrics(cj batchv1.CronJob) models.ResourceMetrics {
	resources := models.ResourceMetrics{}

	if cj.Spec.JobTemplate.Spec.Template.Spec.Containers == nil {
		return resources
	}

	for _, container := range cj.Spec.JobTemplate.Spec.Template.Spec.Containers {
		// Sum up resource requests
		if container.Resources.Requests != nil {
			resources.CPU += container.Resources.Requests.Cpu().MilliValue()
			resources.Memory += container.Resources.Requests.Memory().Value()
			if storage, ok := container.Resources.Requests[v1.ResourceStorage]; ok {
				resources.Storage += storage.Value()
			}
			if ephemeral, ok := container.Resources.Requests[v1.ResourceEphemeralStorage]; ok {
				resources.EphemeralStorage += ephemeral.Value()
			}
		}

		// Check for GPU requests
		if gpuQuantity, ok := container.Resources.Requests["nvidia.com/gpu"]; ok {
			gpuMetric := models.GPUMetrics{
				DeviceID:    fmt.Sprintf("cronjob-%s-%s-gpu", cj.Namespace, cj.Name),
				MemoryTotal: uint64(gpuQuantity.Value() * 1024 * 1024 * 1024), // Convert to bytes
				MemoryUsed:  0,
				OptMetrics: models.OptionalMetrics{
					DutyCycle:   0,
					Temperature: 0,
					PowerUsage:  0,
				},
			}
			resources.GPUDevices = append(resources.GPUDevices, gpuMetric)
		}
	}

	return resources
}

// getAssociatedJobsMetrics collects metrics from jobs associated with the CronJob
func (cjc *CronJobCollector) getAssociatedJobsMetrics(ctx context.Context, cj batchv1.CronJob) ([]models.JobMetrics, error) {
	selector := metav1.ListOptions{
		LabelSelector: fmt.Sprintf("job-name=%s", cj.Name),
	}

	jobs, err := cjc.clientset.BatchV1().Jobs(cj.Namespace).List(ctx, selector)
	if err != nil {
		return nil, fmt.Errorf("failed to list jobs: %w", err)
	}

	metrics := make([]models.JobMetrics, 0, len(jobs.Items))
	for _, job := range jobs.Items {
		metrics = append(metrics, cjc.parseJobMetrics(job))
	}

	return metrics, nil
}

// parseJobMetrics parses metrics from a Kubernetes job
func (cjc *CronJobCollector) parseJobMetrics(job batchv1.Job) models.JobMetrics {
	return models.JobMetrics{
		Name:            job.Name,
		Namespace:       job.Namespace,
		StartTime:       cjc.convertTime(job.Status.StartTime),
		CompletionTime:  cjc.convertTime(job.Status.CompletionTime),
		Active:          job.Status.Active,
		Succeeded:       job.Status.Succeeded,
		Failed:          job.Status.Failed,
		Status:          cjc.getJobStatus(job.Status),
		Duration:        cjc.calculateJobDuration(job.Status),
		Labels:          job.Labels,
		ResourceMetrics: cjc.getJobResourceMetrics(job),
	}
}

// Helper functions

func (cjc *CronJobCollector) calculateNextSchedule(cj batchv1.CronJob) (*time.Time, error) {
	schedule, err := cjc.parser.Parse(cj.Spec.Schedule)
	if err != nil {
		return nil, fmt.Errorf("failed to parse schedule: %w", err)
	}

	var baseTime time.Time
	if cj.Status.LastScheduleTime != nil {
		baseTime = cj.Status.LastScheduleTime.Time
	} else {
		baseTime = time.Now()
	}

	next := schedule.Next(baseTime)
	return &next, nil
}

func (cjc *CronJobCollector) calculateSuccessRate(jobs []models.JobMetrics) float64 {
	if len(jobs) == 0 {
		return 0
	}

	completed := 0
	succeeded := 0
	for _, job := range jobs {
		if job.CompletionTime != nil {
			completed++
			if job.Succeeded > 0 {
				succeeded++
			}
		}
	}

	if completed == 0 {
		return 0
	}

	return float64(succeeded) / float64(completed) * 100
}

func (cjc *CronJobCollector) getJobResourceMetrics(job batchv1.Job) models.ResourceMetrics {
	resources := models.ResourceMetrics{}

	if job.Spec.Template.Spec.Containers == nil {
		return resources
	}

	for _, container := range job.Spec.Template.Spec.Containers {
		if container.Resources.Requests != nil {
			resources.CPU += container.Resources.Requests.Cpu().MilliValue()
			resources.Memory += container.Resources.Requests.Memory().Value()
			if storage, ok := container.Resources.Requests[v1.ResourceStorage]; ok {
				resources.Storage += storage.Value()
			}
			if ephemeral, ok := container.Resources.Requests[v1.ResourceEphemeralStorage]; ok {
				resources.EphemeralStorage += ephemeral.Value()
			}
		}

		if gpuQuantity, ok := container.Resources.Requests["nvidia.com/gpu"]; ok {
			gpuMetric := models.GPUMetrics{
				DeviceID:    fmt.Sprintf("job-%s-%s-gpu", job.Namespace, job.Name),
				MemoryTotal: uint64(gpuQuantity.Value() * 1024 * 1024 * 1024),
				MemoryUsed:  0,
				OptMetrics: models.OptionalMetrics{
					DutyCycle:   0,
					Temperature: 0,
					PowerUsage:  0,
				},
			}
			resources.GPUDevices = append(resources.GPUDevices, gpuMetric)
		}
	}

	return resources
}

// Utility functions for handling nullable values
func (cjc *CronJobCollector) convertTime(t *metav1.Time) *time.Time {
	if t == nil {
		return nil
	}
	converted := t.Time
	return &converted
}

func (cjc *CronJobCollector) getBoolValue(b *bool) bool {
	if b == nil {
		return false
	}
	return *b
}

func (cjc *CronJobCollector) getInt64Value(i *int64) int64 {
	if i == nil {
		return 0
	}
	return *i
}

func (cjc *CronJobCollector) getInt32Value(i *int32) int32 {
	if i == nil {
		return 0
	}
	return *i
}

func (cjc *CronJobCollector) getJobStatus(status batchv1.JobStatus) string {
	switch {
	case status.Succeeded > 0:
		return "Succeeded"
	case status.Failed > 0:
		return "Failed"
	case status.Active > 0:
		return "Active"
	default:
		return "Pending"
	}
}

func (cjc *CronJobCollector) calculateJobDuration(status batchv1.JobStatus) *time.Duration {
	if status.StartTime == nil {
		return nil
	}

	endTime := time.Now()
	if status.CompletionTime != nil {
		endTime = status.CompletionTime.Time
	}

	duration := endTime.Sub(status.StartTime.Time)
	return &duration
}

================
File: daemonsets.go
================
// Copyright 2024 Vega Cloud, Inc.
//
// Use of this software is governed by the Business Source License
// included in the file licenses/BSL.txt.
//
// As of the Change Date specified in that file, in accordance with
// the Business Source License, use of this software will be governed
// by the Apache License, Version 2.0, included in the file
// licenses/APL.txt.
// File: pkg/collectors/daemonsets.go

// Package collectors hosts the collection functions
package collectors

import (
	"context"
	"fmt"

	"github.com/sirupsen/logrus"
	appsv1 "k8s.io/api/apps/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/client-go/kubernetes"

	"github.com/vegacloud/kubernetes/metricsagent/pkg/config"
	"github.com/vegacloud/kubernetes/metricsagent/pkg/models"
)

// DaemonSetCollector collects metrics for daemon sets
type DaemonSetCollector struct {
	clientset *kubernetes.Clientset
	config    *config.Config
}

// NewDaemonSetCollector creates a new DaemonSetCollector instance
func NewDaemonSetCollector(clientset *kubernetes.Clientset, cfg *config.Config) *DaemonSetCollector {
	collector := &DaemonSetCollector{
		clientset: clientset,
		config:    cfg,
	}
	logrus.Debug("DaemonSetCollector created successfully")
	return collector
}

// CollectMetrics collects metrics for daemon sets
func (dc *DaemonSetCollector) CollectMetrics(ctx context.Context) (interface{}, error) {
	daemonsets, err := dc.clientset.AppsV1().DaemonSets("").List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to list daemonsets: %w", err)
	}

	metrics := make([]models.DaemonSetMetrics, 0, len(daemonsets.Items))
	for _, ds := range daemonsets.Items {
		metric := dc.convertDaemonSetToMetrics(&ds)
		metrics = append(metrics, metric)
	}

	logrus.Debugf("Collected metrics for %d daemonsets", len(metrics))
	return metrics, nil
}

// convertDaemonSetToMetrics converts a Kubernetes DaemonSet to metrics
func (dc *DaemonSetCollector) convertDaemonSetToMetrics(ds *appsv1.DaemonSet) models.DaemonSetMetrics {
	conditions := make([]models.DaemonSetCondition, 0, len(ds.Status.Conditions))
	for _, condition := range ds.Status.Conditions {
		conditions = append(conditions, models.DaemonSetCondition{
			Type:               string(condition.Type),
			Status:             string(condition.Status),
			LastTransitionTime: &condition.LastTransitionTime.Time,
			Reason:             condition.Reason,
			Message:            condition.Message,
		})
	}

	return models.DaemonSetMetrics{
		Name:                   ds.Name,
		Namespace:              ds.Namespace,
		DesiredNumberScheduled: ds.Status.DesiredNumberScheduled,
		CurrentNumberScheduled: ds.Status.CurrentNumberScheduled,
		NumberReady:            ds.Status.NumberReady,
		UpdatedNumberScheduled: ds.Status.UpdatedNumberScheduled,
		NumberAvailable:        ds.Status.NumberAvailable,
		NumberUnavailable:      ds.Status.NumberUnavailable,
		NumberMisscheduled:     ds.Status.NumberMisscheduled,
		Labels:                 ds.Labels,
		Annotations:            ds.Annotations,
		CreationTimestamp:      &ds.CreationTimestamp.Time,
		CollisionCount:         ds.Status.CollisionCount,
		Status: models.DaemonSetStatus{
			ObservedGeneration: ds.Status.ObservedGeneration,
		},
		Conditions: conditions,
	}
}

================
File: hpa.go
================
// Copyright 2024 Vega Cloud, Inc.
//
// Use of this software is governed by the Business Source License
// included in the file licenses/BSL.txt.
//
// As of the Change Date specified in that file, in accordance with
// the Business Source License, use of this software will be governed
// by the Apache License, Version 2.0, included in the file
// licenses/APL.txt.
// File: pkg/collectors/hpa.go

// Package collectors hosts the collection functions
package collectors

import (
	"context"
	"fmt"
	"time"

	"github.com/sirupsen/logrus"
	autoscalingv1 "k8s.io/api/autoscaling/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/client-go/kubernetes"

	"github.com/vegacloud/kubernetes/metricsagent/pkg/config"
	"github.com/vegacloud/kubernetes/metricsagent/pkg/models"
)

// HPACollector collects metrics from Kubernetes horizontal pod autoscalers.
type HPACollector struct {
	clientset *kubernetes.Clientset
	config    *config.Config
}

// NewHPACollector creates a new HPACollector.
func NewHPACollector(clientset *kubernetes.Clientset, cfg *config.Config) *HPACollector {
	collector := &HPACollector{
		clientset: clientset,
		config:    cfg,
	}
	logrus.Debug("HPACollector created successfully")
	return collector
}

// CollectMetrics collects metrics from Kubernetes horizontal pod autoscalers.
func (hc *HPACollector) CollectMetrics(ctx context.Context) (interface{}, error) {
	metrics, err := hc.CollectHPAMetrics(ctx)
	if err != nil {
		return nil, err
	}
	logrus.Debug("Successfully collected HPA metrics")
	return metrics, nil
}

// CollectHPAMetrics collects metrics from Kubernetes horizontal pod autoscalers.
func (hc *HPACollector) CollectHPAMetrics(ctx context.Context) ([]models.HPAMetrics, error) {
	hpas, err := hc.clientset.AutoscalingV1().HorizontalPodAutoscalers("").List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to list HPAs: %w", err)
	}
	logrus.Debugf("Successfully listed %d HPAs", len(hpas.Items))

	metrics := make([]models.HPAMetrics, 0, len(hpas.Items))
	for _, hpa := range hpas.Items {
		metrics = append(metrics, hc.parseHPAMetrics(hpa))
	}

	logrus.Debugf("Collected metrics for %d HPAs", len(metrics))
	return metrics, nil
}

func (hc *HPACollector) parseHPAMetrics(hpa autoscalingv1.HorizontalPodAutoscaler) models.HPAMetrics {
	metrics := models.HPAMetrics{
		Name:      hpa.Name,
		Namespace: hpa.Namespace,
		ScaleTargetRef: models.ScaleTargetRef{
			Kind:       hpa.Spec.ScaleTargetRef.Kind,
			Name:       hpa.Spec.ScaleTargetRef.Name,
			APIVersion: hpa.Spec.ScaleTargetRef.APIVersion,
		},
		MinReplicas:                     hpa.Spec.MinReplicas,
		MaxReplicas:                     hpa.Spec.MaxReplicas,
		CurrentReplicas:                 hpa.Status.CurrentReplicas,
		DesiredReplicas:                 hpa.Status.DesiredReplicas,
		CurrentCPUUtilizationPercentage: hpa.Status.CurrentCPUUtilizationPercentage,
		TargetCPUUtilizationPercentage:  hpa.Spec.TargetCPUUtilizationPercentage,
		LastScaleTime: func() *time.Time {
			if hpa.Status.LastScaleTime != nil {
				t := hpa.Status.LastScaleTime.Time
				return &t
			}
			return nil
		}(),
		ObservedGeneration: hpa.Status.ObservedGeneration,
		Labels:             hpa.Labels,
		Annotations:        hpa.Annotations,
		Status: models.HPAStatus{
			CurrentReplicas:                 hpa.Status.CurrentReplicas,
			DesiredReplicas:                 hpa.Status.DesiredReplicas,
			CurrentCPUUtilizationPercentage: hpa.Status.CurrentCPUUtilizationPercentage,
			LastScaleTime: func() *time.Time {
				if hpa.Status.LastScaleTime != nil {
					t := hpa.Status.LastScaleTime.Time
					return &t
				}
				return nil
			}(),
		},
	}

	logrus.Debugf("Parsed HPA metrics for HPA %s/%s", hpa.Namespace, hpa.Name)
	return metrics
}

================
File: job.go
================
// Copyright 2024 Vega Cloud, Inc.
//
// Use of this software is governed by the Business Source License
// included in the file licenses/BSL.txt.
//
// As of the Change Date specified in that file, in accordance with
// the Business Source License, use of this software will be governed
// by the Apache License, Version 2.0, included in the file
// licenses/APL.txt.
// File: pkg/collectors/job.go

// Package collectors hosts the collection functions
package collectors

import (
	"context"
	"fmt"
	"time"

	"github.com/sirupsen/logrus"
	batchv1 "k8s.io/api/batch/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/client-go/kubernetes"

	"github.com/vegacloud/kubernetes/metricsagent/pkg/config"
	"github.com/vegacloud/kubernetes/metricsagent/pkg/models"
)

// JobCollector collects metrics from Kubernetes jobs.
type JobCollector struct {
	clientset *kubernetes.Clientset
	config    *config.Config
}

// NewJobCollector creates a new JobCollector.
func NewJobCollector(clientset *kubernetes.Clientset, cfg *config.Config) *JobCollector {
	collector := &JobCollector{
		clientset: clientset,
		config:    cfg,
	}
	logrus.Debug("JobCollector created successfully")
	return collector
}

// CollectMetrics collects metrics from Kubernetes jobs.
func (jc *JobCollector) CollectMetrics(ctx context.Context) (interface{}, error) {
	metrics, err := jc.CollectJobMetrics(ctx)
	if err != nil {
		return nil, err
	}
	logrus.Debug("Successfully collected job metrics")
	return metrics, nil
}

// CollectJobMetrics collects metrics from Kubernetes jobs.
func (jc *JobCollector) CollectJobMetrics(ctx context.Context) ([]models.JobMetrics, error) {
	jobs, err := jc.clientset.BatchV1().Jobs("").List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to list jobs: %w", err)
	}
	logrus.Debugf("Successfully listed %d jobs", len(jobs.Items))

	jobMetrics := make([]models.JobMetrics, 0, len(jobs.Items))

	for _, job := range jobs.Items {
		if job.Labels == nil {
			job.Labels = make(map[string]string)
		}

		var duration *time.Duration
		if job.Status.CompletionTime != nil && job.Status.StartTime != nil {
			d := job.Status.CompletionTime.Sub(job.Status.StartTime.Time)
			duration = &d
		}

		status := calculateJobStatus(&job)

		metrics := models.JobMetrics{
			Name:            job.Name,
			Namespace:       job.Namespace,
			Labels:          job.Labels,
			Active:          job.Status.Active,
			Succeeded:       job.Status.Succeeded,
			Failed:          job.Status.Failed,
			Status:          status,
			StartTime:       timePtr(job.Status.StartTime),
			CompletionTime:  timePtr(job.Status.CompletionTime),
			Duration:        duration,
			Parallelism:     job.Spec.Parallelism,
			Completions:     job.Spec.Completions,
			BackoffLimit:    job.Spec.BackoffLimit,
			Suspended:       job.Spec.Suspend != nil && *job.Spec.Suspend,
			CreationTime:    &job.CreationTimestamp.Time,
			Conditions:      convertJobConditions(job.Status.Conditions),
			ResourceMetrics: jc.collectJobResourceMetrics(ctx, &job),
		}

		jobMetrics = append(jobMetrics, metrics)
		logrus.Debugf("Collected metrics for job %s/%s", job.Namespace, job.Name)
	}

	return jobMetrics, nil
}

// Helper functions

func timePtr(t *metav1.Time) *time.Time {
	if t == nil {
		return nil
	}
	tt := t.Time
	return &tt
}

func calculateJobStatus(job *batchv1.Job) string {
	if job.Status.Succeeded > 0 {
		return "Completed"
	}
	if job.Status.Failed > 0 {
		return "Failed"
	}
	if job.Status.Active > 0 {
		return "Active"
	}
	if job.Spec.Suspend != nil && *job.Spec.Suspend {
		return "Suspended"
	}
	return "Pending"
}

func convertJobConditions(conditions []batchv1.JobCondition) []models.JobCondition {
	result := make([]models.JobCondition, 0, len(conditions))
	for _, c := range conditions {
		condition := models.JobCondition{
			Type:               string(c.Type),
			Status:             string(c.Status),
			LastProbeTime:      timePtr(&c.LastProbeTime),
			LastTransitionTime: timePtr(&c.LastTransitionTime),
			Reason:             c.Reason,
			Message:            c.Message,
		}
		result = append(result, condition)
	}
	return result
}

func (jc *JobCollector) collectJobResourceMetrics(ctx context.Context, job *batchv1.Job) models.ResourceMetrics {
	selector := metav1.LabelSelector{MatchLabels: job.Spec.Selector.MatchLabels}
	labelSelector, err := metav1.LabelSelectorAsSelector(&selector)
	if err != nil {
		logrus.Errorf("Failed to create selector for job %s/%s: %v", job.Namespace, job.Name, err)
		return models.ResourceMetrics{}
	}

	pods, err := jc.clientset.CoreV1().Pods(job.Namespace).List(ctx, metav1.ListOptions{
		LabelSelector: labelSelector.String(),
	})
	if err != nil {
		logrus.Errorf("Failed to list pods for job %s/%s: %v", job.Namespace, job.Name, err)
		return models.ResourceMetrics{}
	}

	metrics := models.ResourceMetrics{}
	for _, pod := range pods.Items {
		for _, container := range pod.Spec.Containers {
			metrics.CPU += container.Resources.Requests.Cpu().MilliValue()
			metrics.Memory += container.Resources.Requests.Memory().Value()
			if container.Resources.Requests.Storage() != nil {
				metrics.Storage += container.Resources.Requests.Storage().Value()
			}
		}
	}

	return metrics
}

================
File: namespace.go
================
// Copyright 2024 Vega Cloud, Inc.
//
// Use of this software is governed by the Business Source License
// included in the file licenses/BSL.txt.
//
// As of the Change Date specified in that file, in accordance with
// the Business Source License, use of this software will be governed
// by the Apache License, Version 2.0, included in the file
// licenses/APL.txt.
// File: pkg/collectors/namespace.go

// Package collectors hosts the collection functions
package collectors

import (
	"context"
	"fmt"
	"strings"

	"github.com/sirupsen/logrus"
	v1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/client-go/kubernetes"

	"github.com/vegacloud/kubernetes/metricsagent/pkg/config"
	"github.com/vegacloud/kubernetes/metricsagent/pkg/models"
)

// NamespaceCollector collects metrics from Kubernetes namespaces.
type NamespaceCollector struct {
	clientset *kubernetes.Clientset
	config    *config.Config
}

// NewNamespaceCollector creates a new NamespaceCollector.
func NewNamespaceCollector(clientset *kubernetes.Clientset, cfg *config.Config) *NamespaceCollector {
	logrus.Debug("Creating new NamespaceCollector")
	return &NamespaceCollector{
		clientset: clientset,
		config:    cfg,
	}
}

// CollectMetrics collects metrics from Kubernetes namespaces.
func (nc *NamespaceCollector) CollectMetrics(ctx context.Context) (interface{}, error) {
	logrus.Debug("Collecting namespace metrics")
	return nc.CollectNamespaceMetrics(ctx)
}

// CollectNamespaceMetrics collects metrics from Kubernetes namespaces.
func (nc *NamespaceCollector) CollectNamespaceMetrics(ctx context.Context) ([]models.NamespaceMetrics, error) {
	namespaces, err := nc.clientset.CoreV1().Namespaces().List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to list namespaces: %w", err)
	}
	logrus.Debugf("Listed %d namespaces", len(namespaces.Items))

	namespaceMetrics := make([]models.NamespaceMetrics, 0, len(namespaces.Items))

	for _, ns := range namespaces.Items {
		metrics, err := nc.collectSingleNamespaceMetrics(ctx, ns)
		if err != nil {
			logrus.Warnf("Failed to collect metrics for namespace %s: %v", ns.Name, err)
			continue
		}
		namespaceMetrics = append(namespaceMetrics, metrics)
		logrus.Debugf("Collected metrics for namespace %s", ns.Name)
	}

	return namespaceMetrics, nil
}

// collectSingleNamespaceMetrics collects metrics from a single Kubernetes namespace.
func (nc *NamespaceCollector) collectSingleNamespaceMetrics(
	ctx context.Context,
	ns v1.Namespace,
) (models.NamespaceMetrics, error) {
	if ns.Labels == nil {
		ns.Labels = make(map[string]string)
	}

	metrics := models.NamespaceMetrics{
		Name:              ns.Name,
		Status:            string(ns.Status.Phase),
		Phase:             string(ns.Status.Phase),
		CreationTimestamp: ns.CreationTimestamp.Time,
		DeletionTimestamp: nil,
		Finalizers:        ns.Finalizers,
		Labels:            ns.Labels,
		Annotations:       ns.Annotations,
	}

	// Set deletion timestamp if exists
	if ns.DeletionTimestamp != nil {
		deletionTime := ns.DeletionTimestamp.Time
		metrics.DeletionTimestamp = &deletionTime
	}

	// Convert conditions
	for _, condition := range ns.Status.Conditions {
		metrics.Conditions = append(metrics.Conditions, models.NamespaceCondition{
			Type:               string(condition.Type),
			Status:             string(condition.Status),
			LastTransitionTime: &condition.LastTransitionTime.Time,
			Reason:             condition.Reason,
			Message:            condition.Message,
		})
	}

	// Collect ResourceQuotas
	quotas, err := nc.clientset.CoreV1().ResourceQuotas(ns.Name).List(ctx, metav1.ListOptions{})
	if err != nil {
		return metrics, fmt.Errorf("failed to list resource quotas: %w", err)
	}
	logrus.Debugf("Listed %d resource quotas for namespace %s", len(quotas.Items), ns.Name)

	for _, quota := range quotas.Items {
		metrics.ResourceQuotas = append(metrics.ResourceQuotas, nc.parseResourceQuota(quota))
	}

	// Collect LimitRanges
	limitRanges, err := nc.clientset.CoreV1().LimitRanges(ns.Name).List(ctx, metav1.ListOptions{})
	if err != nil {
		return metrics, fmt.Errorf("failed to list limit ranges: %w", err)
	}
	logrus.Debugf("Listed %d limit ranges for namespace %s", len(limitRanges.Items), ns.Name)

	for _, lr := range limitRanges.Items {
		metrics.LimitRanges = append(metrics.LimitRanges, nc.parseLimitRange(lr))
	}

	// Collect usage metrics
	metrics.Usage, err = nc.collectNamespaceUsage(ctx, ns.Name)
	if err != nil {
		logrus.Warnf("Failed to collect usage metrics for namespace %s: %v", ns.Name, err)
	}

	return metrics, nil
}

// parseResourceQuota parses metrics from a Kubernetes resource quota.
func (nc *NamespaceCollector) parseResourceQuota(quota v1.ResourceQuota) models.ResourceQuotaMetrics {
	metrics := models.ResourceQuotaMetrics{
		Name: quota.Name,
	}

	// Parse basic resources
	for resourceName, hard := range quota.Status.Hard {
		used := quota.Status.Used[resourceName]
		metrics.Resources = append(metrics.Resources, models.ResourceMetric{
			ResourceName: string(resourceName),
			Hard:         hard.String(),
			Used:         used.String(),
		})
	}

	// Parse quota scopes
	if len(quota.Spec.Scopes) > 0 {
		for _, scope := range quota.Spec.Scopes {
			scopeMetric := models.QuotaScopeMetrics{
				ScopeName: string(scope),
			}
			if quota.Spec.ScopeSelector != nil && quota.Spec.ScopeSelector.MatchExpressions != nil {
				for _, expr := range quota.Spec.ScopeSelector.MatchExpressions {
					scopeMetric.MatchScopes = append(scopeMetric.MatchScopes, string(expr.Operator))
				}
			}
			metrics.Scopes = append(metrics.Scopes, scopeMetric)
		}
	}

	// Parse priority class quotas
	for resourceName, hard := range quota.Status.Hard {
		if isPriorityClassResource(string(resourceName)) {
			priorityClass := extractPriorityClass(string(resourceName))
			metrics.PriorityQuotas = append(metrics.PriorityQuotas, models.PriorityClassQuotaMetrics{
				PriorityClass: priorityClass,
				Hard:          map[string]string{string(resourceName): hard.ToUnstructured().(string)},
				Used:          map[string]string{string(resourceName): quota.Status.Used[resourceName].ToUnstructured().(string)},
			})
		}
	}

	return metrics
}

// Helper functions
func isPriorityClassResource(resource string) bool {
	return strings.HasPrefix(resource, "count/pods.") && strings.Contains(resource, "priorityclass")
}

func extractPriorityClass(resource string) string {
	parts := strings.Split(resource, ".")
	if len(parts) > 1 {
		return parts[len(parts)-1]
	}
	return ""
}

func isCustomResource(resource string) bool {
	return strings.Contains(resource, "/") && strings.Contains(resource, ".")
}

func parseCustomResource(resource string) (group, version, name string) {
	parts := strings.Split(resource, ".")
	if len(parts) >= 3 {
		group = parts[0]
		version = parts[1]
		name = parts[2]
	}
	return
}

func (nc *NamespaceCollector) parseLimitRange(lr v1.LimitRange) models.LimitRangeMetrics {
	metrics := models.LimitRangeMetrics{
		Name: lr.Name,
	}

	for _, item := range lr.Spec.Limits {
		limit := models.LimitRangeItem{
			Type: string(item.Type),
		}

		if item.Max != nil {
			limit.Max = make(map[string]string)
			for k, v := range item.Max {
				limit.Max[string(k)] = v.String()
			}
		}

		if item.Min != nil {
			limit.Min = make(map[string]string)
			for k, v := range item.Min {
				limit.Min[string(k)] = v.String()
			}
		}

		if item.Default != nil {
			limit.Default = make(map[string]string)
			for k, v := range item.Default {
				limit.Default[string(k)] = v.String()
			}
		}

		if item.DefaultRequest != nil {
			limit.DefaultRequest = make(map[string]string)
			for k, v := range item.DefaultRequest {
				limit.DefaultRequest[string(k)] = v.String()
			}
		}

		metrics.Limits = append(metrics.Limits, limit)
	}

	logrus.Debugf("Parsed limit range %s", lr.Name)
	return metrics
}

func (nc *NamespaceCollector) collectNamespaceUsage(
	ctx context.Context,
	namespace string,
) (models.ResourceUsage, error) {
	usage := models.ResourceUsage{}

	pods, err := nc.clientset.CoreV1().Pods(namespace).List(ctx, metav1.ListOptions{})
	if err != nil {
		return usage, fmt.Errorf("failed to list pods: %w", err)
	}
	logrus.Debugf("Listed %d pods for namespace %s", len(pods.Items), namespace)

	for _, pod := range pods.Items {
		for _, container := range pod.Spec.Containers {
			usage.CPU += container.Resources.Requests.Cpu().MilliValue()
			usage.Memory += container.Resources.Requests.Memory().Value()
		}
	}

	pvcs, err := nc.clientset.CoreV1().PersistentVolumeClaims(namespace).List(ctx, metav1.ListOptions{})
	if err != nil {
		return usage, fmt.Errorf("failed to list PVCs: %w", err)
	}
	logrus.Debugf("Listed %d PVCs for namespace %s", len(pvcs.Items), namespace)

	for _, pvc := range pvcs.Items {
		if pvc.Spec.Resources.Requests != nil {
			if storage, ok := pvc.Spec.Resources.Requests[v1.ResourceStorage]; ok {
				usage.Storage += storage.Value()
			}
		}
	}

	usage.Pods = int64(len(pods.Items))

	logrus.Debugf("Collected usage metrics for namespace %s", namespace)
	return usage, nil
}

================
File: networking.go
================
// Copyright 2024 Vega Cloud, Inc.
//
// Use of this software is governed by the Business Source License
// included in the file licenses/BSL.txt.
//
// As of the Change Date specified in that file, in accordance with
// the Business Source License, use of this software will be governed
// by the Apache License, Version 2.0, included in the file
// licenses/APL.txt.
// File: pkg/collectors/networking.go

// Package collectors hosts the collection functions
package collectors

import (
	"context"
	"fmt"

	"github.com/sirupsen/logrus"
	corev1 "k8s.io/api/core/v1"
	networkingv1 "k8s.io/api/networking/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/client-go/kubernetes"

	"github.com/vegacloud/kubernetes/metricsagent/pkg/config"
	"github.com/vegacloud/kubernetes/metricsagent/pkg/models"
)

// NetworkingCollector collects metrics from Kubernetes networking resources.
type NetworkingCollector struct {
	clientset *kubernetes.Clientset
	config    *config.Config
}

// NewNetworkingCollector creates a new NetworkingCollector.
func NewNetworkingCollector(clientset *kubernetes.Clientset, cfg *config.Config) *NetworkingCollector {
	collector := &NetworkingCollector{
		clientset: clientset,
		config:    cfg,
	}
	logrus.Debug("NetworkingCollector created successfully")
	return collector
}

// CollectMetrics collects metrics from Kubernetes networking resources.
func (nc *NetworkingCollector) CollectMetrics(ctx context.Context) (interface{}, error) {
	metrics, err := nc.CollectNetworkingMetrics(ctx)
	if err != nil {
		return nil, err
	}
	logrus.Debug("Successfully collected networking metrics")
	return metrics, nil
}

// CollectNetworkingMetrics collects metrics from Kubernetes networking resources.
func (nc *NetworkingCollector) CollectNetworkingMetrics(ctx context.Context) (*models.NetworkingMetrics, error) {
	metrics := &models.NetworkingMetrics{}

	var err error
	metrics.Services, err = nc.collectServiceMetrics(ctx)
	if err != nil {
		logrus.Warnf("Failed to collect service metrics: %v", err)
	} else {
		logrus.Debug("Successfully collected service metrics")
	}

	metrics.Ingresses, err = nc.collectIngressMetrics(ctx)
	if err != nil {
		logrus.Warnf("Failed to collect ingress metrics: %v", err)
	} else {
		logrus.Debug("Successfully collected ingress metrics")
	}

	metrics.NetworkPolicies, err = nc.collectNetworkPolicyMetrics(ctx)
	if err != nil {
		logrus.Warnf("Failed to collect network policy metrics: %v", err)
	} else {
		logrus.Debug("Successfully collected network policy metrics")
	}

	return metrics, nil
}

func (nc *NetworkingCollector) collectServiceMetrics(ctx context.Context) ([]models.ServiceMetrics, error) {
	services, err := nc.clientset.CoreV1().Services("").List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to list services: %w", err)
	}
	logrus.Debugf("Successfully listed %d services", len(services.Items))

	metrics := make([]models.ServiceMetrics, 0, len(services.Items))

	for _, svc := range services.Items {
		metrics = append(metrics, nc.parseServiceMetrics(svc))
	}

	return metrics, nil
}

func (nc *NetworkingCollector) parseServiceMetrics(svc corev1.Service) models.ServiceMetrics {
	metrics := models.ServiceMetrics{
		Name:                  svc.Name,
		Namespace:             svc.Namespace,
		Type:                  string(svc.Spec.Type),
		ClusterIP:             svc.Spec.ClusterIP,
		ExternalIP:            svc.Spec.ExternalIPs,
		Labels:                svc.Labels,
		Annotations:           svc.Annotations,
		SessionAffinity:       string(svc.Spec.SessionAffinity),
		ExternalTrafficPolicy: string(svc.Spec.ExternalTrafficPolicy),
		HealthCheckNodePort:   svc.Spec.HealthCheckNodePort,
		IPFamilies:            make([]string, 0),
		IPFamilyPolicy: func() string {
			if svc.Spec.IPFamilyPolicy != nil {
				return string(*svc.Spec.IPFamilyPolicy)
			}
			return ""
		}(),
	}

	for _, family := range svc.Spec.IPFamilies {
		metrics.IPFamilies = append(metrics.IPFamilies, string(family))
	}

	if svc.Spec.LoadBalancerIP != "" {
		metrics.LoadBalancerIP = svc.Spec.LoadBalancerIP
	}

	// Parse status
	metrics.Status.LoadBalancer = models.LoadBalancerStatus{
		Ingress: make([]models.LoadBalancerIngress, len(svc.Status.LoadBalancer.Ingress)),
	}

	for i, ing := range svc.Status.LoadBalancer.Ingress {
		metrics.Status.LoadBalancer.Ingress[i] = models.LoadBalancerIngress{
			IP:       ing.IP,
			Hostname: ing.Hostname,
		}
	}

	// Parse conditions
	for _, cond := range svc.Status.Conditions {
		metrics.Status.Conditions = append(metrics.Status.Conditions, models.ServiceCondition{
			Type:               string(cond.Type),
			Status:             string(cond.Status),
			LastTransitionTime: &cond.LastTransitionTime.Time,
			Reason:             cond.Reason,
			Message:            cond.Message,
		})
	}

	for _, port := range svc.Spec.Ports {
		metrics.Ports = append(metrics.Ports, models.ServicePort{
			Name:       port.Name,
			Protocol:   string(port.Protocol),
			Port:       port.Port,
			TargetPort: port.TargetPort.String(),
			NodePort:   port.NodePort,
		})
	}

	metrics.Selector = svc.Spec.Selector

	logrus.Debugf("Parsed service metrics for service %s/%s", svc.Namespace, svc.Name)
	return metrics
}

func (nc *NetworkingCollector) collectIngressMetrics(ctx context.Context) ([]models.IngressMetrics, error) {
	ingresses, err := nc.clientset.NetworkingV1().Ingresses("").List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to list ingresses: %w", err)
	}
	logrus.Debugf("Successfully listed %d ingresses", len(ingresses.Items))

	metrics := make([]models.IngressMetrics, 0, len(ingresses.Items))

	for _, ing := range ingresses.Items {
		metrics = append(metrics, nc.parseIngressMetrics(ing))
	}

	return metrics, nil
}

func (nc *NetworkingCollector) parseIngressMetrics(ing networkingv1.Ingress) models.IngressMetrics {
	metrics := models.IngressMetrics{
		Name:      ing.Name,
		Namespace: ing.Namespace,
		ClassName: func() string {
			if ing.Spec.IngressClassName != nil {
				return *ing.Spec.IngressClassName
			}
			return ""
		}(),
		Labels:            ing.Labels,
		Annotations:       ing.Annotations,
		CreationTimestamp: &ing.CreationTimestamp.Time,
	}

	// Parse status
	for _, ing := range ing.Status.LoadBalancer.Ingress {
		if ing.IP != "" {
			metrics.LoadBalancerIngress = append(metrics.LoadBalancerIngress, ing.IP)
		}
		if ing.Hostname != "" {
			metrics.LoadBalancerIngress = append(metrics.LoadBalancerIngress, ing.Hostname)
		}
	}

	for _, rule := range ing.Spec.Rules {
		ingressRule := models.IngressRule{
			Host: rule.Host,
		}

		if rule.HTTP != nil {
			for _, path := range rule.HTTP.Paths {
				ingressRule.Paths = append(ingressRule.Paths, models.IngressPath{
					Path:     path.Path,
					PathType: string(*path.PathType),
					Backend: models.IngressBackend{
						Service: models.IngressServiceBackend{
							Name: path.Backend.Service.Name,
							Port: path.Backend.Service.Port.Number,
						},
					},
				})
			}
		}

		metrics.Rules = append(metrics.Rules, ingressRule)
	}

	for _, tls := range ing.Spec.TLS {
		metrics.TLS = append(metrics.TLS, models.IngressTLS{
			Hosts:      tls.Hosts,
			SecretName: tls.SecretName,
		})
	}

	logrus.Debugf("Parsed ingress metrics for ingress %s/%s", ing.Namespace, ing.Name)
	return metrics
}

func (nc *NetworkingCollector) collectNetworkPolicyMetrics(ctx context.Context) ([]models.NetworkPolicyMetrics, error) {
	networkPolicies, err := nc.clientset.NetworkingV1().NetworkPolicies("").List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to list network policies: %w", err)
	}
	logrus.Debugf("Successfully listed %d network policies", len(networkPolicies.Items))

	metrics := make([]models.NetworkPolicyMetrics, 0, len(networkPolicies.Items))

	for _, policy := range networkPolicies.Items {
		metrics = append(metrics, nc.parseNetworkPolicyMetrics(policy))
	}

	return metrics, nil
}

func (nc *NetworkingCollector) parseNetworkPolicyMetrics(policy networkingv1.NetworkPolicy) models.NetworkPolicyMetrics {
	metrics := models.NetworkPolicyMetrics{
		Name:        policy.Name,
		Namespace:   policy.Namespace,
		Labels:      policy.Labels,
		Annotations: policy.Annotations,
		PodSelector: policy.Spec.PodSelector.MatchLabels,
	}

	// Parse policy types
	for _, pType := range policy.Spec.PolicyTypes {
		metrics.PolicyTypes = append(metrics.PolicyTypes, string(pType))
	}

	// Parse ingress rules
	for _, rule := range policy.Spec.Ingress {
		ingressRule := models.NetworkPolicyIngressRule{}

		// Parse ports
		for _, port := range rule.Ports {
			ingressRule.Ports = append(ingressRule.Ports, models.NetworkPolicyPort{
				Protocol: string(*port.Protocol),
				Port:     port.Port.IntVal,
			})
		}

		// Parse from rules
		for _, from := range rule.From {
			peer := models.NetworkPolicyPeer{}
			if from.PodSelector != nil {
				peer.PodSelector = from.PodSelector.MatchLabels
			}
			if from.NamespaceSelector != nil {
				peer.NamespaceSelector = from.NamespaceSelector.MatchLabels
			}
			if from.IPBlock != nil {
				peer.IPBlock = &models.IPBlock{
					CIDR:   from.IPBlock.CIDR,
					Except: from.IPBlock.Except,
				}
			}
			ingressRule.From = append(ingressRule.From, peer)
		}

		metrics.Ingress = append(metrics.Ingress, ingressRule)
	}

	// Parse egress rules
	for _, rule := range policy.Spec.Egress {
		egressRule := models.NetworkPolicyEgressRule{}

		// Parse ports
		for _, port := range rule.Ports {
			egressRule.Ports = append(egressRule.Ports, models.NetworkPolicyPort{
				Protocol: string(*port.Protocol),
				Port:     port.Port.IntVal,
			})
		}

		// Parse to rules
		for _, to := range rule.To {
			peer := models.NetworkPolicyPeer{}
			if to.PodSelector != nil {
				peer.PodSelector = to.PodSelector.MatchLabels
			}
			if to.NamespaceSelector != nil {
				peer.NamespaceSelector = to.NamespaceSelector.MatchLabels
			}
			if to.IPBlock != nil {
				peer.IPBlock = &models.IPBlock{
					CIDR:   to.IPBlock.CIDR,
					Except: to.IPBlock.Except,
				}
			}
			egressRule.To = append(egressRule.To, peer)
		}

		metrics.Egress = append(metrics.Egress, egressRule)
	}

	return metrics
}

================
File: node.go
================
// Copyright 2024 Vega Cloud, Inc.
//
// Use of this software is governed by the Business Source License
// included in the file licenses/BSL.txt.
//
// As of the Change Date specified in that file, in accordance with
// the Business Source License, use of this software will be governed
// by the Apache License, Version 2.0, included in the file
// licenses/APL.txt.

// Package collectors hosts the collection functions
package collectors

import (
	"context"
	"crypto/tls"
	"errors"
	"fmt"
	"net/http"
	"os"
	"strings"
	"sync"
	"time"

	dto "github.com/prometheus/client_model/go"
	"github.com/prometheus/common/expfmt"
	"github.com/sirupsen/logrus"
	"golang.org/x/sync/errgroup"
	v1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/client-go/kubernetes"
	metricsv1beta1 "k8s.io/metrics/pkg/apis/metrics/v1beta1"
	metricsclientset "k8s.io/metrics/pkg/client/clientset/versioned"

	"github.com/vegacloud/kubernetes/metricsagent/pkg/config"
	"github.com/vegacloud/kubernetes/metricsagent/pkg/models"
)

// NodeCollector collects metrics from Kubernetes nodes.
type NodeCollector struct {
	clientset        *kubernetes.Clientset
	metricsClientset *metricsclientset.Clientset
	config           *config.Config
	bearerToken      string
	httpClient       *http.Client
}

// NewNodeCollector initializes a new NodeCollector.
func NewNodeCollector(
	clientset *kubernetes.Clientset,
	metricsClientset *metricsclientset.Clientset,
	cfg *config.Config,
) (*NodeCollector, error) {
	nc := &NodeCollector{
		clientset:        clientset,
		metricsClientset: metricsClientset,
		config:           cfg,
	}

	// Get bearer token
	token, err := nc.getBearerToken()
	if err != nil {
		return nil, fmt.Errorf("failed to get bearer token: %w", err)
	}
	nc.bearerToken = token
	logrus.Debug("Successfully retrieved bearer token")

	// Create HTTP client
	transport := &http.Transport{
		TLSClientConfig: &tls.Config{
			InsecureSkipVerify: cfg.VegaInsecure, //#nosec this is only off for local testing and will be true in prod.
		},
	}

	nc.httpClient = &http.Client{
		Timeout:   10 * time.Second,
		Transport: transport,
	}
	logrus.Debug("HTTP client created successfully")

	return nc, nil
}

// CollectMetrics collects enhanced node metrics.
func (nc *NodeCollector) CollectMetrics(ctx context.Context) (interface{}, error) {
	nodes, err := nc.clientset.CoreV1().Nodes().List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to list nodes: %w", err)
	}

	var mu sync.Mutex
	enhancedNodeMetrics := make([]models.EnhancedNodeMetrics, 0, len(nodes.Items))

	g, gCtx := errgroup.WithContext(ctx)
	semaphore := make(chan struct{}, nc.config.VegaMaxConcurrency)

	for _, node := range nodes.Items {
		if gCtx.Err() != nil {
			return nil, gCtx.Err()
		}

		node := node // capture variable
		g.Go(func() error {
			select {
			case <-gCtx.Done():
				return gCtx.Err()
			case semaphore <- struct{}{}:
			}
			defer func() { <-semaphore }()

			// Get node metrics from metrics server
			nodeMetricsList, err := nc.metricsClientset.MetricsV1beta1().NodeMetricses().List(gCtx, metav1.ListOptions{})
			if err != nil {
				return fmt.Errorf("failed to fetch node metrics: %w", err)
			}

			metricsMap := make(map[string]metricsv1beta1.NodeMetrics)
			for _, m := range nodeMetricsList.Items {
				metricsMap[m.Name] = m
			}

			metrics, err := nc.collectSingleNodeMetrics(gCtx, node, metricsMap[node.Name])
			if err != nil {
				return fmt.Errorf("failed to collect metrics for node %s: %w", node.Name, err)
			}

			mu.Lock()
			enhancedNodeMetrics = append(enhancedNodeMetrics, metrics)
			mu.Unlock()

			return nil
		})
	}

	if err := g.Wait(); err != nil {
		return nil, fmt.Errorf("error collecting node metrics: %w", err)
	}

	return enhancedNodeMetrics, nil
}

// collectEnhancedNodeMetrics collects metrics for all nodes.
func (nc *NodeCollector) collectEnhancedNodeMetrics(ctx context.Context) ([]models.EnhancedNodeMetrics, error) {
	// Fetch node details
	nodes, err := nc.clientset.CoreV1().Nodes().List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to list nodes: %w", err)
	}
	logrus.Debugf("Successfully listed %d nodes", len(nodes.Items))

	// Fetch node metrics from the Metrics Server
	nodeMetricsList, err := nc.metricsClientset.MetricsV1beta1().NodeMetricses().List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to fetch node metrics from metrics server: %w", err)
	}
	logrus.Debugf("Successfully fetched node metrics from metrics server")

	metricsMap := nc.mapNodeMetrics(nodeMetricsList)

	var (
		enhancedNodeMetrics = make([]models.EnhancedNodeMetrics, 0, len(nodes.Items))
		mu                  sync.Mutex
	)

	g, ctx := errgroup.WithContext(ctx)
	semaphore := make(chan struct{}, nc.config.VegaMaxConcurrency) // Configurable concurrency

	for _, node := range nodes.Items {
		node := node // capture variable
		g.Go(func() error {
			select {
			case semaphore <- struct{}{}:
				defer func() { <-semaphore }()
			case <-ctx.Done():
				return ctx.Err()
			}

			nodeMetric, found := metricsMap[node.Name]
			if !found {
				nodeMetric = metricsv1beta1.NodeMetrics{}
			}

			metrics, err := nc.collectSingleNodeMetrics(ctx, node, nodeMetric)
			if err != nil {
				logrus.Warnf("Failed to collect metrics for node %s: %v", node.Name, err)
				return nil // Continue with other nodes
			}

			mu.Lock()
			enhancedNodeMetrics = append(enhancedNodeMetrics, metrics)
			mu.Unlock()
			logrus.Debugf("Successfully collected metrics for node %s", node.Name)
			return nil
		})
	}

	if err := g.Wait(); err != nil {
		logrus.Warnf("Error waiting for node metrics collection: %v", err)
	}

	logrus.Debug("Successfully collected enhanced node metrics for all nodes")
	return enhancedNodeMetrics, nil
}

// mapNodeMetrics creates a map of node metrics for easier lookup by node name.
func (nc *NodeCollector) mapNodeMetrics(
	nodeMetricsList *metricsv1beta1.NodeMetricsList,
) map[string]metricsv1beta1.NodeMetrics {
	metricsMap := make(map[string]metricsv1beta1.NodeMetrics)
	for _, nodeMetric := range nodeMetricsList.Items {
		metricsMap[nodeMetric.Name] = nodeMetric
	}
	logrus.Debug("Successfully mapped node metrics")
	return metricsMap
}

// collectSingleNodeMetrics collects metrics for a single node.
func (nc *NodeCollector) collectSingleNodeMetrics(
	ctx context.Context,
	node v1.Node,
	nodeMetrics metricsv1beta1.NodeMetrics,
) (models.EnhancedNodeMetrics, error) {
	if node.Labels == nil {
		node.Labels = make(map[string]string)
	}
	metrics := models.EnhancedNodeMetrics{
		Name:        node.Name,
		Labels:      node.Labels,
		Annotations: node.Annotations,
		Capacity: models.ResourceMetrics{
			CPU:     node.Status.Capacity.Cpu().MilliValue(),
			Memory:  node.Status.Capacity.Memory().Value(),
			Pods:    node.Status.Capacity.Pods().Value(),
			Storage: node.Status.Capacity.StorageEphemeral().Value(),
		},
		Allocatable: models.ResourceMetrics{
			CPU:     node.Status.Allocatable.Cpu().MilliValue(),
			Memory:  node.Status.Allocatable.Memory().Value(),
			Pods:    node.Status.Allocatable.Pods().Value(),
			Storage: node.Status.Allocatable.StorageEphemeral().Value(),
		},
		Usage: models.ResourceMetrics{
			CPU:    nodeMetrics.Usage.Cpu().MilliValue(),
			Memory: nodeMetrics.Usage.Memory().Value(),
		},
	}

	// Add node conditions
	for _, condition := range node.Status.Conditions {
		switch condition.Type {
		case v1.NodeReady:
			metrics.Conditions.Ready = condition.Status == v1.ConditionTrue
		case v1.NodeMemoryPressure:
			metrics.Conditions.MemoryPressure = condition.Status == v1.ConditionTrue
		case v1.NodeDiskPressure:
			metrics.Conditions.DiskPressure = condition.Status == v1.ConditionTrue
		case v1.NodePIDPressure:
			metrics.Conditions.PIDPressure = condition.Status == v1.ConditionTrue
		}
	}

	// Get node address
	var nodeAddress string
	for _, addr := range node.Status.Addresses {
		if addr.Type == v1.NodeInternalIP {
			nodeAddress = addr.Address
			break
		}
	}
	if nodeAddress == "" {
		logrus.Warnf("No valid IP found for node %s, some metrics will be unavailable", node.Name)
	} else {
		// Collect detailed metrics
		if detailedMetrics, err := nc.collectDetailedNodeMetrics(ctx, node); err != nil {
			logrus.Warnf("Failed to collect detailed metrics for node %s: %v", node.Name, err)
		} else {
			metrics.DetailedMetrics = detailedMetrics
		}

		// Collect hardware topology
		if topology, err := nc.collectHardwareTopology(ctx, nodeAddress); err != nil {
			logrus.Warnf("Failed to collect hardware topology for node %s: %v", node.Name, err)
		} else {
			metrics.HardwareTopology = topology
		}

		// Collect power metrics
		if powerMetrics, err := nc.collectPowerMetrics(ctx, nodeAddress); err != nil {
			logrus.Warnf("Failed to collect power metrics for node %s: %v", node.Name, err)
		} else {
			metrics.PowerMetrics = powerMetrics
		}
	}

	// Collect taints
	metrics.Taints = nc.collectNodeTaintsAndTolerations(node)

	// Collect lease information
	lease, err := nc.collectNodeLease(ctx, node.Name)
	if err != nil {
		logrus.Warnf("Failed to collect lease information for node %s: %v", node.Name, err)
	} else {
		metrics.Lease = lease
	}

	// Collect extended resources
	metrics.ExtendedResources = nc.collectExtendedResources(node)

	return metrics, nil
}

// collectDetailedNodeMetrics collects detailed metrics from kubelet API v1.30
func (nc *NodeCollector) collectDetailedNodeMetrics(ctx context.Context, node v1.Node) (models.NodeDetailedMetrics, error) {
	metrics := models.NodeDetailedMetrics{}

	// Add node info
	metrics.Info = nc.collectNodeInfo(&node)

	// Get node address
	var nodeAddress string
	for _, addr := range node.Status.Addresses {
		if addr.Type == v1.NodeInternalIP {
			nodeAddress = addr.Address
			break
		}
	}
	if nodeAddress == "" {
		return metrics, fmt.Errorf("no valid IP found for node %s", node.Name)
	}

	// Collect load average
	loadAvg, err := nc.collectLoadAverage(ctx, nodeAddress)
	if err != nil {
		logrus.Warnf("Failed to collect load average: %v", err)
	}
	metrics.LoadAverage = loadAvg

	// Collect network stats
	networkStats, err := nc.collectNetworkStats(ctx, nodeAddress)
	if err != nil {
		logrus.Warnf("Failed to collect network stats: %v", err)
	}
	metrics.Network.Interfaces = networkStats

	// Collect disk stats
	diskStats, err := nc.collectDiskStats(ctx, nodeAddress)
	if err != nil {
		logrus.Warnf("Failed to collect disk stats: %v", err)
	}
	metrics.DiskIO = diskStats

	// Collect inode stats
	inodeStats, err := nc.collectInodeStats(ctx, nodeAddress)
	if err != nil {
		logrus.Warnf("Failed to collect inode stats: %v", err)
	}
	metrics.Filesystem.Inodes = inodeStats

	// Collect Container metrics with enhanced v1.30 support
	containerStats, err := nc.collectContainerStats(ctx, nodeAddress)
	if err != nil {
		logrus.Warnf("Failed to collect container stats: %v", err)
	}
	metrics.Containers = containerStats

	// Collect Runtime metrics with enhanced v1.30 support
	runtimeMetrics, err := nc.collectRuntimeMetrics(ctx, nodeAddress)
	if err != nil {
		logrus.Warnf("Failed to collect runtime metrics: %v", err)
	}
	metrics.Runtime = &runtimeMetrics

	// Collect Process metrics with enhanced v1.30 support
	processMetrics, err := nc.collectProcessMetrics(ctx, nodeAddress)
	if err != nil {
		logrus.Warnf("Failed to collect process metrics: %v", err)
	}
	metrics.Process = &processMetrics

	// Add new metric collection
	systemMetrics, err := nc.collectSystemMetrics(ctx, nodeAddress)
	if err != nil {
		logrus.Warnf("Failed to collect system metrics: %v", err)
	}
	metrics.System = &systemMetrics

	kubeletMetrics, err := nc.collectKubeletMetrics(ctx, nodeAddress)
	if err != nil {
		logrus.Warnf("Failed to collect kubelet metrics: %v", err)
	}
	metrics.Kubelet = &kubeletMetrics

	volumeMetrics, err := nc.collectVolumeMetrics(ctx, nodeAddress)
	if err != nil {
		logrus.Warnf("Failed to collect volume metrics: %v", err)
	}
	metrics.Volume = &volumeMetrics

	return metrics, nil
}

// collectCPUMetrics collects CPU metrics according to v1.30 spec
func (nc *NodeCollector) collectCPUMetrics(ctx context.Context, nodeAddress string) (models.CPUMetrics, error) {
	metricsURL := fmt.Sprintf("https://%s:%d/metrics/cadvisor", nodeAddress, 10250)
	metricFamilies, err := nc.fetchMetrics(ctx, metricsURL)
	if err != nil {
		return models.CPUMetrics{}, err
	}

	cpuMetrics := models.CPUMetrics{
		PerCoreUsage: make(map[string]uint64),
		Throttling:   models.ThrottlingMetrics{},
	}

	// Parse CPU throttling metrics
	if periods, ok := metricFamilies["container_cpu_cfs_periods_total"]; ok {
		for _, metric := range periods.Metric {
			if metric.Counter != nil {
				cpuMetrics.Throttling.Periods = uint64(metric.Counter.GetValue())
			}
		}
	}

	if throttled, ok := metricFamilies["container_cpu_cfs_throttled_periods_total"]; ok {
		for _, metric := range throttled.Metric {
			if metric.Counter != nil {
				cpuMetrics.Throttling.ThrottledPeriods = uint64(metric.Counter.GetValue())
			}
		}
	}

	if throttledTime, ok := metricFamilies["container_cpu_cfs_throttled_seconds_total"]; ok {
		for _, metric := range throttledTime.Metric {
			if metric.Counter != nil {
				cpuMetrics.Throttling.ThrottledTime = uint64(metric.Counter.GetValue() * 1e9) // Convert to nanoseconds
			}
		}
	}

	// Parse CPU usage metrics
	if cpuUsage, ok := metricFamilies["container_cpu_usage_seconds_total"]; ok {
		for _, metric := range cpuUsage.Metric {
			if metric.Counter != nil {
				cpuMetrics.UsageTotal = uint64(metric.Counter.GetValue() * 1e9) // Convert to nanoseconds
			}
		}
	}

	// Parse CPU scheduler metrics
	if scheduler, ok := metricFamilies["node_schedstat"]; ok {
		for _, metric := range scheduler.Metric {
			if metric.Counter != nil {
				cpuMetrics.SchedulerStats.RunQueueLength = uint64(metric.Counter.GetValue())
			}
		}
	}

	return cpuMetrics, nil
}

// collectRuntimeMetrics collects container runtime metrics according to v1.30 spec
func (nc *NodeCollector) collectRuntimeMetrics(ctx context.Context, nodeAddress string) (models.RuntimeMetrics, error) {
	metricsURL := fmt.Sprintf("https://%s:%d/metrics/probes", nodeAddress, 10250)
	metricFamilies, err := nc.fetchMetrics(ctx, metricsURL)
	if err != nil {
		return models.RuntimeMetrics{}, err
	}

	runtimeMetrics := models.RuntimeMetrics{
		Operations: make(map[string]models.RuntimeOperation),
	}

	// Parse runtime operations
	if operations, ok := metricFamilies["runtime_operations_total"]; ok {
		for _, metric := range operations.Metric {
			if metric.Counter != nil {
				opType := getLabel(metric, "operation_type")
				if opType != "" {
					runtimeMetrics.Operations[opType] = models.RuntimeOperation{
						Count: uint64(metric.Counter.GetValue()),
						Type:  opType,
					}
				}
			}
		}
	}

	// Parse runtime errors
	if errors, ok := metricFamilies["runtime_operations_errors_total"]; ok {
		for _, metric := range errors.Metric {
			if metric.Counter != nil {
				opType := getLabel(metric, "operation_type")
				if op, exists := runtimeMetrics.Operations[opType]; exists {
					op.Errors = uint64(metric.Counter.GetValue())
					runtimeMetrics.Operations[opType] = op
				}
			}
		}
	}

	return runtimeMetrics, nil
}

// Helper function to get label value
func getLabel(metric *dto.Metric, name string) string {
	for _, label := range metric.Label {
		if label.GetName() == name {
			return label.GetValue()
		}
	}
	return ""
}

func (nc *NodeCollector) collectMemoryMetrics(ctx context.Context, nodeAddress string) (models.MemoryMetrics, error) {
	metricsURL := fmt.Sprintf("https://%s:%d/metrics/cadvisor", nodeAddress, 10250)
	metricFamilies, err := nc.fetchMetrics(ctx, metricsURL)
	if err != nil {
		return models.MemoryMetrics{}, err
	}

	memoryMetrics := models.MemoryMetrics{}

	// Parse memory usage
	if memUsage, ok := metricFamilies["container_memory_usage_bytes"]; ok {
		for _, metric := range memUsage.Metric {
			if metric.Gauge != nil {
				memoryMetrics.Used = uint64(metric.Gauge.GetValue())
			}
		}
	}

	// Parse page faults
	if pageFaults, ok := metricFamilies["container_memory_page_faults_total"]; ok {
		for _, metric := range pageFaults.Metric {
			if metric.Counter != nil {
				memoryMetrics.PageFaults = uint64(metric.Counter.GetValue())
			}
		}
	}

	// Parse major page faults
	if majorPageFaults, ok := metricFamilies["container_memory_major_page_faults_total"]; ok {
		for _, metric := range majorPageFaults.Metric {
			if metric.Counter != nil {
				memoryMetrics.MajorPageFaults = uint64(metric.Counter.GetValue())
			}
		}
	}

	// Parse memory pressure
	if pressure, ok := metricFamilies["node_memory_pressure"]; ok {
		for _, metric := range pressure.Metric {
			if metric.Gauge != nil {
				if metric.Gauge.GetValue() > 0 {
					memoryMetrics.PressureLevel = "high"
				} else {
					memoryMetrics.PressureLevel = "normal"
				}
			}
		}
	}

	return memoryMetrics, nil
}

// Helper method to fetch metrics
func (nc *NodeCollector) fetchMetrics(ctx context.Context, metricsURL string) (map[string]*dto.MetricFamily, error) {
	req, err := http.NewRequestWithContext(ctx, "GET", metricsURL, nil)
	if err != nil {
		return nil, fmt.Errorf("failed to create request: %w", err)
	}

	req.Header.Add("Authorization", "Bearer "+nc.bearerToken)

	resp, err := nc.httpClient.Do(req)
	if err != nil {
		return nil, fmt.Errorf("failed to fetch metrics: %w", err)
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		return nil, fmt.Errorf("received non-200 status: %d", resp.StatusCode)
	}

	var parser expfmt.TextParser
	return parser.TextToMetricFamilies(resp.Body)
}

// getBearerToken retrieves the token from a file or environment variable.
func (nc *NodeCollector) getBearerToken() (string, error) {
	// First, try to read from file
	if nc.config.VegaBearerTokenPath != "" {
		tokenBytes, err := os.ReadFile(nc.config.VegaBearerTokenPath)
		if err == nil {
			token := strings.TrimSpace(string(tokenBytes))
			logrus.Debug("Successfully read Service Account bearer token from file")
			return token, nil
		}
		logrus.Printf("Failed to read bearer token from file, defaulting to BEARER_TOKEN environment variable: %v", err)
	}

	// If file read failed or no file path was provided, try environment variable
	token := strings.TrimSpace(os.Getenv("BEARER_TOKEN"))
	if token != "" {
		logrus.Debug("Successfully read bearer token from environment variable")
		return token, nil
	}

	// If both file and environment variable failed, return an error
	return "", errors.New("bearer token not found in file or environment")
}

// parseCPUUsage parses CPU usage from the metric family.
func (nc *NodeCollector) parseCPUUsage(family *dto.MetricFamily) int64 {
	var totalCPUUsage float64
	for _, metric := range family.Metric {
		mode := ""
		for _, label := range metric.Label {
			if label.GetName() == "mode" {
				mode = label.GetValue()
				break
			}
		}
		if mode == "user" || mode == "system" {
			if metric.Counter != nil && metric.Counter.Value != nil {
				totalCPUUsage += metric.Counter.GetValue()
			}
		}
	}
	logrus.Debug("Successfully parsed CPU usage metrics")
	return int64(totalCPUUsage * 1000) // Convert to millicores
}

func (nc *NodeCollector) collectNetworkMetrics(ctx context.Context, nodeAddress string) (models.NetworkMetrics, error) {
	metricsURL := fmt.Sprintf("https://%s:%d/metrics/cadvisor", nodeAddress, 10250)
	metricFamilies, err := nc.fetchMetrics(ctx, metricsURL)
	if err != nil {
		return models.NetworkMetrics{}, err
	}

	networkMetrics := models.NetworkMetrics{
		Interfaces: make([]models.InterfaceStats, 0),
		Summary:    models.NetworkSummary{},
	}

	// Parse network metrics
	if rxBytes, ok := metricFamilies["container_network_receive_bytes_total"]; ok {
		for _, metric := range rxBytes.Metric {
			if metric.Counter != nil {
				networkMetrics.Summary.RxBytesTotal += uint64(metric.Counter.GetValue())
			}
		}
	}

	if txBytes, ok := metricFamilies["container_network_transmit_bytes_total"]; ok {
		for _, metric := range txBytes.Metric {
			if metric.Counter != nil {
				networkMetrics.Summary.TxBytesTotal += uint64(metric.Counter.GetValue())
			}
		}
	}

	return networkMetrics, nil
}

func (nc *NodeCollector) collectDiskMetrics(ctx context.Context, nodeAddress string) (models.DiskMetrics, error) {
	metricsURL := fmt.Sprintf("https://%s:%d/metrics/cadvisor", nodeAddress, 10250)
	metricFamilies, err := nc.fetchMetrics(ctx, metricsURL)
	if err != nil {
		return models.DiskMetrics{}, err
	}

	diskMetrics := models.DiskMetrics{}

	// Parse disk metrics
	if readBytes, ok := metricFamilies["container_fs_reads_bytes_total"]; ok {
		for _, metric := range readBytes.Metric {
			if metric.Counter != nil {
				diskMetrics.ReadBytes += int64(metric.Counter.GetValue())
			}
		}
	}

	if writeBytes, ok := metricFamilies["container_fs_writes_bytes_total"]; ok {
		for _, metric := range writeBytes.Metric {
			if metric.Counter != nil {
				diskMetrics.WriteBytes += int64(metric.Counter.GetValue())
			}
		}
	}

	return diskMetrics, nil
}

func (nc *NodeCollector) collectProcessMetrics(ctx context.Context, nodeAddress string) (models.ProcessMetrics, error) {
	metricsURL := fmt.Sprintf("https://%s:%d/metrics/cadvisor", nodeAddress, 10250)
	metricFamilies, err := nc.fetchMetrics(ctx, metricsURL)
	if err != nil {
		return models.ProcessMetrics{}, err
	}

	processMetrics := models.ProcessMetrics{}

	// Parse process metrics
	if processCount, ok := metricFamilies["process_total"]; ok {
		for _, metric := range processCount.Metric {
			if metric.Gauge != nil {
				processMetrics.ProcessCount = int(metric.Gauge.GetValue())
				break
			}
		}
	}

	return processMetrics, nil
}

func (nc *NodeCollector) collectNodeMetrics(ctx context.Context, node v1.Node) (*models.NodeMetrics, error) {
	var nodeAddress string
	for _, addr := range node.Status.Addresses {
		if addr.Type == v1.NodeInternalIP {
			nodeAddress = addr.Address
			break
		}
	}

	metrics := &models.NodeMetrics{
		Name:        node.Name,
		Capacity:    nc.parseResourceList(node.Status.Capacity),
		Allocatable: nc.parseResourceList(node.Status.Allocatable),
		Usage:       models.ResourceMetrics{},
	}

	// Get metrics from kubelet API
	kubeletMetrics, err := nc.getKubeletMetrics(ctx, nodeAddress)
	if err != nil {
		return nil, fmt.Errorf("failed to get kubelet metrics: %w", err)
	}

	metrics.Usage = kubeletMetrics
	return metrics, nil
}

func (nc *NodeCollector) getKubeletMetrics(ctx context.Context, nodeAddress string) (models.ResourceMetrics, error) {
	metricsURL := fmt.Sprintf("https://%s:10250/metrics/resource", nodeAddress)

	// Create request with service account token
	req, err := http.NewRequestWithContext(ctx, "GET", metricsURL, nil)
	if err != nil {
		return models.ResourceMetrics{}, err
	}

	token, err := os.ReadFile("/var/run/secrets/kubernetes.io/serviceaccount/token")
	if err != nil {
		return models.ResourceMetrics{}, err
	}
	req.Header.Set("Authorization", "Bearer "+string(token))

	// Make request with TLS skip verify
	client := &http.Client{
		Transport: &http.Transport{
			TLSClientConfig: &tls.Config{InsecureSkipVerify: true},
		},
	}

	resp, err := client.Do(req)
	if err != nil {
		return models.ResourceMetrics{}, err
	}
	defer resp.Body.Close()

	// Parse metrics
	var parser expfmt.TextParser
	metricFamilies, err := parser.TextToMetricFamilies(resp.Body)
	if err != nil {
		return models.ResourceMetrics{}, err
	}

	metrics := models.ResourceMetrics{}

	// Extract CPU and memory metrics
	if cpuMetric := metricFamilies["node_cpu_usage_seconds_total"]; cpuMetric != nil {
		for _, m := range cpuMetric.Metric {
			metrics.CPU = int64(*m.Counter.Value * 1000) // Convert to millicores
		}
	}

	if memMetric := metricFamilies["node_memory_working_set_bytes"]; memMetric != nil {
		for _, m := range memMetric.Metric {
			metrics.Memory = int64(*m.Gauge.Value)
		}
	}

	return metrics, nil
}

// parseResourceList converts Kubernetes resource list to ResourceMetrics
func (nc *NodeCollector) parseResourceList(rl v1.ResourceList) models.ResourceMetrics {
	metrics := models.ResourceMetrics{}

	// CPU is reported in cores, convert to millicores
	if cpu, ok := rl[v1.ResourceCPU]; ok {
		metrics.CPU = cpu.MilliValue()
	}

	// Memory is reported in bytes
	if memory, ok := rl[v1.ResourceMemory]; ok {
		metrics.Memory = memory.Value()
	}

	// Persistent Storage
	if storage, ok := rl[v1.ResourceStorage]; ok {
		metrics.Storage = storage.Value()
	}

	// Ephemeral storage should be tracked separately from persistent storage
	if ephemeralStorage, ok := rl[v1.ResourceEphemeralStorage]; ok {
		metrics.EphemeralStorage = ephemeralStorage.Value()
	}

	// Add pods resource if available
	if pods, ok := rl[v1.ResourcePods]; ok {
		metrics.Pods = pods.Value()
	}

	return metrics
}

func (nc *NodeCollector) collectFilesystemMetrics(ctx context.Context, nodeAddress string) (models.FSMetrics, error) {
	metricsURL := fmt.Sprintf("https://%s:%d/metrics/cadvisor", nodeAddress, 10250)
	metricFamilies, err := nc.fetchMetrics(ctx, metricsURL)
	if err != nil {
		return models.FSMetrics{}, err
	}

	fsMetrics := models.FSMetrics{
		DeviceStats: make(map[string]models.FSStats),
	}

	if fs, ok := metricFamilies["container_fs_usage_bytes"]; ok {
		for _, metric := range fs.Metric {
			if metric.Gauge != nil {
				fsMetrics.UsedBytes = uint64(metric.Gauge.GetValue())
			}
		}
	}

	return fsMetrics, nil
}

func (nc *NodeCollector) collectContainerStats(ctx context.Context, nodeAddress string) (models.ContainerStats, error) {
	metricsURL := fmt.Sprintf("https://%s:%d/metrics/cadvisor", nodeAddress, 10250)
	metricFamilies, err := nc.fetchMetrics(ctx, metricsURL)
	if err != nil {
		return models.ContainerStats{}, err
	}

	stats := models.ContainerStats{
		PerContainer: make(map[string]models.ContainerMetrics),
	}

	// Basic container count from running containers
	if count, ok := metricFamilies["container_last_seen"]; ok {
		stats.RunningCount = len(count.Metric)
	}

	return stats, nil
}

func (nc *NodeCollector) collectSystemMetrics(ctx context.Context, nodeAddress string) (models.SystemMetrics, error) {
	metricsURL := fmt.Sprintf("https://%s:%d/metrics", nodeAddress, 10250)
	metricFamilies, err := nc.fetchMetrics(ctx, metricsURL)
	if err != nil {
		return models.SystemMetrics{}, err
	}

	systemMetrics := models.SystemMetrics{}

	// Parse system metrics
	if uptime, ok := metricFamilies["node_boot_time_seconds"]; ok {
		for _, metric := range uptime.Metric {
			if metric.Gauge != nil {
				systemMetrics.BootTimeSeconds = metric.Gauge.GetValue()
			}
		}
	}

	// Add more system metric parsing...

	return systemMetrics, nil
}

func (nc *NodeCollector) collectKubeletMetrics(ctx context.Context, nodeAddress string) (models.KubeletMetrics, error) {
	metricsURL := fmt.Sprintf("https://%s:%d/metrics", nodeAddress, 10250)
	metricFamilies, err := nc.fetchMetrics(ctx, metricsURL)
	if err != nil {
		return models.KubeletMetrics{}, err
	}

	kubeletMetrics := models.KubeletMetrics{
		OperationLatency:  make(map[string]models.LatencyMetric),
		RuntimeOperations: make(map[string]models.RuntimeOpMetric),
	}

	// Parse kubelet metrics
	if latency, ok := metricFamilies["kubelet_pod_start_duration_seconds"]; ok {
		// Parse pod start latency metrics
		kubeletMetrics.PodStartLatency = nc.parseLatencyMetric(latency)
	}

	// Add more kubelet metric parsing...

	return kubeletMetrics, nil
}

func (nc *NodeCollector) parseLatencyMetric(family *dto.MetricFamily) models.LatencyMetric {
	latencyMetric := models.LatencyMetric{}

	for _, metric := range family.Metric {
		if metric.Summary != nil {
			for _, quantile := range metric.Summary.Quantile {
				switch *quantile.Quantile {
				case 0.50:
					latencyMetric.P50 = *quantile.Value
				case 0.90:
					latencyMetric.P90 = *quantile.Value
				case 0.99:
					latencyMetric.P99 = *quantile.Value
				}
			}
			if metric.Summary.SampleCount != nil {
				latencyMetric.Count = int64(*metric.Summary.SampleCount)
			}
		}
	}

	return latencyMetric
}

func (nc *NodeCollector) collectVolumeMetrics(ctx context.Context, nodeAddress string) (models.VolumeMetrics, error) {
	metricsURL := fmt.Sprintf("https://%s:%d/metrics", nodeAddress, 10250)
	metricFamilies, err := nc.fetchMetrics(ctx, metricsURL)
	if err != nil {
		return models.VolumeMetrics{}, err
	}

	volumeMetrics := models.VolumeMetrics{
		OperationLatency: make(map[string]models.LatencyMetric),
	}

	// Parse volume metrics
	if attachCount, ok := metricFamilies["volume_manager_total_volumes"]; ok {
		for _, metric := range attachCount.Metric {
			if metric.Gauge != nil {
				volumeMetrics.InUseCount = uint64(metric.Gauge.GetValue())
			}
		}
	}

	return volumeMetrics, nil
}

func (nc *NodeCollector) collectNodeInfo(node *v1.Node) models.NodeInfo {
	return models.NodeInfo{
		Architecture:            node.Status.NodeInfo.Architecture,
		ContainerRuntimeVersion: node.Status.NodeInfo.ContainerRuntimeVersion,
		KernelVersion:           node.Status.NodeInfo.KernelVersion,
		OSImage:                 node.Status.NodeInfo.OSImage,
		KubeletVersion:          node.Status.NodeInfo.KubeletVersion,
	}
}

func (nc *NodeCollector) collectLoadAverage(ctx context.Context, nodeAddress string) (models.LoadAverageStats, error) {
	metricsURL := fmt.Sprintf("https://%s:%d/metrics/cadvisor", nodeAddress, 10250)
	metrics, err := nc.fetchMetrics(ctx, metricsURL)
	if err != nil {
		return models.LoadAverageStats{}, err
	}

	var result models.LoadAverageStats
	if metric, ok := metrics["node_load1"]; ok && len(metric.Metric) > 0 {
		result.Load1 = metric.Metric[0].Gauge.GetValue()
	}
	if metric, ok := metrics["node_load5"]; ok && len(metric.Metric) > 0 {
		result.Load5 = metric.Metric[0].Gauge.GetValue()
	}
	if metric, ok := metrics["node_load15"]; ok && len(metric.Metric) > 0 {
		result.Load15 = metric.Metric[0].Gauge.GetValue()
	}

	return result, nil
}

func (nc *NodeCollector) collectNetworkStats(ctx context.Context, nodeAddress string) ([]models.InterfaceStats, error) {
	metricsURL := fmt.Sprintf("https://%s:%d/metrics/cadvisor", nodeAddress, 10250)
	metrics, err := nc.fetchMetrics(ctx, metricsURL)
	if err != nil {
		return nil, err
	}

	interfaces := make(map[string]*models.InterfaceStats)

	// Define processMetric before using it
	processMetric := func(metricFamily *dto.MetricFamily, setter func(*models.InterfaceStats, float64)) {
		if metricFamily == nil {
			return
		}
		for _, metric := range metricFamily.Metric {
			var ifName string
			for _, label := range metric.Label {
				if label.GetName() == "interface" {
					ifName = label.GetValue()
					break
				}
			}
			if ifName == "" {
				continue
			}
			if _, exists := interfaces[ifName]; !exists {
				interfaces[ifName] = &models.InterfaceStats{InterfaceName: ifName}
			}
			if metric.Counter != nil {
				setter(interfaces[ifName], metric.Counter.GetValue())
			}
		}
	}

	// Process all network metrics
	metricMappings := map[string]func(*models.InterfaceStats, float64){
		"container_network_receive_bytes_total": func(stats *models.InterfaceStats, val float64) {
			stats.RxBytes = uint64(val)
		},
		"container_network_receive_packets_total": func(stats *models.InterfaceStats, val float64) {
			stats.RxPackets = uint64(val)
		},
		"container_network_receive_errors_total": func(stats *models.InterfaceStats, val float64) {
			stats.RxErrors = uint64(val)
		},
		"container_network_receive_packets_dropped_total": func(stats *models.InterfaceStats, val float64) {
			stats.RxDropped = uint64(val)
		},
		"container_network_transmit_bytes_total": func(stats *models.InterfaceStats, val float64) {
			stats.TxBytes = uint64(val)
		},
		"container_network_transmit_packets_total": func(stats *models.InterfaceStats, val float64) {
			stats.TxPackets = uint64(val)
		},
		"container_network_transmit_errors_total": func(stats *models.InterfaceStats, val float64) {
			stats.TxErrors = uint64(val)
		},
		"container_network_transmit_packets_dropped_total": func(stats *models.InterfaceStats, val float64) {
			stats.TxDropped = uint64(val)
		},
	}

	// Process all metrics using the mappings
	for metricName, setter := range metricMappings {
		if metricFamily, ok := metrics[metricName]; ok {
			processMetric(metricFamily, setter)
		}
	}

	// Convert map to slice
	result := make([]models.InterfaceStats, 0, len(interfaces))
	for _, stats := range interfaces {
		result = append(result, *stats)
	}

	logrus.Debugf("Successfully collected network stats for %d interfaces", len(result))
	return result, nil
}

func (nc *NodeCollector) collectDiskStats(ctx context.Context, nodeAddress string) ([]models.NodeDiskStats, error) {
	metricsURL := fmt.Sprintf("https://%s:%d/metrics/cadvisor", nodeAddress, 10250)
	metrics, err := nc.fetchMetrics(ctx, metricsURL)
	if err != nil {
		return nil, err
	}

	devices := make(map[string]*models.NodeDiskStats)

	// Define processMetric with access to devices map
	processMetric := func(metricFamily *dto.MetricFamily, setter func(*models.NodeDiskStats, float64)) {
		if metricFamily == nil {
			return
		}
		for _, metric := range metricFamily.Metric {
			var deviceName string
			for _, label := range metric.Label {
				if label.GetName() == "device" {
					deviceName = label.GetValue()
					break
				}
			}
			if deviceName == "" {
				continue
			}
			if _, exists := devices[deviceName]; !exists {
				devices[deviceName] = &models.NodeDiskStats{Device: deviceName}
			}
			if metric.Counter != nil {
				setter(devices[deviceName], metric.Counter.GetValue())
			}
		}
	}

	// Process all disk metrics
	metricMappings := map[string]func(*models.NodeDiskStats, float64){
		"node_disk_reads_completed_total": func(stats *models.NodeDiskStats, val float64) {
			stats.ReadOps = uint64(val)
		},
		"node_disk_writes_completed_total": func(stats *models.NodeDiskStats, val float64) {
			stats.WriteOps = uint64(val)
		},
		"node_disk_read_bytes_total": func(stats *models.NodeDiskStats, val float64) {
			stats.ReadBytes = uint64(val)
		},
		"node_disk_written_bytes_total": func(stats *models.NodeDiskStats, val float64) {
			stats.WriteBytes = uint64(val)
		},
		"node_disk_read_time_seconds_total": func(stats *models.NodeDiskStats, val float64) {
			stats.ReadLatency = uint64(val * 1000) // Convert to milliseconds
		},
		"node_disk_write_time_seconds_total": func(stats *models.NodeDiskStats, val float64) {
			stats.WriteLatency = uint64(val * 1000) // Convert to milliseconds
		},
		"node_disk_io_now": func(stats *models.NodeDiskStats, val float64) {
			stats.IoInProgress = uint64(val)
		},
		"node_disk_io_time_seconds_total": func(stats *models.NodeDiskStats, val float64) {
			stats.IoTime = uint64(val * 1000) // Convert to milliseconds
		},
		"node_disk_io_time_weighted_seconds_total": func(stats *models.NodeDiskStats, val float64) {
			stats.WeightedIoTime = uint64(val * 1000) // Convert to milliseconds
		},
	}

	for metricName, setter := range metricMappings {
		if metricFamily, ok := metrics[metricName]; ok {
			processMetric(metricFamily, setter)
		}
	}

	// Convert map to slice
	result := make([]models.NodeDiskStats, 0, len(devices))
	for _, stats := range devices {
		result = append(result, *stats)
	}

	logrus.Debugf("Successfully collected disk stats for %d devices", len(result))
	return result, nil
}

func (nc *NodeCollector) collectInodeStats(ctx context.Context, nodeAddress string) (models.InodeStats, error) {
	metricsURL := fmt.Sprintf("https://%s:%d/metrics/cadvisor", nodeAddress, 10250)
	metrics, err := nc.fetchMetrics(ctx, metricsURL)
	if err != nil {
		return models.InodeStats{}, err
	}

	var result models.InodeStats

	if metric, ok := metrics["node_filesystem_files"]; ok && len(metric.Metric) > 0 {
		result.Total = uint64(metric.Metric[0].Gauge.GetValue())
	}
	if metric, ok := metrics["node_filesystem_files_free"]; ok && len(metric.Metric) > 0 {
		result.Free = uint64(metric.Metric[0].Gauge.GetValue())
	}
	result.Used = result.Total - result.Free

	return result, nil
}

func (nc *NodeCollector) collectHardwareTopology(ctx context.Context, nodeAddress string) (*models.HardwareTopology, error) {
	metricsURL := fmt.Sprintf("https://%s:%d/metrics/resource", nodeAddress, 10250)
	metricFamilies, err := nc.fetchMetrics(ctx, metricsURL)
	if err != nil {
		return nil, err
	}

	topology := &models.HardwareTopology{
		NUMANodes: make([]models.NUMANode, 0),
	}

	// Parse CPU topology information
	if cpuInfo, ok := metricFamilies["node_cpu_info"]; ok {
		for _, metric := range cpuInfo.Metric {
			for _, label := range metric.Label {
				switch label.GetName() {
				case "socket":
					topology.Sockets++
				case "core":
					topology.Cores++
				case "thread":
					topology.Threads++
				}
			}
		}
	}

	// Parse NUMA topology
	if numaInfo, ok := metricFamilies["node_numa_info"]; ok {
		numaNodes := make(map[int32]models.NUMANode)
		for _, metric := range numaInfo.Metric {
			var numaID int32
			for _, label := range metric.Label {
				if label.GetName() == "numa_id" {
					numaID = int32(metric.Gauge.GetValue())
					break
				}
			}
			if _, exists := numaNodes[numaID]; !exists {
				numaNodes[numaID] = models.NUMANode{ID: numaID}
			}
		}
		for _, node := range numaNodes {
			topology.NUMANodes = append(topology.NUMANodes, node)
		}
	}

	return topology, nil
}

func (nc *NodeCollector) collectPowerMetrics(ctx context.Context, nodeAddress string) (*models.PowerMetrics, error) {
	metricsURL := fmt.Sprintf("https://%s:%d/metrics/resource", nodeAddress, 10250)
	metricFamilies, err := nc.fetchMetrics(ctx, metricsURL)
	if err != nil {
		return nil, err
	}

	powerMetrics := &models.PowerMetrics{}

	// Parse power consumption metrics if available
	if power, ok := metricFamilies["node_power_watts"]; ok {
		for _, metric := range power.Metric {
			if metric.Gauge != nil {
				powerMetrics.CurrentWatts = metric.Gauge.GetValue()
				break
			}
		}
	}

	return powerMetrics, nil
}

func (nc *NodeCollector) collectNodeTaintsAndTolerations(node v1.Node) []models.NodeTaint {
	taints := make([]models.NodeTaint, 0, len(node.Spec.Taints))
	for _, taint := range node.Spec.Taints {
		nodeTaint := models.NodeTaint{
			Key:    taint.Key,
			Value:  taint.Value,
			Effect: string(taint.Effect),
		}
		if taint.TimeAdded != nil {
			timeAdded := taint.TimeAdded.Time
			nodeTaint.TimeAdded = &timeAdded
		}
		taints = append(taints, nodeTaint)
	}
	return taints
}

func (nc *NodeCollector) collectNodeLease(ctx context.Context, nodeName string) (*models.NodeLease, error) {
	lease, err := nc.clientset.CoordinationV1().Leases(v1.NamespaceNodeLease).Get(ctx, nodeName, metav1.GetOptions{})
	if err != nil {
		return nil, err
	}

	renewTime := lease.Spec.RenewTime.Time
	acquireTime := lease.Spec.AcquireTime.Time
	return &models.NodeLease{
		HolderIdentity:       *lease.Spec.HolderIdentity,
		LeaseDurationSeconds: int32(*lease.Spec.LeaseDurationSeconds),
		AcquireTime:          &acquireTime,
		RenewTime:            &renewTime,
	}, nil
}

func (nc *NodeCollector) collectExtendedResources(node v1.Node) map[string]models.ExtendedResource {
	extended := make(map[string]models.ExtendedResource)

	for resourceName, quantity := range node.Status.Capacity {
		// Skip standard resources
		if isStandardResource(resourceName) {
			continue
		}

		qty := quantity.DeepCopy()
		alloc := node.Status.Allocatable[resourceName].DeepCopy()
		extended[string(resourceName)] = models.ExtendedResource{
			Name:        string(resourceName),
			Capacity:    qty.String(),
			Allocatable: alloc.String(),
		}
	}

	return extended
}

// Helper function to identify standard resources
func isStandardResource(resourceName v1.ResourceName) bool {
	switch resourceName {
	case v1.ResourceCPU, v1.ResourceMemory, v1.ResourceStorage,
		v1.ResourceEphemeralStorage, v1.ResourcePods:
		return true
	default:
		return false
	}
}

func (nc *NodeCollector) collectVolumeHealthMetrics(ctx context.Context, nodeAddress string) ([]models.VolumeHealthMetrics, error) {
	var healthMetrics []models.VolumeHealthMetrics

	pods, err := nc.clientset.CoreV1().Pods("").List(ctx, metav1.ListOptions{
		FieldSelector: fmt.Sprintf("spec.nodeName=%s", nodeAddress),
	})
	if err != nil {
		return nil, err
	}

	for _, pod := range pods.Items {
		for _, volume := range pod.Spec.Volumes {
			if volume.PersistentVolumeClaim != nil {
				health := models.VolumeHealthMetrics{
					VolumeName:    volume.Name,
					PodName:       pod.Name,
					Namespace:     pod.Namespace,
					State:         "Healthy", // Default state
					LastCheckTime: time.Now(),
				}

				// Check mount status
				if mountPath := nc.getMountPath(pod, volume.Name); mountPath != "" {
					health.FsIntegrity = true
					health.IoPerformance = "normal"
				} else {
					health.State = "Warning"
					health.ErrorMessage = "Volume not mounted"
					health.RepairRequired = true
				}

				healthMetrics = append(healthMetrics, health)
			}
		}
	}

	return healthMetrics, nil
}

func (nc *NodeCollector) collectVolumeAttachmentMetrics(ctx context.Context, nodeName string) ([]models.VolumeAttachmentMetrics, error) {
	var attachments []models.VolumeAttachmentMetrics

	volumeAttachments, err := nc.clientset.StorageV1().VolumeAttachments().List(ctx, metav1.ListOptions{
		FieldSelector: fmt.Sprintf("spec.nodeName=%s", nodeName),
	})
	if err != nil {
		return nil, err
	}

	for _, va := range volumeAttachments.Items {
		attachment := models.VolumeAttachmentMetrics{
			VolumeName:      *va.Spec.Source.PersistentVolumeName,
			AttachmentState: fmt.Sprintf("%v", va.Status.Attached),
			AttachTime:      va.CreationTimestamp.Time,
		}

		if va.Status.Attached {
			if va.Status.AttachmentMetadata != nil {
				attachment.DevicePath = va.Status.AttachmentMetadata["devicePath"]
			}
		} else if va.Status.AttachError != nil {
			attachment.AttachError = va.Status.AttachError.Message
		}

		attachments = append(attachments, attachment)
	}

	return attachments, nil
}

func (nc *NodeCollector) getMountPath(pod v1.Pod, volumeName string) string {
	for _, container := range pod.Spec.Containers {
		for _, mount := range container.VolumeMounts {
			if mount.Name == volumeName {
				return mount.MountPath
			}
		}
	}
	return ""
}

================
File: persistentvolume.go
================
// Copyright 2024 Vega Cloud, Inc.
//
// Use of this software is governed by the Business Source License
// included in the file licenses/BSL.txt.
//
// As of the Change Date specified in that file, in accordance with
// the Business Source License, use of this software will be governed
// by the Apache License, Version 2.0, included in the file
// licenses/APL.txt.
// File: pkg/collectors/persistentvolume.go

// Package collectors hosts the collection functions
package collectors

import (
	"context"
	"fmt"

	snapshots "github.com/kubernetes-csi/external-snapshotter/client/v6/clientset/versioned"
	v1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/client-go/kubernetes"

	"github.com/sirupsen/logrus"
	"github.com/vegacloud/kubernetes/metricsagent/pkg/config"
	"github.com/vegacloud/kubernetes/metricsagent/pkg/models"
)

// PersistentVolumeCollector collects metrics from Kubernetes persistent volumes.
type PersistentVolumeCollector struct {
	clientset      *kubernetes.Clientset
	snapshotClient *snapshots.Clientset
	config         *config.Config
}

// NewPersistentVolumeCollector creates a new PersistentVolumeCollector.
func NewPersistentVolumeCollector(clientset *kubernetes.Clientset, cfg *config.Config) *PersistentVolumeCollector {
	collector := &PersistentVolumeCollector{
		clientset: clientset,
		config:    cfg,
	}
	logrus.Debug("PersistentVolumeCollector created successfully")
	return collector
}

// CollectMetrics collects metrics from Kubernetes persistent volumes.
func (pvc *PersistentVolumeCollector) CollectMetrics(ctx context.Context) (interface{}, error) {
	pvs, err := pvc.clientset.CoreV1().PersistentVolumes().List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to list persistent volumes: %w", err)
	}
	logrus.Debugf("Successfully listed %d persistent volumes", len(pvs.Items))

	metrics := pvc.collectPVMetrics(pvs.Items)
	logrus.Debug("Successfully collected persistent volume metrics")
	return metrics, nil
}

// collectPVMetrics collects metrics from Kubernetes persistent volumes.
func (pvc *PersistentVolumeCollector) collectPVMetrics(pvs []v1.PersistentVolume) []models.PVMetric {
	pvMetrics := make([]models.PVMetric, 0, len(pvs))

	for _, pv := range pvs {
		if pv.Labels == nil {
			pv.Labels = make(map[string]string)
		}

		accessModes := make([]string, 0)
		for _, mode := range pv.Spec.AccessModes {
			accessModes = append(accessModes, string(mode))
		}

		metric := models.PVMetric{
			Name:          pv.Name,
			Capacity:      pv.Spec.Capacity.Storage().Value(),
			Phase:         string(pv.Status.Phase),
			StorageClass:  pv.Spec.StorageClassName,
			Labels:        pv.Labels,
			AccessModes:   accessModes,
			ReclaimPolicy: string(pv.Spec.PersistentVolumeReclaimPolicy),
			VolumeMode:    string(*pv.Spec.VolumeMode),
			Status: models.PVStatus{
				Phase:   string(pv.Status.Phase),
				Message: pv.Status.Message,
				Reason:  pv.Status.Reason,
			},
			MountOptions: pv.Spec.MountOptions,
		}

		if sc, err := pvc.clientset.StorageV1().StorageClasses().Get(context.Background(), pv.Spec.StorageClassName, metav1.GetOptions{}); err == nil {
			metric.VolumeBindingMode = string(*sc.VolumeBindingMode)
		}

		if pv.Spec.ClaimRef != nil {
			metric.BoundPVC = fmt.Sprintf("%s/%s", pv.Spec.ClaimRef.Namespace, pv.Spec.ClaimRef.Name)
		}

		if ann := pv.Annotations; ann != nil {
			if provisioner, ok := ann["pv.kubernetes.io/provisioned-by"]; ok {
				metric.StorageProvisioner = provisioner
			}
		}

		// Collect snapshot metrics
		snapshots, err := pvc.collectVolumeSnapshots(context.Background(), pv.Name)
		if err != nil {
			logrus.Warnf("Failed to collect snapshots for PV %s: %v", pv.Name, err)
		} else {
			metric.Snapshots = snapshots
		}

		pvMetrics = append(pvMetrics, metric)
	}

	return pvMetrics
}

func (pvc *PersistentVolumeCollector) collectVolumeSnapshots(ctx context.Context, pvName string) ([]models.VolumeSnapshotMetrics, error) {
	snapshots, err := pvc.snapshotClient.SnapshotV1().VolumeSnapshots("").List(ctx, metav1.ListOptions{
		LabelSelector: fmt.Sprintf("source-pv=%s", pvName),
	})
	if err != nil {
		return nil, err
	}

	var metrics []models.VolumeSnapshotMetrics
	for _, snap := range snapshots.Items {
		metric := models.VolumeSnapshotMetrics{
			Name:         snap.Name,
			Namespace:    snap.Namespace,
			SourcePVName: pvName,
			CreationTime: snap.CreationTimestamp.Time,
			ReadyToUse:   *snap.Status.ReadyToUse,
		}

		if snap.Status.RestoreSize != nil {
			metric.RestoreSize = snap.Status.RestoreSize.Value()
		}
		if snap.DeletionTimestamp != nil {
			metric.DeletionTime = &snap.DeletionTimestamp.Time
		}
		if snap.Status.Error != nil && snap.Status.Error.Message != nil {
			metric.Error = *snap.Status.Error.Message
		}

		metrics = append(metrics, metric)
	}

	return metrics, nil
}

================
File: persistentvolumeclaim.go
================
package collectors

import (
	"context"
	"fmt"

	v1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/client-go/kubernetes"

	"github.com/sirupsen/logrus"
	"github.com/vegacloud/kubernetes/metricsagent/pkg/config"
	"github.com/vegacloud/kubernetes/metricsagent/pkg/models"
)

// PersistentVolumeClaimCollector collects metrics from Kubernetes persistent volume claims.
type PersistentVolumeClaimCollector struct {
	clientset *kubernetes.Clientset
	config    *config.Config
}

// NewPersistentVolumeClaimCollector creates a new PersistentVolumeClaimCollector.
func NewPersistentVolumeClaimCollector(clientset *kubernetes.Clientset, cfg *config.Config) *PersistentVolumeClaimCollector {
	collector := &PersistentVolumeClaimCollector{
		clientset: clientset,
		config:    cfg,
	}
	logrus.Debug("PersistentVolumeClaimCollector created successfully")
	return collector
}

// CollectMetrics collects metrics from Kubernetes persistent volume claims.
func (pvcc *PersistentVolumeClaimCollector) CollectMetrics(ctx context.Context) (interface{}, error) {
	pvcs, err := pvcc.clientset.CoreV1().PersistentVolumeClaims("").List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to list persistent volume claims: %w", err)
	}
	logrus.Debugf("Successfully listed %d persistent volume claims", len(pvcs.Items))

	metrics := pvcc.collectPVCMetrics(pvcs.Items)
	logrus.Debug("Successfully collected persistent volume claim metrics")
	return metrics, nil
}

// collectPVCMetrics collects metrics from Kubernetes persistent volume claims.
func (pvcc *PersistentVolumeClaimCollector) collectPVCMetrics(pvcs []v1.PersistentVolumeClaim) []models.PVCMetric {
	pvcMetrics := make([]models.PVCMetric, 0, len(pvcs))

	for _, claim := range pvcs {
		if claim.Labels == nil {
			claim.Labels = make(map[string]string)
		}

		accessModes := make([]string, 0)
		for _, mode := range claim.Spec.AccessModes {
			accessModes = append(accessModes, string(mode))
		}

		conditions := make([]models.PVCCondition, 0)
		for _, cond := range claim.Status.Conditions {
			conditions = append(conditions, models.PVCCondition{
				Type:               string(cond.Type),
				Status:             string(cond.Status),
				Reason:             cond.Reason,
				Message:            cond.Message,
				LastTransitionTime: &cond.LastTransitionTime.Time,
			})
		}

		metric := models.PVCMetric{
			Name:             claim.Name,
			Namespace:        claim.Namespace,
			Phase:            string(claim.Status.Phase),
			Capacity:         claim.Status.Capacity.Storage().Value(),
			RequestedStorage: claim.Spec.Resources.Requests.Storage().Value(),
			Labels:           claim.Labels,
			AccessModes:      accessModes,
			VolumeMode:       string(*claim.Spec.VolumeMode),
			VolumeName:       claim.Spec.VolumeName,
			Status: models.PVCStatus{
				Phase:      string(claim.Status.Phase),
				Conditions: conditions,
			},
		}

		if claim.Spec.StorageClassName != nil {
			metric.StorageClass = *claim.Spec.StorageClassName
			if sc, err := pvcc.clientset.StorageV1().StorageClasses().Get(context.Background(), *claim.Spec.StorageClassName, metav1.GetOptions{}); err == nil {
				metric.VolumeBindingMode = string(*sc.VolumeBindingMode)
			}
		}

		if claim.Spec.VolumeName != "" {
			metric.BoundPV = claim.Spec.VolumeName
			if boundPV, err := pvcc.clientset.CoreV1().PersistentVolumes().Get(context.Background(), claim.Spec.VolumeName, metav1.GetOptions{}); err == nil {
				metric.MountOptions = boundPV.Spec.MountOptions
			}
		}

		if ann := claim.Annotations; ann != nil {
			if provisioner, ok := ann["volume.kubernetes.io/storage-provisioner"]; ok {
				metric.StorageProvisioner = provisioner
			}
		}

		if claim.Spec.Resources.Requests.Storage() != nil {
			expansion := &models.VolumeExpansionMetrics{
				CurrentSize:   claim.Status.Capacity.Storage().Value(),
				RequestedSize: claim.Spec.Resources.Requests.Storage().Value(),
				InProgress:    false,
			}

			for _, condition := range claim.Status.Conditions {
				if condition.Type == v1.PersistentVolumeClaimResizing {
					expansion.InProgress = true
					expansion.LastResizeTime = &condition.LastTransitionTime.Time
					expansion.ResizeStatus = string(condition.Status)
					expansion.FailureMessage = condition.Message
					break
				}
			}

			metric.Expansion = expansion
		}

		pvcMetrics = append(pvcMetrics, metric)
	}

	return pvcMetrics
}

================
File: pod.go
================
// Copyright 2024 Vega Cloud, Inc.
//
// Use of this software is governed by the Business Source License
// included in the file licenses/BSL.txt.
//
// As of the Change Date specified in that file, in accordance with
// the Business Source License, use of this software will be governed
// by the Apache License, Version 2.0, included in the file
// licenses/APL.txt.
// File: pkg/collectors/pod.go

// Package collectors hosts the collection functions
package collectors

import (
	"context"
	"crypto/tls"
	"fmt"
	"net/http"
	"os"
	"time"

	"github.com/prometheus/common/expfmt"
	"github.com/sirupsen/logrus"
	v1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/labels"
	"k8s.io/client-go/kubernetes"

	"github.com/vegacloud/kubernetes/metricsagent/pkg/config"
	"github.com/vegacloud/kubernetes/metricsagent/pkg/models"
)

// PodCollector collects metrics for all pods in the cluster
type PodCollector struct {
	clientset  *kubernetes.Clientset
	config     *config.Config
	httpClient *http.Client
}

// NewPodCollector creates a new PodCollector
func NewPodCollector(clientset *kubernetes.Clientset, cfg *config.Config) *PodCollector {
	return &PodCollector{
		clientset: clientset,
		config:    cfg,
		httpClient: &http.Client{
			Transport: &http.Transport{
				TLSClientConfig: &tls.Config{
					InsecureSkipVerify: true,
				},
			},
			Timeout: time.Second * 30,
		},
	}
}

// CollectMetrics collects metrics for all pods in the cluster
func (pc *PodCollector) CollectMetrics(ctx context.Context) (interface{}, error) {
	metrics, err := pc.CollectEnhancedPodMetrics(ctx)
	if err != nil {
		return nil, err
	}
	logrus.Debug("Successfully collected pod metrics")
	return metrics, nil
}

// CollectEnhancedPodMetrics collects metrics from Kubernetes pods.
func (pc *PodCollector) CollectEnhancedPodMetrics(ctx context.Context) ([]models.EnhancedPodMetrics, error) {
	pods, err := pc.clientset.CoreV1().Pods("").List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to list pods: %w", err)
	}
	logrus.Debugf("Successfully listed %d pods", len(pods.Items))

	enhancedPodMetrics := make([]models.EnhancedPodMetrics, 0, len(pods.Items))

	for _, pod := range pods.Items {
		metrics, err := pc.collectSinglePodMetrics(ctx, pod)
		if err != nil {
			logrus.Warnf("Failed to collect metrics for pod %s/%s: %v", pod.Namespace, pod.Name, err)
			continue
		}
		enhancedPodMetrics = append(enhancedPodMetrics, metrics)
	}

	logrus.Debug("Successfully collected enhanced pod metrics")
	return enhancedPodMetrics, nil
}

func (pc *PodCollector) collectSinglePodMetrics(ctx context.Context, pod v1.Pod) (models.EnhancedPodMetrics, error) {
	if pod.Labels == nil {
		pod.Labels = make(map[string]string)
	}

	metrics := models.EnhancedPodMetrics{
		PodMetrics: models.PodMetrics{
			Name:      pod.Name,
			Namespace: pod.Namespace,
			Phase:     string(pod.Status.Phase),
			Labels:    pod.Labels,
		},
		QoSClass: string(pod.Status.QOSClass),
		StartTime: func() *time.Time {
			if pod.Status.StartTime != nil {
				t := pod.Status.StartTime.Time
				return &t
			}
			return nil
		}(),
		Priority:          pod.Spec.Priority,
		PriorityClassName: pod.Spec.PriorityClassName,
		NodeName:          pod.Spec.NodeName,
		HostIP:            pod.Status.HostIP,
		NominatedNodeName: pod.Status.NominatedNodeName,
	}

	// Collect Pod IPs
	podIPs := make([]string, 0, len(pod.Status.PodIPs))
	for _, ip := range pod.Status.PodIPs {
		podIPs = append(podIPs, ip.IP)
	}
	metrics.PodIPs = podIPs

	// Collect Readiness Gates
	readinessGates := make([]models.PodReadinessGate, 0, len(pod.Spec.ReadinessGates))
	for _, gate := range pod.Spec.ReadinessGates {
		status := false
		for _, condition := range pod.Status.Conditions {
			if string(condition.Type) == string(gate.ConditionType) {
				status = condition.Status == v1.ConditionTrue
				break
			}
		}
		readinessGates = append(readinessGates, models.PodReadinessGate{
			ConditionType: string(gate.ConditionType),
			Status:        status,
		})
	}
	metrics.ReadinessGates = readinessGates

	// Collect Pod Conditions
	for _, condition := range pod.Status.Conditions {
		switch condition.Type {
		case v1.PodScheduled:
			metrics.Conditions.PodScheduled = condition.Status == v1.ConditionTrue
		case v1.PodInitialized:
			metrics.Conditions.Initialized = condition.Status == v1.ConditionTrue
		case v1.ContainersReady:
			metrics.Conditions.ContainersReady = condition.Status == v1.ConditionTrue
		case v1.PodReady:
			metrics.Conditions.Ready = condition.Status == v1.ConditionTrue
		}
	}

	// Collect Resource Metrics
	for _, container := range pod.Spec.Containers {
		metrics.Requests.CPU += container.Resources.Requests.Cpu().MilliValue()
		metrics.Requests.Memory += container.Resources.Requests.Memory().Value()
		metrics.Limits.CPU += container.Resources.Limits.Cpu().MilliValue()
		metrics.Limits.Memory += container.Resources.Limits.Memory().Value()
	}

	// Get Pod Metrics from Kubelet
	podMetrics, err := pc.getPodMetrics(ctx, &pod)
	if err != nil {
		return metrics, fmt.Errorf("failed to get pod metrics: %w", err)
	}

	metrics.Containers = pc.extractContainerMetrics(pod, podMetrics)
	metrics.TotalRestarts = pc.getTotalRestarts(pod)

	// Set completion time for completed pods
	if pod.Status.Phase == v1.PodSucceeded || pod.Status.Phase == v1.PodFailed {
		for _, containerStatus := range pod.Status.ContainerStatuses {
			if containerStatus.State.Terminated != nil {
				metrics.CompletionTime = &containerStatus.State.Terminated.FinishedAt.Time
				break
			}
		}
	}

	// Add annotations
	metrics.Annotations = pod.Annotations

	// Add volume mounts
	metrics.VolumeMounts = make([]models.VolumeMountMetrics, 0)
	for _, container := range pod.Spec.Containers {
		for _, volumeMount := range container.VolumeMounts {
			metrics.VolumeMounts = append(metrics.VolumeMounts, models.VolumeMountMetrics{
				Name:        volumeMount.Name,
				MountPath:   volumeMount.MountPath,
				ReadOnly:    volumeMount.ReadOnly,
				SubPath:     volumeMount.SubPath,
				SubPathExpr: volumeMount.SubPathExpr,
				MountPropagation: func() string {
					if volumeMount.MountPropagation != nil {
						return string(*volumeMount.MountPropagation)
					}
					return ""
				}(),
			})
		}
	}

	// Add image pull policy
	imagePullPolicies := make([]string, 0)
	for _, container := range pod.Spec.Containers {
		imagePullPolicies = append(imagePullPolicies, string(container.ImagePullPolicy))
	}
	// Use the most common pull policy, or "Mixed" if there are different policies
	if len(imagePullPolicies) > 0 {
		allSame := true
		for i := 1; i < len(imagePullPolicies); i++ {
			if imagePullPolicies[i] != imagePullPolicies[0] {
				allSame = false
				break
			}
		}
		if allSame {
			metrics.ImagePullPolicy = imagePullPolicies[0]
		} else {
			metrics.ImagePullPolicy = "Mixed"
		}
	}

	// Add service account information
	metrics.ServiceAccountName = pod.Spec.ServiceAccountName

	// Collect PDB metrics
	pdbMetrics, err := pc.collectPodDisruptionBudget(ctx, pod)
	if err != nil {
		logrus.Warnf("Failed to collect PDB metrics for pod %s/%s: %v", pod.Namespace, pod.Name, err)
	} else {
		metrics.DisruptionBudget = pdbMetrics
	}

	// Collect topology spread constraints
	metrics.TopologySpread = pc.collectTopologySpread(pod)

	// Collect pod overhead
	if pod.Spec.Overhead != nil {
		metrics.Overhead = &models.PodOverheadMetrics{
			CPU:    pod.Spec.Overhead.Cpu().String(),
			Memory: pod.Spec.Overhead.Memory().String(),
		}
	}

	// Collect scheduling gates
	for _, gate := range pod.Spec.SchedulingGates {
		metrics.SchedulingGates = append(metrics.SchedulingGates, models.PodSchedulingGate{
			Name:   gate.Name,
			Active: true,
		})
	}

	// Collect security context
	metrics.SecurityContext = pc.collectSecurityContext(pod)

	// Collect QoS details
	metrics.QoSDetails = pc.collectQoSDetails(pod)

	logrus.Debugf("Successfully collected metrics for pod %s/%s", pod.Namespace, pod.Name)
	return metrics, nil
}

func (pc *PodCollector) getPodMetrics(ctx context.Context, pod *v1.Pod) (*models.PodMetrics, error) {
	// Get node internal IP where the pod is running
	node, err := pc.clientset.CoreV1().Nodes().Get(ctx, pod.Spec.NodeName, metav1.GetOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to get node info: %w", err)
	}

	var nodeAddress string
	for _, addr := range node.Status.Addresses {
		if addr.Type == v1.NodeInternalIP {
			nodeAddress = addr.Address
			break
		}
	}

	// Construct URL for kubelet metrics
	metricsURL := fmt.Sprintf("https://%s:10250/metrics/resource", nodeAddress)

	// Create request
	req, err := http.NewRequestWithContext(ctx, "GET", metricsURL, nil)
	if err != nil {
		return nil, fmt.Errorf("failed to create request: %w", err)
	}

	// Get bearer token from service account
	token, err := os.ReadFile("/var/run/secrets/kubernetes.io/serviceaccount/token")
	if err != nil {
		return nil, fmt.Errorf("failed to read service account token: %w", err)
	}
	req.Header.Set("Authorization", "Bearer "+string(token))

	// Make request
	resp, err := pc.httpClient.Do(req)
	if err != nil {
		return nil, fmt.Errorf("failed to get metrics from kubelet: %w", err)
	}
	defer resp.Body.Close()

	// Parse metrics
	var parser expfmt.TextParser
	metricFamilies, err := parser.TextToMetricFamilies(resp.Body)
	if err != nil {
		return nil, fmt.Errorf("failed to parse metrics: %w", err)
	}

	// Create pod metrics object
	metrics := &models.PodMetrics{
		Name:       pod.Name,
		Namespace:  pod.Namespace,
		Usage:      models.ResourceMetrics{},
		Containers: make([]models.ContainerMetrics, 0),
	}

	containerMetrics := make(map[string]*models.ContainerMetrics)

	// Parse container metrics for the pod
	for _, family := range metricFamilies {
		for _, metric := range family.Metric {
			labels := make(map[string]string)
			for _, label := range metric.Label {
				labels[*label.Name] = *label.Value
			}

			// Match metrics for this specific pod
			if labels["pod"] == pod.Name && labels["namespace"] == pod.Namespace {
				containerName := labels["container"]

				// Initialize container metrics if not exists
				if _, exists := containerMetrics[containerName]; !exists {
					containerMetrics[containerName] = &models.ContainerMetrics{
						Name: containerName,
					}
				}

				// Update container metrics based on metric type
				switch family.GetName() {
				case "container_cpu_usage_seconds_total":
					value := int64(*metric.Counter.Value * 1000) // Convert to millicores
					containerMetrics[containerName].UsageNanos = value
					metrics.Usage.CPU += value
				case "container_memory_working_set_bytes":
					value := int64(*metric.Gauge.Value)
					containerMetrics[containerName].UsageBytes = value
					metrics.Usage.Memory += value
					containerMetrics[containerName].Memory.WorkingSet = uint64(*metric.Gauge.Value)
				}
			}
		}
	}

	// Convert map to slice
	for _, cm := range containerMetrics {
		metrics.Containers = append(metrics.Containers, *cm)
	}

	return metrics, nil
}

func (pc *PodCollector) extractContainerMetrics(
	pod v1.Pod,
	podMetrics *models.PodMetrics,
) []models.ContainerMetrics {

	containerMetrics := make([]models.ContainerMetrics, 0, len(pod.Status.ContainerStatuses))

	for _, container := range pod.Status.ContainerStatuses {
		metrics := models.ContainerMetrics{
			Name:         container.Name,
			RestartCount: container.RestartCount,
			Ready:        container.Ready,
			State:        getContainerState(container.State),
		}

		if container.LastTerminationState.Terminated != nil {
			metrics.LastTerminationReason = container.LastTerminationState.Terminated.Reason
		}

		// Match container usage from podMetrics
		for _, containerName := range podMetrics.Containers {
			if containerName.Name == container.Name {
				metrics.CPU = containerName.CPU
				metrics.Memory = containerName.Memory
				break
			}
		}

		containerMetrics = append(containerMetrics, metrics)
	}

	logrus.Debugf("Successfully extracted container metrics for pod %s/%s", pod.Namespace, pod.Name)
	return containerMetrics
}

func (pc *PodCollector) getTotalRestarts(pod v1.Pod) int32 {
	var totalRestarts int32
	for _, containerStatus := range pod.Status.ContainerStatuses {
		totalRestarts += containerStatus.RestartCount
	}
	logrus.Debugf("Total restarts for pod %s/%s: %d", pod.Namespace, pod.Name, totalRestarts)
	return totalRestarts
}

func getContainerState(state v1.ContainerState) string {
	if state.Running != nil {
		return "Running"
	}
	if state.Waiting != nil {
		return "Waiting"
	}
	if state.Terminated != nil {
		return "Terminated"
	}
	return "Unknown"
}

func (pc *PodCollector) collectPodDisruptionBudget(ctx context.Context, pod v1.Pod) (*models.PodDisruptionBudgetMetrics, error) {
	pdbs, err := pc.clientset.PolicyV1().PodDisruptionBudgets(pod.Namespace).List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, err
	}

	for _, pdb := range pdbs.Items {
		selector, err := metav1.LabelSelectorAsSelector(pdb.Spec.Selector)
		if err != nil {
			continue
		}
		if selector.Matches(labels.Set(pod.Labels)) {
			return &models.PodDisruptionBudgetMetrics{
				MinAvailable:       pdb.Spec.MinAvailable.String(),
				MaxUnavailable:     pdb.Spec.MaxUnavailable.String(),
				CurrentHealthy:     pdb.Status.CurrentHealthy,
				DesiredHealthy:     pdb.Status.DesiredHealthy,
				DisruptionsAllowed: pdb.Status.DisruptionsAllowed,
				ExpectedPods:       pdb.Status.ExpectedPods,
			}, nil
		}
	}
	return nil, nil
}

func (pc *PodCollector) collectTopologySpread(pod v1.Pod) []models.TopologySpreadConstraint {
	var constraints []models.TopologySpreadConstraint
	for _, constraint := range pod.Spec.TopologySpreadConstraints {
		constraints = append(constraints, models.TopologySpreadConstraint{
			MaxSkew:           constraint.MaxSkew,
			TopologyKey:       constraint.TopologyKey,
			WhenUnsatisfiable: string(constraint.WhenUnsatisfiable),
			LabelSelector:     constraint.LabelSelector.String(),
			MinDomains:        constraint.MinDomains,
		})
	}
	return constraints
}

func (pc *PodCollector) collectSecurityContext(pod v1.Pod) *models.SecurityContextMetrics {
	if pod.Spec.SecurityContext == nil {
		return nil
	}

	return &models.SecurityContextMetrics{
		RunAsUser:    pod.Spec.SecurityContext.RunAsUser,
		RunAsGroup:   pod.Spec.SecurityContext.RunAsGroup,
		FSGroup:      pod.Spec.SecurityContext.FSGroup,
		RunAsNonRoot: pod.Spec.SecurityContext.RunAsNonRoot,
		SELinuxOptions: func() map[string]string {
			if pod.Spec.SecurityContext.SELinuxOptions != nil {
				return map[string]string{
					"user":  pod.Spec.SecurityContext.SELinuxOptions.User,
					"role":  pod.Spec.SecurityContext.SELinuxOptions.Role,
					"type":  pod.Spec.SecurityContext.SELinuxOptions.Type,
					"level": pod.Spec.SecurityContext.SELinuxOptions.Level,
				}
			}
			return nil
		}(),
	}
}

func (pc *PodCollector) collectQoSDetails(pod v1.Pod) *models.QoSMetrics {
	qosMetrics := &models.QoSMetrics{
		Class:            string(pod.Status.QOSClass),
		CPUGuaranteed:    true,
		MemoryGuaranteed: true,
	}

	for _, container := range pod.Spec.Containers {
		if container.Resources.Requests.Cpu().IsZero() {
			qosMetrics.CPUGuaranteed = false
		}
		if container.Resources.Requests.Memory().IsZero() {
			qosMetrics.MemoryGuaranteed = false
		}
	}

	return qosMetrics
}

================
File: replicaset.go
================
// Copyright 2024 Vega Cloud, Inc.
//
// Use of this software is governed by the Business Source License
// included in the file licenses/BSL.txt.
//
// As of the Change Date specified in that file, in accordance with
// the Business Source License, use of this software will be governed
// by the Apache License, Version 2.0, included in the file
// licenses/APL.txt.

// Package collectors hosts the collection functions
package collectors

import (
	"context"
	"fmt"

	"github.com/sirupsen/logrus"
	appsv1 "k8s.io/api/apps/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/client-go/kubernetes"

	"github.com/vegacloud/kubernetes/metricsagent/pkg/config"
	"github.com/vegacloud/kubernetes/metricsagent/pkg/models"
)

// ReplicaSetCollector collects metrics for ReplicaSets
type ReplicaSetCollector struct {
	clientset *kubernetes.Clientset
	config    *config.Config
}

// NewReplicaSetCollector creates a new ReplicaSetCollector instance
func NewReplicaSetCollector(clientset *kubernetes.Clientset, cfg *config.Config) *ReplicaSetCollector {
	collector := &ReplicaSetCollector{
		clientset: clientset,
		config:    cfg,
	}
	logrus.Debug("ReplicaSetCollector created successfully")
	return collector
}

// CollectMetrics collects metrics for all ReplicaSets in the cluster
func (rsc *ReplicaSetCollector) CollectMetrics(ctx context.Context) (interface{}, error) {
	metrics, err := rsc.CollectReplicaSetMetrics(ctx)
	if err != nil {
		return nil, err
	}
	logrus.Debug("Successfully collected ReplicaSet metrics")
	return metrics, nil
}

// CollectReplicaSetMetrics collects metrics for all ReplicaSets in the cluster
func (rsc *ReplicaSetCollector) CollectReplicaSetMetrics(ctx context.Context) ([]models.ReplicaSetMetrics, error) {
	replicaSets, err := rsc.clientset.AppsV1().ReplicaSets("").List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to list ReplicaSets: %w", err)
	}
	logrus.Debugf("Successfully listed %d ReplicaSets", len(replicaSets.Items))

	metrics := make([]models.ReplicaSetMetrics, 0, len(replicaSets.Items))

	for _, rs := range replicaSets.Items {
		metrics = append(metrics, rsc.parseReplicaSetMetrics(rs))
	}

	logrus.Debugf("Collected metrics for %d ReplicaSets", len(metrics))
	return metrics, nil
}

// parseReplicaSetMetrics parses metrics for a single ReplicaSet
func (rsc *ReplicaSetCollector) parseReplicaSetMetrics(rs appsv1.ReplicaSet) models.ReplicaSetMetrics {
	if rs.Labels == nil {
		rs.Labels = make(map[string]string)
	}
	if rs.Annotations == nil {
		rs.Annotations = make(map[string]string)
	}

	conditions := make([]models.RSCondition, 0, len(rs.Status.Conditions))
	for _, condition := range rs.Status.Conditions {
		conditions = append(conditions, models.RSCondition{
			Type:               string(condition.Type),
			Status:             string(condition.Status),
			LastTransitionTime: &condition.LastTransitionTime.Time,
			Reason:             condition.Reason,
			Message:            condition.Message,
		})
	}

	return models.ReplicaSetMetrics{
		Name:                 rs.Name,
		Namespace:            rs.Namespace,
		Replicas:             *rs.Spec.Replicas,
		ReadyReplicas:        rs.Status.ReadyReplicas,
		AvailableReplicas:    rs.Status.AvailableReplicas,
		CurrentReplicas:      rs.Status.Replicas,
		FullyLabeledReplicas: rs.Status.FullyLabeledReplicas,
		ObservedGeneration:   rs.Status.ObservedGeneration,
		Conditions:           conditions,
		Labels:               rs.Labels,
		Annotations:          rs.Annotations,
		CreationTimestamp:    &rs.CreationTimestamp.Time,
	}
}

================
File: replicationController..go
================
// Copyright 2024 Vega Cloud, Inc.
//
// Use of this software is governed by the Business Source License
// included in the file licenses/BSL.txt.
//
// As of the Change Date specified in that file, in accordance with
// the Business Source License, use of this software will be governed
// by the Apache License, Version 2.0, included in the file
// licenses/APL.txt.

// Package collectors hosts the collection functions
package collectors

import (
	"context"
	"fmt"

	"github.com/sirupsen/logrus"
	v1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/client-go/kubernetes"

	"github.com/vegacloud/kubernetes/metricsagent/pkg/config"
	"github.com/vegacloud/kubernetes/metricsagent/pkg/models"
)

// ReplicationControllerCollector collects metrics from Kubernetes replication controllers.
type ReplicationControllerCollector struct {
	clientset *kubernetes.Clientset
	config    *config.Config
}

// NewReplicationControllerCollector creates a new ReplicationControllerCollector.
func NewReplicationControllerCollector(clientset *kubernetes.Clientset,
	cfg *config.Config) *ReplicationControllerCollector {
	collector := &ReplicationControllerCollector{
		clientset: clientset,
		config:    cfg,
	}
	logrus.Debug("ReplicationControllerCollector created successfully")
	return collector
}

// CollectMetrics collects metrics from Kubernetes replication controllers.
func (rcc *ReplicationControllerCollector) CollectMetrics(ctx context.Context) (interface{}, error) {
	metrics, err := rcc.CollectReplicationControllerMetrics(ctx)
	if err != nil {
		return nil, err
	}
	logrus.Debug("Successfully collected replication controller metrics")
	return metrics, nil
}

// CollectReplicationControllerMetrics collects metrics from Kubernetes replication controllers.
func (rcc *ReplicationControllerCollector) CollectReplicationControllerMetrics(
	ctx context.Context) ([]models.ReplicationControllerMetrics, error) {
	rcs, err := rcc.clientset.CoreV1().ReplicationControllers("").List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to list replication controllers: %w", err)
	}
	logrus.Debugf("Successfully listed %d replication controllers", len(rcs.Items))

	metrics := make([]models.ReplicationControllerMetrics, 0, len(rcs.Items))
	for _, rc := range rcs.Items {
		metrics = append(metrics, rcc.parseReplicationControllerMetrics(rc))
	}

	logrus.Debugf("Collected metrics for %d replication controllers", len(metrics))
	return metrics, nil
}

func (rcc *ReplicationControllerCollector) parseReplicationControllerMetrics(
	rc v1.ReplicationController) models.ReplicationControllerMetrics {
	if rc.Labels == nil {
		rc.Labels = make(map[string]string)
	}
	if rc.Annotations == nil {
		rc.Annotations = make(map[string]string)
	}

	conditions := make([]models.RCCondition, 0, len(rc.Status.Conditions))
	for _, condition := range rc.Status.Conditions {
		conditions = append(conditions, models.RCCondition{
			Type:               string(condition.Type),
			Status:             string(condition.Status),
			LastTransitionTime: &condition.LastTransitionTime.Time,
			Reason:             condition.Reason,
			Message:            condition.Message,
		})
	}

	metrics := models.ReplicationControllerMetrics{
		Name:                 rc.Name,
		Namespace:            rc.Namespace,
		Replicas:             rc.Status.Replicas,
		ReadyReplicas:        rc.Status.ReadyReplicas,
		AvailableReplicas:    rc.Status.AvailableReplicas,
		Labels:               rc.Labels,
		ObservedGeneration:   rc.Status.ObservedGeneration,
		FullyLabeledReplicas: rc.Status.FullyLabeledReplicas,
		Conditions:           conditions,
		Annotations:          rc.Annotations,
		CreationTimestamp:    &rc.CreationTimestamp.Time,
	}

	logrus.Debugf("Parsed replication controller metrics for %s/%s", rc.Namespace, rc.Name)
	return metrics
}

================
File: storageclass.go
================
// Copyright 2024 Vega Cloud, Inc.
//
// Use of this software is governed by the Business Source License
// included in the file licenses/BSL.txt.
//
// As of the Change Date specified in that file, in accordance with
// the Business Source License, use of this software will be governed
// by the Apache License, Version 2.0, included in the file
// licenses/APL.txt.

package collectors

import (
	"context"
	"fmt"
	"strings"

	"github.com/sirupsen/logrus"
	v1 "k8s.io/api/core/v1"
	storagev1 "k8s.io/api/storage/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/client-go/kubernetes"

	"github.com/vegacloud/kubernetes/metricsagent/pkg/config"
	"github.com/vegacloud/kubernetes/metricsagent/pkg/models"
)

// StorageClassCollector collects metrics from Kubernetes storage classes
type StorageClassCollector struct {
	clientset *kubernetes.Clientset
	config    *config.Config
}

// NewStorageClassCollector creates a new StorageClassCollector
func NewStorageClassCollector(clientset *kubernetes.Clientset, cfg *config.Config) *StorageClassCollector {
	collector := &StorageClassCollector{
		clientset: clientset,
		config:    cfg,
	}
	logrus.Debug("StorageClassCollector created successfully")
	return collector
}

// CollectMetrics collects metrics from Kubernetes storage classes
func (sc *StorageClassCollector) CollectMetrics(ctx context.Context) (interface{}, error) {
	storageClasses, err := sc.clientset.StorageV1().StorageClasses().List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to list storage classes: %w", err)
	}

	metrics := make([]models.StorageClassMetrics, 0, len(storageClasses.Items))
	for _, storageClass := range storageClasses.Items {
		metrics = append(metrics, sc.parseStorageClassMetrics(ctx, storageClass))
	}

	logrus.Debugf("Successfully collected metrics for %d storage classes", len(metrics))
	return metrics, nil
}

func (sc *StorageClassCollector) parseStorageClassMetrics(ctx context.Context, storageClass storagev1.StorageClass) models.StorageClassMetrics {
	if storageClass.Labels == nil {
		storageClass.Labels = make(map[string]string)
	}
	if storageClass.Annotations == nil {
		storageClass.Annotations = make(map[string]string)
	}

	metrics := models.StorageClassMetrics{
		Name:                 storageClass.Name,
		Provisioner:          storageClass.Provisioner,
		ReclaimPolicy:        string(*storageClass.ReclaimPolicy),
		VolumeBindingMode:    string(*storageClass.VolumeBindingMode),
		AllowVolumeExpansion: storageClass.AllowVolumeExpansion != nil && *storageClass.AllowVolumeExpansion,
		Labels:               storageClass.Labels,
		Annotations:          storageClass.Annotations,
		Parameters:           storageClass.Parameters,
		MountOptions:         storageClass.MountOptions,
		CreationTimestamp:    &storageClass.CreationTimestamp.Time,
	}

	// Check if this is the default storage class
	metrics.IsDefault = false
	for key, value := range storageClass.Annotations {
		if (key == "storageclass.kubernetes.io/is-default-class" ||
			key == "storageclass.beta.kubernetes.io/is-default-class") &&
			value == "true" {
			metrics.IsDefault = true
			break
		}
	}

	// Collect CSI driver metrics
	csiDriver, err := sc.collectCSIDriverMetrics(ctx, storageClass.Provisioner)
	if err != nil {
		logrus.Warnf("Failed to collect CSI driver metrics for %s: %v", storageClass.Name, err)
	} else {
		metrics.CSIDriver = csiDriver
	}

	// Collect storage pool metrics
	storagePools, err := sc.collectStoragePoolMetrics(ctx, storageClass.Name)
	if err != nil {
		logrus.Warnf("Failed to collect storage pool metrics for %s: %v", storageClass.Name, err)
	} else {
		metrics.StoragePools = storagePools
	}

	// Collect capacity metrics
	capacityMetrics, err := sc.collectCapacityMetrics(ctx, storageClass.Name)
	if err != nil {
		logrus.Warnf("Failed to collect capacity metrics for %s: %v", storageClass.Name, err)
	} else {
		metrics.TotalCapacity = capacityMetrics.TotalCapacity
		metrics.AllocatedCapacity = capacityMetrics.AllocatedCapacity
		metrics.AvailableCapacity = capacityMetrics.AvailableCapacity
		metrics.CapacityUtilization = capacityMetrics.CapacityUtilization
		metrics.ProvisionedPVCs = capacityMetrics.ProvisionedPVCs
		metrics.ProvisioningRate = capacityMetrics.ProvisioningRate
	}

	logrus.Debugf("Parsed metrics for storage class %s", storageClass.Name)
	return metrics
}

func (sc *StorageClassCollector) collectCSIDriverMetrics(ctx context.Context, provisioner string) (*models.CSIDriverMetrics, error) {
	// Get CSI driver info
	csiDrivers, err := sc.clientset.StorageV1().CSIDrivers().List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to list CSI drivers: %w", err)
	}

	for _, driver := range csiDrivers.Items {
		if driver.Name == provisioner {
			metrics := &models.CSIDriverMetrics{
				Name:               driver.Name,
				Available:          true,
				VolumeSnapshotting: driver.Spec.VolumeLifecycleModes != nil,
				VolumeCloning:      driver.Spec.VolumeLifecycleModes != nil,
				VolumeExpansion:    driver.Spec.RequiresRepublish != nil && *driver.Spec.RequiresRepublish,
				NodePluginPods:     make(map[string]string),
				ControllerPods:     make(map[string]string),
			}

			// Get CSI pods
			pods, err := sc.clientset.CoreV1().Pods("").List(ctx, metav1.ListOptions{
				LabelSelector: fmt.Sprintf("app=%s", driver.Name),
			})
			if err == nil {
				for _, pod := range pods.Items {
					if strings.Contains(pod.Name, "node") {
						metrics.NodePluginPods[pod.Spec.NodeName] = string(pod.Status.Phase)
					} else if strings.Contains(pod.Name, "controller") {
						metrics.ControllerPods[pod.Namespace] = string(pod.Status.Phase)
					}
				}
			}

			return metrics, nil
		}
	}

	return nil, fmt.Errorf("CSI driver not found for provisioner: %s", provisioner)
}

func (sc *StorageClassCollector) collectStoragePoolMetrics(ctx context.Context, storageClassName string) ([]models.StoragePoolMetrics, error) {
	var pools []models.StoragePoolMetrics

	// Get PVs using this storage class
	pvs, err := sc.clientset.CoreV1().PersistentVolumes().List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to list PVs: %w", err)
	}

	poolMap := make(map[string]*models.StoragePoolMetrics)
	for _, pv := range pvs.Items {
		if pv.Spec.StorageClassName == storageClassName {
			poolName := pv.Labels["storage-pool"] // Adjust label key based on your CSI driver
			if pool, exists := poolMap[poolName]; exists {
				pool.UsedCapacity += pv.Spec.Capacity.Storage().Value()
				pool.VolumeCount++
			} else {
				poolMap[poolName] = &models.StoragePoolMetrics{
					Name:         poolName,
					Provider:     pv.Spec.CSI.Driver,
					StorageClass: storageClassName,
					UsedCapacity: pv.Spec.Capacity.Storage().Value(),
					VolumeCount:  1,
				}
			}
		}
	}

	for _, pool := range poolMap {
		if pool.TotalCapacity > 0 {
			pool.UtilizationPct = float64(pool.UsedCapacity) / float64(pool.TotalCapacity) * 100
		}
		pool.AvailableSpace = pool.TotalCapacity - pool.UsedCapacity
		pools = append(pools, *pool)
	}

	return pools, nil
}

func (sc *StorageClassCollector) collectCapacityMetrics(ctx context.Context, storageClassName string) (*models.StorageClassMetrics, error) {
	metrics := &models.StorageClassMetrics{}

	// Get PVCs using this storage class
	pvcs, err := sc.clientset.CoreV1().PersistentVolumeClaims("").List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to list PVCs: %w", err)
	}

	var totalRequested int64
	provisionedCount := 0
	for _, pvc := range pvcs.Items {
		if pvc.Spec.StorageClassName != nil && *pvc.Spec.StorageClassName == storageClassName {
			totalRequested += pvc.Spec.Resources.Requests.Storage().Value()
			if pvc.Status.Phase == v1.ClaimBound {
				provisionedCount++
			}
		}
	}

	metrics.AllocatedCapacity = totalRequested
	metrics.ProvisionedPVCs = int32(provisionedCount)

	return metrics, nil
}

================
File: workload.go
================
// Copyright 2024 Vega Cloud, Inc.
//
// Use of this software is governed by the Business Source License
// included in the file licenses/BSL.txt.
//
// As of the Change Date specified in that file, in accordance with
// the Business Source License, use of this software will be governed
// by the Apache License, Version 2.0, included in the file
// licenses/APL.txt.
// File: pkg/collectors/workload.go

// Package collectors hosts the collection functions
package collectors

import (
	"context"
	"fmt"
	"time"

	"github.com/sirupsen/logrus"
	appsv1 "k8s.io/api/apps/v1"
	batchv1 "k8s.io/api/batch/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/client-go/kubernetes"

	"github.com/vegacloud/kubernetes/metricsagent/pkg/config"
	"github.com/vegacloud/kubernetes/metricsagent/pkg/models"
	v1 "k8s.io/api/core/v1"
)

// WorkloadCollector collects metrics from Kubernetes workloads.
type WorkloadCollector struct {
	clientset *kubernetes.Clientset
	config    *config.Config
}

// NewWorkloadCollector creates a new WorkloadCollector.
func NewWorkloadCollector(clientset *kubernetes.Clientset, cfg *config.Config) *WorkloadCollector {
	return &WorkloadCollector{
		clientset: clientset,
		config:    cfg,
	}
}

// CollectMetrics collects metrics from Kubernetes workloads.
func (wc *WorkloadCollector) CollectMetrics(ctx context.Context) (interface{}, error) {
	return wc.CollectWorkloadMetrics(ctx)
}

// CollectWorkloadMetrics collects metrics from Kubernetes workloads.
func (wc *WorkloadCollector) CollectWorkloadMetrics(ctx context.Context) (*models.WorkloadMetrics, error) {
	metrics := &models.WorkloadMetrics{}

	var err error
	metrics.Deployments, err = wc.collectDeploymentMetrics(ctx)
	if err != nil {
		logrus.Warnf("Failed to collect deployment metrics: %v", err)
	} else {
		logrus.Debug("Successfully collected deployment metrics")
	}

	metrics.StatefulSets, err = wc.collectStatefulSetMetrics(ctx)
	if err != nil {
		logrus.Warnf("Failed to collect statefulset metrics: %v", err)
	} else {
		logrus.Debug("Successfully collected statefulset metrics")
	}

	metrics.DaemonSets, err = wc.collectDaemonSetMetrics(ctx)
	if err != nil {
		logrus.Warnf("Failed to collect daemonset metrics: %v", err)
	} else {
		logrus.Debug("Successfully collected daemonset metrics")
	}

	metrics.Jobs, err = wc.collectJobMetrics(ctx)
	if err != nil {
		logrus.Warnf("Failed to collect job metrics: %v", err)
	} else {
		logrus.Debug("Successfully collected job metrics")
	}

	return metrics, nil
}

// collectDeploymentMetrics collects metrics from Kubernetes deployments.
func (wc *WorkloadCollector) collectDeploymentMetrics(ctx context.Context) ([]models.DeploymentMetrics, error) {
	deployments, err := wc.clientset.AppsV1().Deployments("").List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to list deployments: %w", err)
	}

	metrics := make([]models.DeploymentMetrics, 0, len(deployments.Items))

	for _, d := range deployments.Items {
		metrics = append(metrics, wc.parseDeploymentMetrics(d))
	}

	logrus.Debugf("Collected metrics for %d deployments", len(metrics))
	return metrics, nil
}

func (wc *WorkloadCollector) parseDeploymentMetrics(d appsv1.Deployment) models.DeploymentMetrics {
	if d.Labels == nil {
		d.Labels = make(map[string]string)
	}

	conditions := make([]string, 0)
	for _, condition := range d.Status.Conditions {
		conditions = append(conditions, string(condition.Type))
	}

	return models.DeploymentMetrics{
		Name:               d.Name,
		Namespace:          d.Namespace,
		Replicas:           *d.Spec.Replicas,
		ReadyReplicas:      d.Status.ReadyReplicas,
		UpdatedReplicas:    d.Status.UpdatedReplicas,
		AvailableReplicas:  d.Status.AvailableReplicas,
		Labels:             d.Labels,
		CollisionCount:     d.Status.CollisionCount,
		Conditions:         conditions,
		Generation:         d.Generation,
		ObservedGeneration: d.Status.ObservedGeneration,
	}
}

func (wc *WorkloadCollector) collectStatefulSetMetrics(ctx context.Context) ([]models.StatefulSetMetrics, error) {
	statefulSets, err := wc.clientset.AppsV1().StatefulSets("").List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to list statefulsets: %w", err)
	}
	metrics := make([]models.StatefulSetMetrics, 0, len(statefulSets.Items))

	for _, s := range statefulSets.Items {
		metrics = append(metrics, wc.parseStatefulSetMetrics(s))
	}

	logrus.Debugf("Collected metrics for %d statefulsets", len(metrics))
	return metrics, nil
}

func (wc *WorkloadCollector) parseStatefulSetMetrics(s appsv1.StatefulSet) models.StatefulSetMetrics {
	if s.Labels == nil {
		s.Labels = make(map[string]string)
	}

	conditions := make([]string, 0)
	for _, condition := range s.Status.Conditions {
		conditions = append(conditions, string(condition.Type))
	}

	return models.StatefulSetMetrics{
		Name:               s.Name,
		Namespace:          s.Namespace,
		Replicas:           *s.Spec.Replicas,
		ReadyReplicas:      s.Status.ReadyReplicas,
		CurrentReplicas:    s.Status.CurrentReplicas,
		UpdatedReplicas:    s.Status.UpdatedReplicas,
		AvailableReplicas:  s.Status.AvailableReplicas,
		Labels:             s.Labels,
		CollisionCount:     s.Status.CollisionCount,
		Conditions:         conditions,
		Generation:         s.Generation,
		ObservedGeneration: s.Status.ObservedGeneration,
	}
}

func (wc *WorkloadCollector) collectDaemonSetMetrics(ctx context.Context) ([]models.DaemonSetMetrics, error) {
	daemonSets, err := wc.clientset.AppsV1().DaemonSets("").List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to list daemonsets: %w", err)
	}
	metrics := make([]models.DaemonSetMetrics, 0, len(daemonSets.Items))

	for _, d := range daemonSets.Items {
		metrics = append(metrics, wc.parseDaemonSetMetrics(d))
	}

	logrus.Debugf("Collected metrics for %d daemonsets", len(metrics))
	return metrics, nil
}

func (wc *WorkloadCollector) parseDaemonSetMetrics(d appsv1.DaemonSet) models.DaemonSetMetrics {
	if d.Labels == nil {
		d.Labels = make(map[string]string)
	}

	conditions := make([]models.DaemonSetCondition, 0)
	for _, condition := range d.Status.Conditions {
		conditions = append(conditions, models.DaemonSetCondition{
			Type:               string(condition.Type),
			Status:             string(condition.Status),
			LastTransitionTime: &condition.LastTransitionTime.Time,
			Reason:             condition.Reason,
			Message:            condition.Message,
		})
	}

	return models.DaemonSetMetrics{
		Name:                   d.Name,
		Namespace:              d.Namespace,
		DesiredNumberScheduled: d.Status.DesiredNumberScheduled,
		CurrentNumberScheduled: d.Status.CurrentNumberScheduled,
		NumberReady:            d.Status.NumberReady,
		UpdatedNumberScheduled: d.Status.UpdatedNumberScheduled,
		NumberAvailable:        d.Status.NumberAvailable,
		NumberUnavailable:      d.Status.NumberUnavailable,
		NumberMisscheduled:     d.Status.NumberMisscheduled,
		Labels:                 d.Labels,
		Generation:             d.Generation,
		ObservedGeneration:     d.Status.ObservedGeneration,
		Conditions:             conditions,
	}
}

func (wc *WorkloadCollector) collectJobMetrics(ctx context.Context) ([]models.JobMetrics, error) {
	jobs, err := wc.clientset.BatchV1().Jobs("").List(ctx, metav1.ListOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to list jobs: %w", err)
	}
	metrics := make([]models.JobMetrics, 0, len(jobs.Items))

	for _, j := range jobs.Items {
		metrics = append(metrics, wc.parseJobMetrics(j))
	}

	logrus.Debugf("Collected metrics for %d jobs", len(metrics))
	return metrics, nil
}

func (wc *WorkloadCollector) parseJobMetrics(j batchv1.Job) models.JobMetrics {
	if j.Labels == nil {
		j.Labels = make(map[string]string)
	}

	conditions := make([]models.JobCondition, 0)
	for _, condition := range j.Status.Conditions {
		conditions = append(conditions, models.JobCondition{
			Type:               string(condition.Type),
			Status:             string(condition.Status),
			LastProbeTime:      &condition.LastProbeTime.Time,
			LastTransitionTime: &condition.LastTransitionTime.Time,
			Reason:             condition.Reason,
			Message:            condition.Message,
		})
	}

	metrics := models.JobMetrics{
		Name:             j.Name,
		Namespace:        j.Namespace,
		Labels:           j.Labels,
		Active:           j.Status.Active,
		Succeeded:        j.Status.Succeeded,
		Failed:           j.Status.Failed,
		Status:           getJobStatus(j.Status),
		CompletedIndexes: j.Status.CompletedIndexes,
		Conditions:       conditions,
		Generation:       j.Generation,
	}

	// Add existing time-related fields
	if j.Status.StartTime != nil {
		metrics.StartTime = &j.Status.StartTime.Time
	}
	if j.Status.CompletionTime != nil {
		metrics.CompletionTime = &j.Status.CompletionTime.Time
	}
	if metrics.StartTime != nil {
		endTime := time.Now()
		if metrics.CompletionTime != nil {
			endTime = *metrics.CompletionTime
		}
		duration := endTime.Sub(*metrics.StartTime)
		metrics.Duration = &duration
	}

	metrics.ResourceMetrics = wc.getJobResourceMetrics(j)
	return metrics
}

// Helper function to get job status
func getJobStatus(status batchv1.JobStatus) string {
	switch {
	case status.Succeeded > 0:
		return "Succeeded"
	case status.Failed > 0:
		return "Failed"
	case status.Active > 0:
		return "Active"
	default:
		return "Pending"
	}
}

// Helper function to get resource metrics for a job
func (wc *WorkloadCollector) getJobResourceMetrics(job batchv1.Job) models.ResourceMetrics {
	metrics := models.ResourceMetrics{}

	if job.Spec.Template.Spec.Containers == nil {
		return metrics
	}

	for _, container := range job.Spec.Template.Spec.Containers {
		if container.Resources.Requests != nil {
			metrics.CPU += container.Resources.Requests.Cpu().MilliValue()
			metrics.Memory += container.Resources.Requests.Memory().Value()
			if storage := container.Resources.Requests.Storage(); storage != nil {
				metrics.Storage += storage.Value()
			}
			if ephemeral, ok := container.Resources.Requests[v1.ResourceEphemeralStorage]; ok {
				metrics.EphemeralStorage += ephemeral.Value()
			}
		}
	}

	return metrics
}
