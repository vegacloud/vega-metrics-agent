diff --git a/.gitignore b/.gitignore
index 5cdd932..84d1f5b 100644
--- a/.gitignore
+++ b/.gitignore
@@ -1,2 +1,3 @@
 # Ignore all Chart.lock files
 **/Chart.lock
+bin/*
diff --git a/Dockerfile b/Dockerfile
index bcfe2c2..dd5992b 100644
--- a/Dockerfile
+++ b/Dockerfile
@@ -8,7 +8,7 @@
 # by the Apache License, Version 2.0, included in the file
 # licenses/APL.txt.
 # Build Stage
-ARG golang_version=1.22
+ARG golang_version=1.23
 FROM golang:${golang_version}-alpine AS builder
 
 # Set the working directory
diff --git a/Makefile b/Makefile
index 949e8d8..8157284 100644
--- a/Makefile
+++ b/Makefile
@@ -11,7 +11,8 @@
 APPLICATION = vega-metrics-agent
 VERSION := $(shell cat VERSION)
 DOCKER_IMAGE = public.ecr.aws/c0f8b9o4/vegacloud/${APPLICATION}
-GOLANG_VERSION ?= 1.22
+DOCKER_IMAGE_DEV = ${APPLICATION}
+GOLANG_VERSION ?= 1.23
 
 # Go commands
 GO_BUILD = CGO_ENABLED=0 GOOS=linux go build -ldflags "-X main.version=${VERSION}" -o bin/${APPLICATION}
@@ -27,10 +28,26 @@ DOCKER_BUILD = docker build -f Dockerfile \
 	--build-arg app_version=${VERSION} \
 	-t ${DOCKER_IMAGE}:${VERSION} .
 
+
+DOCKER_BUILD_DEV = docker build -f Dockerfile \
+	--build-arg golang_version=${GOLANG_VERSION} \
+	--build-arg app_version=${VERSION} \
+	-t ${DOCKER_IMAGE_DEV}:${VERSION} .
+
+
 # Default target
 .PHONY: all
 all: fmt vet lint sec test build docker-build
 
+# Dev target
+.PHONY: alldev
+alldev: fmt vet lint sec build docker-build-dev
+
+# Dev target without security checks
+.PHONY: alldevnosec
+alldevnosec: fmt vet lint build docker-build-dev
+
+
 # Format Go code
 .PHONY: fmt
 fmt:
@@ -74,6 +91,14 @@ docker-build:
 	@echo "Building Docker image..."
 	${DOCKER_BUILD}
 
+
+# Build Docker image
+.PHONY: docker-build-dev
+docker-build-dev:
+	@echo "Building Docker dev image..."
+	${DOCKER_BUILD_DEV}
+
+
 # Push Docker image
 .PHONY: docker-push
 docker-push:
diff --git a/VERSION b/VERSION
index 3eefcb9..8376431 100644
--- a/VERSION
+++ b/VERSION
@@ -1 +1 @@
-1.0.0
+1.1.0-rc1
diff --git a/bin/vega-metrics-agent b/bin/vega-metrics-agent
new file mode 100755
index 0000000..ffc6268
Binary files /dev/null and b/bin/vega-metrics-agent differ
diff --git a/charts/vega-metrics-agent/.gitignore b/charts/vega-metrics-agent/.gitignore
new file mode 100644
index 0000000..59875d3
--- /dev/null
+++ b/charts/vega-metrics-agent/.gitignore
@@ -0,0 +1 @@
+values-dev.yaml
\ No newline at end of file
diff --git a/charts/vega-metrics-agent/Chart.yaml b/charts/vega-metrics-agent/Chart.yaml
index 721b029..7bdf02c 100644
--- a/charts/vega-metrics-agent/Chart.yaml
+++ b/charts/vega-metrics-agent/Chart.yaml
@@ -10,12 +10,6 @@
 apiVersion: v2
 name: vega-metrics-agent
 description: A Helm chart for deploying Vega Metrics Agent
-version: 1.0.0
-appVersion: "1.0.0"
+version: 1.1.0-dev
+appVersion: "1.1.0-dev"
 type: application
-# This agent requires the metrics-server to be installed in the cluster
-dependencies:
-  - name: metrics-server
-    version: 3.12.1
-    repository: https://kubernetes-sigs.github.io/metrics-server/
-    condition: metrics-server.enabled
diff --git a/charts/vega-metrics-agent/charts/metrics-server-3.12.1.tgz b/charts/vega-metrics-agent/charts/metrics-server-3.12.1.tgz
deleted file mode 100644
index c9e8bb9..0000000
Binary files a/charts/vega-metrics-agent/charts/metrics-server-3.12.1.tgz and /dev/null differ
diff --git a/charts/vega-metrics-agent/templates/clusterrole.yaml b/charts/vega-metrics-agent/templates/clusterrole.yaml
index 6e952a6..d2f36ad 100644
--- a/charts/vega-metrics-agent/templates/clusterrole.yaml
+++ b/charts/vega-metrics-agent/templates/clusterrole.yaml
@@ -1,138 +1,166 @@
-# Copyright 2024 Vega Cloud, Inc.
-#
-# Use of this software is governed by the Business Source License
-# included in the file licenses/BSL.txt.
-#
-# As of the Change Date specified in that file, in accordance with
-# the Business Source License, use of this software will be governed
-# by the Apache License, Version 2.0, included in the file
-# licenses/APL.txt.
 apiVersion: rbac.authorization.k8s.io/v1
 kind: ClusterRole
 metadata:
   name: metrics-agents
+  annotations:
+    "helm.sh/hook-weight": "1"
 rules:
+  - apiGroups: ["authentication.k8s.io"]
+    resources: ["selfsubjectreviews"]
+    verbs: ["create"]
+  # Core API resources
   - apiGroups: [""]
     resources:
+      - configmaps
+      - endpoints
+      - events
+      - limitranges
+      - limitranges/status
       - namespaces
-      - pods
-      - services
+      - namespaces/finalizers
+      - namespaces/status
       - nodes
-      - persistentvolumes
+      - nodes/log
+      - nodes/metrics
+      - nodes/proxy
+      - nodes/stats
+      - nodes/stats/summary
+      - nodes/status
       - persistentvolumeclaims
-      - endpoints
-      - events
-      - replicationcontrollers    
-      - resourcequotas      
-      - limitranges       
-    verbs:
-      - "get"
-      - "list"
-      - "watch"
-  - apiGroups: ["apps", "autoscaling", "batch", "extensions"]
+      - persistentvolumeclaims/status
+      - persistentvolumes
+      - persistentvolumes/status
+      - pods
+      - pods/ephemeralcontainers
+      - pods/exec
+      - pods/log
+      - pods/metrics
+      - pods/portforward
+      - pods/proxy
+      - pods/stats
+      - pods/status
+      - replicationcontrollers
+      - resourcequotas
+      - resourcequotas/status
+      - serviceaccounts
+      - services
+      - services/proxy
+      - services/status
+    verbs: ["get", "list", "watch"]
+
+  # Workload resources (batch and apps)
+  - apiGroups: ["batch", "apps"]
     resources:
+      - cronjobs
+      - cronjobs/status
+      - daemonsets
+      - daemonsets/status
       - deployments
+      - deployments/scale
+      - deployments/status
+      - jobs
+      - jobs/status
       - replicasets
+      - replicasets/scale
+      - replicasets/status
       - statefulsets
-      - daemonsets
-      - jobs
-      - cronjobs
-      - horizontalpodautoscalers
-    verbs:
-      - "get"
-      - "list"
-      - "watch"
-  - apiGroups: [""]
-    resources:
-      - configmaps
-    verbs:
-      - "get"
-      - "list"
-      - "watch"
-  - apiGroups: ["metrics.k8s.io"]
-    resources:
-      - nodes
-      - pods
-    verbs:
-      - "get"
-      - "list"
-      - "watch"
-  - apiGroups: ["networking.k8s.io"]
+      - statefulsets/scale
+      - statefulsets/status
+    verbs: ["get", "list", "watch"]
+
+  # Networking resources (networking.k8s.io and extensions)
+  - apiGroups: ["networking.k8s.io", "extensions"]
     resources:
-      - ingresses               
+      - ingresses
+      - ingresses/status
+      - ingressclasses
       - networkpolicies
-    verbs:
-      - "get"
-      - "list"
-      - "watch"
-  - apiGroups: [""]
+      - networkpolicies/status
+    verbs: ["get", "list", "watch"]
+
+  # Metrics resources
+  - apiGroups: ["metrics.k8s.io", "custom.metrics.k8s.io", "external.metrics.k8s.io", "pohpa.metrics.k8s.io"]
     resources:
-      - services/proxy
-      - pods/proxy
-      - nodes/proxy
-      - nodes/stats
-    verbs:
-      - "get"
-      - "list"
-      - "watch"
-  - apiGroups: ["policy"]
+      - "*"
+      - pods/recommendations
+    verbs: ["get", "list", "watch"]
+
+  # Autoscaling resources
+  - apiGroups: ["autoscaling", "autoscaling.k8s.io"]
     resources:
-      - poddisruptionbudgets
-    verbs:
-      - "get"
-      - "list"
-      - "watch"
-  - apiGroups: ["storage.k8s.io"]
+      - horizontalpodautoscalers
+      - horizontalpodautoscalers/scale
+      - horizontalpodautoscalers/status
+      - verticalpodautoscalers
+      - verticalpodautoscalers/status
+    verbs: ["get", "list", "watch"]
+
+  # Storage & Snapshot & CSI resources
+  - apiGroups: ["storage.k8s.io", "snapshot.storage.k8s.io", "csi.storage.k8s.io"]
     resources:
       - storageclasses
       - volumeattachments
-    verbs:
-      - "get"
-      - "list"
-      - "watch"
-  - apiGroups: ["events.k8s.io"]
-    resources:
-      - events
-    verbs:
-      - "get"
-      - "list"
-      - "watch"
-  - apiGroups: ["external.metrics.k8s.io"]
-    resources:
-      - "*"
-    verbs:
-      - "get"
-      - "list"
-      - "watch"
-  - apiGroups: [""]
+      - volumeattachments/status
+      - csidrivers
+      - csinodes
+      - csistoragecapacities
+      - volumeattributes
+      - volumesnapshots
+      - volumesnapshots/status
+      - volumesnapshotcontents
+      - volumesnapshotclasses
+      - csiservices
+    verbs: ["get", "list", "watch"]
+
+  # Security resources (policy and security.k8s.io)
+  - apiGroups: ["policy", "security.k8s.io"]
     resources:
-      - nodes/metrics
-      - nodes/stats
-      - pods/metrics
-    verbs:
-      - "get"
-      - "list"
-      - "watch"
-  - apiGroups: ["apiextensions.k8s.io"]
+      - poddisruptionbudgets
+      - poddisruptionbudgets/status
+      - podlabels
+      - podmetrics
+      - podsecuritypolicies
+      - securitycontextconstraints
+    verbs: ["get", "list", "watch"]
+
+  # Coordination for node leases
+  - apiGroups: ["coordination.k8s.io"]
+    resources: ["leases"]
+    verbs: ["get", "list", "watch"]
+
+  # Node resources
+  - apiGroups: ["node.k8s.io"]
     resources:
-      - customresourcedefinitions
-    verbs:
-      - "get"
-      - "list"
-      - "watch"
-  - apiGroups: [""]
+      - runtimeclasses
+      - runtimeclasses/status
+    verbs: ["get", "list", "watch"]
+
+  # GPU resources
+  - apiGroups: ["nvidia.com"]
     resources:
-      - namespaces/finalizers
-    verbs:
-      - "get"
-      - "list"
-      - "watch"
-  - apiGroups: [""]
+      - gpus
+      - podgpus
+    verbs: ["get", "list", "watch"]
+
+  # Topology
+  - apiGroups: ["topology.kubernetes.io"]
     resources:
-      - pods/log
-    verbs:
-      - "get"
-      - "list"
-      - "watch"
+      - nodes
+      - noderesourcetopologies
+      - topologyspreadconstraints
+    verbs: ["get", "list", "watch"]
 
+  # Events.k8s.io
+  - apiGroups: ["events.k8s.io"]
+    resources: ["events"]
+    verbs: ["get", "list", "watch"]
 
+  # Kubelet metrics and stats (NonResourceURLs)
+  - nonResourceURLs:
+      - "/metrics"
+      - "/metrics/cadvisor"
+      - "/metrics/probes"
+      - "/metrics/resource"
+      - "/stats/*"
+      - "/stats/summary"
+    verbs: ["get"]
diff --git a/charts/vega-metrics-agent/templates/clusterrolebinding.yaml b/charts/vega-metrics-agent/templates/clusterrolebinding.yaml
index 2e1297b..be42b20 100644
--- a/charts/vega-metrics-agent/templates/clusterrolebinding.yaml
+++ b/charts/vega-metrics-agent/templates/clusterrolebinding.yaml
@@ -11,6 +11,8 @@ apiVersion: rbac.authorization.k8s.io/v1
 kind: ClusterRoleBinding
 metadata:
   name: metrics-agents
+  annotations:
+    "helm.sh/hook-weight": "2"
 subjects:
   - kind: ServiceAccount
     name: {{ .Values.serviceAccount.name }}
diff --git a/charts/vega-metrics-agent/templates/deployment.yaml b/charts/vega-metrics-agent/templates/deployment.yaml
index 1bc5897..bbbe996 100644
--- a/charts/vega-metrics-agent/templates/deployment.yaml
+++ b/charts/vega-metrics-agent/templates/deployment.yaml
@@ -14,6 +14,9 @@ metadata:
   namespace: "{{ .Values.namespace }}"
   labels:
     app: metrics-agent
+  annotations:
+    "helm.sh/hook-weight": "5"
+    "helm.sh/hook-depends-on": "pre-install"
 spec:
   replicas: {{ .Values.replicaCount }}
   selector:
@@ -25,14 +28,20 @@ spec:
         app: metrics-agent
     spec:
       serviceAccountName: {{ .Values.serviceAccount.name }}
-      automountServiceAccountToken: true
+      automountServiceAccountToken: true     
+      initContainers:
+        - name: wait-for-permissions
+          image: bitnami/kubectl:1.30.7
+          command: ['sh', '-c', 'until kubectl auth can-i list pods --all-namespaces; do echo waiting for permissions; sleep 2; done']
       containers:
         - name: metrics-agent
           image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
           imagePullPolicy: "{{ .Values.image.pullPolicy }}"
           args:
-            - 'kubernetes'
-          env:
+            - 'kubernetes'          
+          env:          
+            - name: KUBERNETES_CLIENT_LOG_LEVEL
+              value: "8"
             - name: VEGA_CLIENT_ID
               valueFrom:
                 secretKeyRef:
diff --git a/charts/vega-metrics-agent/templates/serviceaccount.yaml b/charts/vega-metrics-agent/templates/serviceaccount.yaml
index 1b634b8..87d90f4 100644
--- a/charts/vega-metrics-agent/templates/serviceaccount.yaml
+++ b/charts/vega-metrics-agent/templates/serviceaccount.yaml
@@ -10,5 +10,8 @@
 apiVersion: v1
 kind: ServiceAccount
 metadata:
-  name: {{ .Values.serviceAccount.name  }}
-  namespace: {{ .Values.namespace }}
\ No newline at end of file
+  name: {{ .Values.serviceAccount.name }}
+  namespace: {{ .Values.namespace }}
+  annotations:
+    "helm.sh/hook-weight": "0"
+automountServiceAccountToken: true  # Add this line  
\ No newline at end of file
diff --git a/charts/vega-metrics-agent/values.yaml b/charts/vega-metrics-agent/values.yaml
index 2798706..20d0abd 100644
--- a/charts/vega-metrics-agent/values.yaml
+++ b/charts/vega-metrics-agent/values.yaml
@@ -13,7 +13,7 @@ serviceAccount:
   name: vega-metrics-sa
 image:
   repository: public.ecr.aws/c0f8b9o4/vegacloud/vega-metrics-agent
-  tag: 1.0.0
+  tag: 1.1.0-rc
   pullPolicy: Always
 replicaCount: 1
 resources:
@@ -33,10 +33,7 @@ vega:
   orgSlug: "XXXX"
   clusterName: "XXXX"
 env:
+  # For production, you should be using certificates internal to your cluster. 
+  # We put this by default becuase most k8s clusters are not using tls certs internally.
+  VEGA_INSECURE: true
 # VEGA_POLL_INTERVAL: "120m" # Default is 60M
-# This agent requires the metrics-server to be installed in the cluster
-metrics-server:
-  enabled: true
-  namespace: kube-system
-  extraArgs:
-    - --metric-resolution=15s
diff --git a/go.mod b/go.mod
index dc1d6ab..836816f 100644
--- a/go.mod
+++ b/go.mod
@@ -1,37 +1,35 @@
 module github.com/vegacloud/kubernetes/metricsagent
 
-go 1.22.6
+go 1.23.4
 
 require (
 	github.com/aws/aws-sdk-go v1.55.5
+	github.com/robfig/cron/v3 v3.0.1
 	github.com/spf13/cobra v1.8.1
 	github.com/spf13/viper v1.19.0
-	golang.org/x/sync v0.7.0
-	k8s.io/api v0.31.1
-	k8s.io/apimachinery v0.31.1
-	k8s.io/client-go v0.31.1
-	k8s.io/metrics v0.31.1
+	k8s.io/api v0.32.0
+	k8s.io/apimachinery v0.32.0
+	k8s.io/client-go v0.32.0
 )
 
 require (
-	github.com/google/pprof v0.0.0-20240528025155-186aa0362fba // indirect
-	github.com/imdario/mergo v0.3.6 // indirect
 	github.com/inconshreveable/mousetrap v1.1.0 // indirect
-	golang.org/x/tools v0.22.0 // indirect
+	github.com/pkg/errors v0.9.1 // indirect
+	gopkg.in/evanphx/json-patch.v4 v4.12.0 // indirect
 )
 
 require (
 	github.com/davecgh/go-spew v1.1.2-0.20180830191138-d8f796af33cc // indirect
-	github.com/emicklei/go-restful/v3 v3.11.0 // indirect
-	github.com/fsnotify/fsnotify v1.7.0 // indirect
+	github.com/emicklei/go-restful/v3 v3.12.1 // indirect
+	github.com/fsnotify/fsnotify v1.8.0 // indirect
 	github.com/fxamacker/cbor/v2 v2.7.0 // indirect
 	github.com/go-logr/logr v1.4.2 // indirect
-	github.com/go-openapi/jsonpointer v0.20.2 // indirect
-	github.com/go-openapi/jsonreference v0.20.4 // indirect
-	github.com/go-openapi/swag v0.22.9 // indirect
+	github.com/go-openapi/jsonpointer v0.21.0 // indirect
+	github.com/go-openapi/jsonreference v0.21.0 // indirect
+	github.com/go-openapi/swag v0.23.0 // indirect
 	github.com/gogo/protobuf v1.3.2 // indirect
 	github.com/golang/protobuf v1.5.4 // indirect
-	github.com/google/gnostic-models v0.6.8 // indirect
+	github.com/google/gnostic-models v0.6.9 // indirect
 	github.com/google/go-cmp v0.6.0 // indirect
 	github.com/google/gofuzz v1.2.0 // indirect
 	github.com/google/uuid v1.6.0
@@ -39,41 +37,40 @@ require (
 	github.com/jmespath/go-jmespath v0.4.0 // indirect
 	github.com/josharian/intern v1.0.0 // indirect
 	github.com/json-iterator/go v1.1.12 // indirect
-	github.com/magiconair/properties v1.8.7 // indirect
-	github.com/mailru/easyjson v0.7.7 // indirect
+	github.com/magiconair/properties v1.8.9 // indirect
+	github.com/mailru/easyjson v0.9.0 // indirect
 	github.com/mitchellh/mapstructure v1.5.0 // indirect
 	github.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd // indirect
 	github.com/modern-go/reflect2 v1.0.2 // indirect
 	github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822 // indirect
-	github.com/pelletier/go-toml/v2 v2.2.2 // indirect
+	github.com/pelletier/go-toml/v2 v2.2.3 // indirect
 	github.com/prometheus/client_model v0.6.1
-	github.com/prometheus/common v0.55.0
-	github.com/sagikazarmark/locafero v0.4.0 // indirect
+	github.com/prometheus/common v0.61.0
+	github.com/sagikazarmark/locafero v0.6.0 // indirect
 	github.com/sagikazarmark/slog-shim v0.1.0 // indirect
 	github.com/sirupsen/logrus v1.9.3
 	github.com/sourcegraph/conc v0.3.0 // indirect
 	github.com/spf13/afero v1.11.0 // indirect
-	github.com/spf13/cast v1.6.0 // indirect
+	github.com/spf13/cast v1.7.0 // indirect
 	github.com/spf13/pflag v1.0.5 // indirect
 	github.com/subosito/gotenv v1.6.0 // indirect
 	github.com/x448/float16 v0.8.4 // indirect
 	go.uber.org/multierr v1.11.0 // indirect
-	golang.org/x/exp v0.0.0-20240119083558-1b970713d09a // indirect
-	golang.org/x/net v0.26.0 // indirect
-	golang.org/x/oauth2 v0.21.0 // indirect
-	golang.org/x/sys v0.21.0 // indirect
-	golang.org/x/term v0.21.0 // indirect
-	golang.org/x/text v0.16.0 // indirect
-	golang.org/x/time v0.5.0 // indirect
-	google.golang.org/protobuf v1.34.2 // indirect
+	golang.org/x/exp v0.0.0-20241215155358-4a5509556b9e // indirect
+	golang.org/x/net v0.32.0 // indirect
+	golang.org/x/oauth2 v0.24.0 // indirect
+	golang.org/x/sys v0.28.0 // indirect
+	golang.org/x/term v0.27.0 // indirect
+	golang.org/x/text v0.21.0 // indirect
+	golang.org/x/time v0.8.0 // indirect
+	google.golang.org/protobuf v1.35.2 // indirect
 	gopkg.in/inf.v0 v0.9.1 // indirect
 	gopkg.in/ini.v1 v1.67.0 // indirect
-	gopkg.in/yaml.v2 v2.4.0 // indirect
 	gopkg.in/yaml.v3 v3.0.1 // indirect
 	k8s.io/klog/v2 v2.130.1 // indirect
-	k8s.io/kube-openapi v0.0.0-20240228011516-70dd3763d340 // indirect
-	k8s.io/utils v0.0.0-20240711033017-18e509b52bc8 // indirect
-	sigs.k8s.io/json v0.0.0-20221116044647-bc3834ca7abd // indirect
-	sigs.k8s.io/structured-merge-diff/v4 v4.4.1 // indirect
+	k8s.io/kube-openapi v0.0.0-20241212222426-2c72e554b1e7 // indirect
+	k8s.io/utils v0.0.0-20241210054802-24370beab758 // indirect
+	sigs.k8s.io/json v0.0.0-20241014173422-cfa47c3a1cc8 // indirect
+	sigs.k8s.io/structured-merge-diff/v4 v4.5.0 // indirect
 	sigs.k8s.io/yaml v1.4.0 // indirect
 )
diff --git a/go.sum b/go.sum
index 0e066eb..9cef88c 100644
--- a/go.sum
+++ b/go.sum
@@ -5,44 +5,42 @@ github.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSs
 github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
 github.com/davecgh/go-spew v1.1.2-0.20180830191138-d8f796af33cc h1:U9qPSI2PIWSS1VwoXQT9A3Wy9MM3WgvqSxFWenqJduM=
 github.com/davecgh/go-spew v1.1.2-0.20180830191138-d8f796af33cc/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
-github.com/emicklei/go-restful/v3 v3.11.0 h1:rAQeMHw1c7zTmncogyy8VvRZwtkmkZ4FxERmMY4rD+g=
-github.com/emicklei/go-restful/v3 v3.11.0/go.mod h1:6n3XBCmQQb25CM2LCACGz8ukIrRry+4bhvbpWn3mrbc=
+github.com/emicklei/go-restful/v3 v3.12.1 h1:PJMDIM/ak7btuL8Ex0iYET9hxM3CI2sjZtzpL63nKAU=
+github.com/emicklei/go-restful/v3 v3.12.1/go.mod h1:6n3XBCmQQb25CM2LCACGz8ukIrRry+4bhvbpWn3mrbc=
 github.com/frankban/quicktest v1.14.6 h1:7Xjx+VpznH+oBnejlPUj8oUpdxnVs4f8XU8WnHkI4W8=
 github.com/frankban/quicktest v1.14.6/go.mod h1:4ptaffx2x8+WTWXmUCuVU6aPUX1/Mz7zb5vbUoiM6w0=
-github.com/fsnotify/fsnotify v1.7.0 h1:8JEhPFa5W2WU7YfeZzPNqzMP6Lwt7L2715Ggo0nosvA=
-github.com/fsnotify/fsnotify v1.7.0/go.mod h1:40Bi/Hjc2AVfZrqy+aj+yEI+/bRxZnMJyTJwOpGvigM=
+github.com/fsnotify/fsnotify v1.8.0 h1:dAwr6QBTBZIkG8roQaJjGof0pp0EeF+tNV7YBP3F/8M=
+github.com/fsnotify/fsnotify v1.8.0/go.mod h1:8jBTzvmWwFyi3Pb8djgCCO5IBqzKJ/Jwo8TRcHyHii0=
 github.com/fxamacker/cbor/v2 v2.7.0 h1:iM5WgngdRBanHcxugY4JySA0nk1wZorNOpTgCMedv5E=
 github.com/fxamacker/cbor/v2 v2.7.0/go.mod h1:pxXPTn3joSm21Gbwsv0w9OSA2y1HFR9qXEeXQVeNoDQ=
 github.com/go-logr/logr v1.4.2 h1:6pFjapn8bFcIbiKo3XT4j/BhANplGihG6tvd+8rYgrY=
 github.com/go-logr/logr v1.4.2/go.mod h1:9T104GzyrTigFIr8wt5mBrctHMim0Nb2HLGrmQ40KvY=
-github.com/go-openapi/jsonpointer v0.20.2 h1:mQc3nmndL8ZBzStEo3JYF8wzmeWffDH4VbXz58sAx6Q=
-github.com/go-openapi/jsonpointer v0.20.2/go.mod h1:bHen+N0u1KEO3YlmqOjTT9Adn1RfD91Ar825/PuiRVs=
-github.com/go-openapi/jsonreference v0.20.4 h1:bKlDxQxQJgwpUSgOENiMPzCTBVuc7vTdXSSgNeAhojU=
-github.com/go-openapi/jsonreference v0.20.4/go.mod h1:5pZJyJP2MnYCpoeoMAql78cCHauHj0V9Lhc506VOpw4=
-github.com/go-openapi/swag v0.22.9 h1:XX2DssF+mQKM2DHsbgZK74y/zj4mo9I99+89xUmuZCE=
-github.com/go-openapi/swag v0.22.9/go.mod h1:3/OXnFfnMAwBD099SwYRk7GD3xOrr1iL7d/XNLXVVwE=
+github.com/go-openapi/jsonpointer v0.21.0 h1:YgdVicSA9vH5RiHs9TZW5oyafXZFc6+2Vc1rr/O9oNQ=
+github.com/go-openapi/jsonpointer v0.21.0/go.mod h1:IUyH9l/+uyhIYQ/PXVA41Rexl+kOkAPDdXEYns6fzUY=
+github.com/go-openapi/jsonreference v0.21.0 h1:Rs+Y7hSXT83Jacb7kFyjn4ijOuVGSvOdF2+tg1TRrwQ=
+github.com/go-openapi/jsonreference v0.21.0/go.mod h1:LmZmgsrTkVg9LG4EaHeY8cBDslNPMo06cago5JNLkm4=
+github.com/go-openapi/swag v0.23.0 h1:vsEVJDUo2hPJ2tu0/Xc+4noaxyEffXNIs3cOULZ+GrE=
+github.com/go-openapi/swag v0.23.0/go.mod h1:esZ8ITTYEsH1V2trKHjAN8Ai7xHb8RV+YSZ577vPjgQ=
 github.com/go-task/slim-sprig/v3 v3.0.0 h1:sUs3vkvUymDpBKi3qH1YSqBQk9+9D/8M2mN1vB6EwHI=
 github.com/go-task/slim-sprig/v3 v3.0.0/go.mod h1:W848ghGpv3Qj3dhTPRyJypKRiqCdHZiAzKg9hl15HA8=
 github.com/gogo/protobuf v1.3.2 h1:Ov1cvc58UF3b5XjBnZv7+opcTcQFZebYjWzi34vdm4Q=
 github.com/gogo/protobuf v1.3.2/go.mod h1:P1XiOD3dCwIKUDQYPy72D8LYyHL2YPYrpS2s69NZV8Q=
 github.com/golang/protobuf v1.5.4 h1:i7eJL8qZTpSEXOPTxNKhASYpMn+8e5Q6AdndVa1dWek=
 github.com/golang/protobuf v1.5.4/go.mod h1:lnTiLA8Wa4RWRcIUkrtSVa5nRhsEGBg48fD6rSs7xps=
-github.com/google/gnostic-models v0.6.8 h1:yo/ABAfM5IMRsS1VnXjTBvUb61tFIHozhlYvRgGre9I=
-github.com/google/gnostic-models v0.6.8/go.mod h1:5n7qKqH0f5wFt+aWF8CW6pZLLNOfYuF5OpfBSENuI8U=
+github.com/google/gnostic-models v0.6.9 h1:MU/8wDLif2qCXZmzncUQ/BOfxWfthHi63KqpoNbWqVw=
+github.com/google/gnostic-models v0.6.9/go.mod h1:CiWsm0s6BSQd1hRn8/QmxqB6BesYcbSZxsz9b0KuDBw=
 github.com/google/go-cmp v0.5.9/go.mod h1:17dUlkBOakJ0+DkrSSNjCkIjxS6bF9zb3elmeNGIjoY=
 github.com/google/go-cmp v0.6.0 h1:ofyhxvXcZhMsU5ulbFiLKl/XBFqE1GSq7atu8tAmTRI=
 github.com/google/go-cmp v0.6.0/go.mod h1:17dUlkBOakJ0+DkrSSNjCkIjxS6bF9zb3elmeNGIjoY=
 github.com/google/gofuzz v1.0.0/go.mod h1:dBl0BpW6vV/+mYPU4Po3pmUjxk6FQPldtuIdl/M65Eg=
 github.com/google/gofuzz v1.2.0 h1:xRy4A+RhZaiKjJ1bPfwQ8sedCA+YS2YcCHW6ec7JMi0=
 github.com/google/gofuzz v1.2.0/go.mod h1:dBl0BpW6vV/+mYPU4Po3pmUjxk6FQPldtuIdl/M65Eg=
-github.com/google/pprof v0.0.0-20240528025155-186aa0362fba h1:ql1qNgCyOB7iAEk8JTNM+zJrgIbnyCKX/wdlyPufP5g=
-github.com/google/pprof v0.0.0-20240528025155-186aa0362fba/go.mod h1:K1liHPHnj73Fdn/EKuT8nrFqBihUSKXoLYU0BuatOYo=
+github.com/google/pprof v0.0.0-20241029153458-d1b30febd7db h1:097atOisP2aRj7vFgYQBbFN4U4JNXUNYpxael3UzMyo=
+github.com/google/pprof v0.0.0-20241029153458-d1b30febd7db/go.mod h1:vavhavw2zAxS5dIdcRluK6cSGGPlZynqzFM8NdvU144=
 github.com/google/uuid v1.6.0 h1:NIvaJDMOsjHA8n1jAhLSgzrAzy1Hgr+hNrb57e+94F0=
 github.com/google/uuid v1.6.0/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=
 github.com/hashicorp/hcl v1.0.0 h1:0Anlzjpi4vEasTeNFn2mLJgTSwt0+6sfsiTG8qcWGx4=
 github.com/hashicorp/hcl v1.0.0/go.mod h1:E5yfLk+7swimpb2L/Alb/PJmXilQ/rhwaUYs4T20WEQ=
-github.com/imdario/mergo v0.3.6 h1:xTNEAn+kxVO7dTZGu0CegyqKZmoWFI0rF8UxjlB2d28=
-github.com/imdario/mergo v0.3.6/go.mod h1:2EnlNZ0deacrJVfApfmtdGgDfMuh/nq6Ok1EcJh5FfA=
 github.com/inconshreveable/mousetrap v1.1.0 h1:wN+x4NVGpMsO7ErUn/mUI3vEoE6Jt13X2s0bqwp9tc8=
 github.com/inconshreveable/mousetrap v1.1.0/go.mod h1:vpF70FUmC8bwa3OWnCshd2FqLfsEA9PFc4w1p2J65bw=
 github.com/jmespath/go-jmespath v0.4.0 h1:BEgLn5cpjn8UN1mAw4NjwDrS35OdebyEtFe+9YPoQUg=
@@ -59,10 +57,10 @@ github.com/kr/pretty v0.3.1 h1:flRD4NNwYAUpkphVc1HcthR4KEIFJ65n8Mw5qdRn3LE=
 github.com/kr/pretty v0.3.1/go.mod h1:hoEshYVHaxMs3cyo3Yncou5ZscifuDolrwPKZanG3xk=
 github.com/kr/text v0.2.0 h1:5Nx0Ya0ZqY2ygV366QzturHI13Jq95ApcVaJBhpS+AY=
 github.com/kr/text v0.2.0/go.mod h1:eLer722TekiGuMkidMxC/pM04lWEeraHUUmBw8l2grE=
-github.com/magiconair/properties v1.8.7 h1:IeQXZAiQcpL9mgcAe1Nu6cX9LLw6ExEHKjN0VQdvPDY=
-github.com/magiconair/properties v1.8.7/go.mod h1:Dhd985XPs7jluiymwWYZ0G4Z61jb3vdS329zhj2hYo0=
-github.com/mailru/easyjson v0.7.7 h1:UGYAvKxe3sBsEDzO8ZeWOSlIQfWFlxbzLZe7hwFURr0=
-github.com/mailru/easyjson v0.7.7/go.mod h1:xzfreul335JAWq5oZzymOObrkdz5UnU4kGfJJLY9Nlc=
+github.com/magiconair/properties v1.8.9 h1:nWcCbLq1N2v/cpNsy5WvQ37Fb+YElfq20WJ/a8RkpQM=
+github.com/magiconair/properties v1.8.9/go.mod h1:Dhd985XPs7jluiymwWYZ0G4Z61jb3vdS329zhj2hYo0=
+github.com/mailru/easyjson v0.9.0 h1:PrnmzHw7262yW8sTBwxi1PdJA3Iw/EKBa8psRf7d9a4=
+github.com/mailru/easyjson v0.9.0/go.mod h1:1+xMtQp2MRNVL/V1bOzuP3aP8VNwRW55fQUto+XFtTU=
 github.com/mitchellh/mapstructure v1.5.0 h1:jeMsZIYE/09sWLaz43PL7Gy6RuMjD2eJVyuac5Z2hdY=
 github.com/mitchellh/mapstructure v1.5.0/go.mod h1:bFUtVrKA4DC2yAKiSyO/QUcy7e+RRV2QTWOzhPopBRo=
 github.com/modern-go/concurrent v0.0.0-20180228061459-e0a39a4cb421/go.mod h1:6dJC0mAP4ikYIbvyc7fijjWJddQyLn8Ig3JB5CqoB9Q=
@@ -72,24 +70,28 @@ github.com/modern-go/reflect2 v1.0.2 h1:xBagoLtFs94CBntxluKeaWgTMpvLxC4ur3nMaC9G
 github.com/modern-go/reflect2 v1.0.2/go.mod h1:yWuevngMOJpCy52FWWMvUC8ws7m/LJsjYzDa0/r8luk=
 github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822 h1:C3w9PqII01/Oq1c1nUAm88MOHcQC9l5mIlSMApZMrHA=
 github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822/go.mod h1:+n7T8mK8HuQTcFwEeznm/DIxMOiR9yIdICNftLE1DvQ=
-github.com/onsi/ginkgo/v2 v2.19.0 h1:9Cnnf7UHo57Hy3k6/m5k3dRfGTMXGvxhHFvkDTCTpvA=
-github.com/onsi/ginkgo/v2 v2.19.0/go.mod h1:rlwLi9PilAFJ8jCg9UE1QP6VBpd6/xj3SRC0d6TU0To=
-github.com/onsi/gomega v1.19.0 h1:4ieX6qQjPP/BfC3mpsAtIGGlxTWPeA3Inl/7DtXw1tw=
-github.com/onsi/gomega v1.19.0/go.mod h1:LY+I3pBVzYsTBU1AnDwOSxaYi9WoWiqgwooUqq9yPro=
-github.com/pelletier/go-toml/v2 v2.2.2 h1:aYUidT7k73Pcl9nb2gScu7NSrKCSHIDE89b3+6Wq+LM=
-github.com/pelletier/go-toml/v2 v2.2.2/go.mod h1:1t835xjRzz80PqgE6HHgN2JOsmgYu/h4qDAS4n929Rs=
+github.com/onsi/ginkgo/v2 v2.21.0 h1:7rg/4f3rB88pb5obDgNZrNHrQ4e6WpjonchcpuBRnZM=
+github.com/onsi/ginkgo/v2 v2.21.0/go.mod h1:7Du3c42kxCUegi0IImZ1wUQzMBVecgIHjR1C+NkhLQo=
+github.com/onsi/gomega v1.35.1 h1:Cwbd75ZBPxFSuZ6T+rN/WCb/gOc6YgFBXLlZLhC7Ds4=
+github.com/onsi/gomega v1.35.1/go.mod h1:PvZbdDc8J6XJEpDK4HCuRBm8a6Fzp9/DmhC9C7yFlog=
+github.com/pelletier/go-toml/v2 v2.2.3 h1:YmeHyLY8mFWbdkNWwpr+qIL2bEqT0o95WSdkNHvL12M=
+github.com/pelletier/go-toml/v2 v2.2.3/go.mod h1:MfCQTFTvCcUyyvvwm1+G6H/jORL20Xlb6rzQu9GuUkc=
+github.com/pkg/errors v0.9.1 h1:FEBLx1zS214owpjy7qsBeixbURkuhQAwrK5UwLGTwt4=
+github.com/pkg/errors v0.9.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=
 github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=
 github.com/pmezard/go-difflib v1.0.1-0.20181226105442-5d4384ee4fb2 h1:Jamvg5psRIccs7FGNTlIRMkT8wgtp5eCXdBlqhYGL6U=
 github.com/pmezard/go-difflib v1.0.1-0.20181226105442-5d4384ee4fb2/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=
 github.com/prometheus/client_model v0.6.1 h1:ZKSh/rekM+n3CeS952MLRAdFwIKqeY8b62p8ais2e9E=
 github.com/prometheus/client_model v0.6.1/go.mod h1:OrxVMOVHjw3lKMa8+x6HeMGkHMQyHDk9E3jmP2AmGiY=
-github.com/prometheus/common v0.55.0 h1:KEi6DK7lXW/m7Ig5i47x0vRzuBsHuvJdi5ee6Y3G1dc=
-github.com/prometheus/common v0.55.0/go.mod h1:2SECS4xJG1kd8XF9IcM1gMX6510RAEL65zxzNImwdc8=
+github.com/prometheus/common v0.61.0 h1:3gv/GThfX0cV2lpO7gkTUwZru38mxevy90Bj8YFSRQQ=
+github.com/prometheus/common v0.61.0/go.mod h1:zr29OCN/2BsJRaFwG8QOBr41D6kkchKbpeNH7pAjb/s=
+github.com/robfig/cron/v3 v3.0.1 h1:WdRxkvbJztn8LMz/QEvLN5sBU+xKpSqwwUO1Pjr4qDs=
+github.com/robfig/cron/v3 v3.0.1/go.mod h1:eQICP3HwyT7UooqI/z+Ov+PtYAWygg1TEWWzGIFLtro=
 github.com/rogpeppe/go-internal v1.12.0 h1:exVL4IDcn6na9z1rAb56Vxr+CgyK3nn3O+epU5NdKM8=
 github.com/rogpeppe/go-internal v1.12.0/go.mod h1:E+RYuTGaKKdloAfM02xzb0FW3Paa99yedzYV+kq4uf4=
 github.com/russross/blackfriday/v2 v2.1.0/go.mod h1:+Rmxgy9KzJVeS9/2gXHxylqXiyQDYRxCVz55jmeOWTM=
-github.com/sagikazarmark/locafero v0.4.0 h1:HApY1R9zGo4DBgr7dqsTH/JJxLTTsOt7u6keLGt6kNQ=
-github.com/sagikazarmark/locafero v0.4.0/go.mod h1:Pe1W6UlPYUk/+wc/6KFhbORCfqzgYEpgQ3O5fPuL3H4=
+github.com/sagikazarmark/locafero v0.6.0 h1:ON7AQg37yzcRPU69mt7gwhFEBwxI6P9T4Qu3N51bwOk=
+github.com/sagikazarmark/locafero v0.6.0/go.mod h1:77OmuIc6VTraTXKXIs/uvUxKGUXjE1GbemJYHqdNjX0=
 github.com/sagikazarmark/slog-shim v0.1.0 h1:diDBnUNK9N/354PgrxMywXnAwEr1QZcOr6gto+ugjYE=
 github.com/sagikazarmark/slog-shim v0.1.0/go.mod h1:SrcSrq8aKtyuqEI1uvTDTK1arOWRIczQRv+GVI1AkeQ=
 github.com/sirupsen/logrus v1.9.3 h1:dueUQJ1C2q9oE3F7wvmSGAaVtTmUizReu6fjN8uqzbQ=
@@ -98,8 +100,8 @@ github.com/sourcegraph/conc v0.3.0 h1:OQTbbt6P72L20UqAkXXuLOj79LfEanQ+YQFNpLA9yS
 github.com/sourcegraph/conc v0.3.0/go.mod h1:Sdozi7LEKbFPqYX2/J+iBAM6HpqSLTASQIKqDmF7Mt0=
 github.com/spf13/afero v1.11.0 h1:WJQKhtpdm3v2IzqG8VMqrr6Rf3UYpEF239Jy9wNepM8=
 github.com/spf13/afero v1.11.0/go.mod h1:GH9Y3pIexgf1MTIWtNGyogA5MwRIDXGUr+hbWNoBjkY=
-github.com/spf13/cast v1.6.0 h1:GEiTHELF+vaR5dhz3VqZfFSzZjYbgeKDpBxQVS4GYJ0=
-github.com/spf13/cast v1.6.0/go.mod h1:ancEpBxwJDODSW/UG4rDrAqiKolqNNh2DX3mk86cAdo=
+github.com/spf13/cast v1.7.0 h1:ntdiHjuueXFgm5nzDRdOS4yfT43P5Fnud6DH50rz/7w=
+github.com/spf13/cast v1.7.0/go.mod h1:ancEpBxwJDODSW/UG4rDrAqiKolqNNh2DX3mk86cAdo=
 github.com/spf13/cobra v1.8.1 h1:e5/vxKd/rZsfSJMUX1agtjeTDf+qv1/JdBF8gg5k9ZM=
 github.com/spf13/cobra v1.8.1/go.mod h1:wHxEcudfqmLYa8iTfL+OuZPbBZkmvliBWKIezN3kD9Y=
 github.com/spf13/pflag v1.0.5 h1:iy+VFUOCP1a+8yFto/drg2CJ5u0yRoB7fZw3DKv/JXA=
@@ -107,16 +109,10 @@ github.com/spf13/pflag v1.0.5/go.mod h1:McXfInJRrz4CZXVZOBLb0bTZqETkiAhM9Iw0y3An
 github.com/spf13/viper v1.19.0 h1:RWq5SEjt8o25SROyN3z2OrDB9l7RPd3lwTWU8EcEdcI=
 github.com/spf13/viper v1.19.0/go.mod h1:GQUN9bilAbhU/jgc1bKs99f/suXKeUMct8Adx5+Ntkg=
 github.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=
-github.com/stretchr/objx v0.4.0/go.mod h1:YvHI0jy2hoMjB+UWwv71VJQ9isScKT/TqJzVSSt89Yw=
-github.com/stretchr/objx v0.5.0/go.mod h1:Yh+to48EsGEfYuaHDzXPcE3xhTkx73EhmCGUpEOglKo=
-github.com/stretchr/objx v0.5.2/go.mod h1:FRsXN1f5AsAjCGJKqEizvkpNtU+EGNCLh3NxZ/8L+MA=
 github.com/stretchr/testify v1.3.0/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn+9EI=
 github.com/stretchr/testify v1.7.0/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=
-github.com/stretchr/testify v1.7.1/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=
-github.com/stretchr/testify v1.8.0/go.mod h1:yNjHg4UonilssWZ8iaSj1OCr/vHnekPRkoO+kdMU+MU=
-github.com/stretchr/testify v1.8.4/go.mod h1:sz/lmYIOXD/1dqDmKjjqLyZ2RngseejIcXlSw2iwfAo=
-github.com/stretchr/testify v1.9.0 h1:HtqpIVDClZ4nwg75+f6Lvsy/wHu+3BoSGCbBAcpTsTg=
-github.com/stretchr/testify v1.9.0/go.mod h1:r2ic/lqez/lEtzL7wO/rwa5dbSLXVDPFyf8C91i36aY=
+github.com/stretchr/testify v1.10.0 h1:Xv5erBjTwe/5IxqUQTdXv5kgmIvbHo3QQyRwhJsOfJA=
+github.com/stretchr/testify v1.10.0/go.mod h1:r2ic/lqez/lEtzL7wO/rwa5dbSLXVDPFyf8C91i36aY=
 github.com/subosito/gotenv v1.6.0 h1:9NlTDc1FTs4qu0DDq7AEtTPNw6SVm7uBMsUCUjABIf8=
 github.com/subosito/gotenv v1.6.0/go.mod h1:Dk4QP5c2W3ibzajGcXpNraDfq2IrhjMIvMSWPKKo0FU=
 github.com/x448/float16 v0.8.4 h1:qLwI1I70+NjRFUR3zs1JPUCgaCXSh3SW62uAKT1mSBM=
@@ -128,52 +124,52 @@ go.uber.org/multierr v1.11.0/go.mod h1:20+QtiLqy0Nd6FdQB9TLXag12DsQkrbs3htMFfDN8
 golang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=
 golang.org/x/crypto v0.0.0-20191011191535-87dc89f01550/go.mod h1:yigFU9vqHzYiE8UmvKecakEJjdnWj3jj499lnFckfCI=
 golang.org/x/crypto v0.0.0-20200622213623-75b288015ac9/go.mod h1:LzIPMQfyMNhhGPhUkYOs5KpL4U8rLKemX1yGLhDgUto=
-golang.org/x/exp v0.0.0-20240119083558-1b970713d09a h1:Q8/wZp0KX97QFTc2ywcOE0YRjZPVIx+MXInMzdvQqcA=
-golang.org/x/exp v0.0.0-20240119083558-1b970713d09a/go.mod h1:idGWGoKP1toJGkd5/ig9ZLuPcZBC3ewk7SzmH0uou08=
+golang.org/x/exp v0.0.0-20241215155358-4a5509556b9e h1:4qufH0hlUYs6AO6XmZC3GqfDPGSXHVXUFR6OND+iJX4=
+golang.org/x/exp v0.0.0-20241215155358-4a5509556b9e/go.mod h1:qj5a5QZpwLU2NLQudwIN5koi3beDhSAlJwa67PuM98c=
 golang.org/x/mod v0.2.0/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=
 golang.org/x/mod v0.3.0/go.mod h1:s0Qsj1ACt9ePp/hMypM3fl4fZqREWJwdYDEqhRiZZUA=
 golang.org/x/net v0.0.0-20190404232315-eb5bcb51f2a3/go.mod h1:t9HGtf8HONx5eT2rtn7q6eTqICYqUVnKs3thJo3Qplg=
 golang.org/x/net v0.0.0-20190620200207-3b0461eec859/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=
 golang.org/x/net v0.0.0-20200226121028-0de0cce0169b/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=
 golang.org/x/net v0.0.0-20201021035429-f5854403a974/go.mod h1:sp8m0HH+o8qH0wwXwYZr8TS3Oi6o0r6Gce1SSxlDquU=
-golang.org/x/net v0.26.0 h1:soB7SVo0PWrY4vPW/+ay0jKDNScG2X9wFeYlXIvJsOQ=
-golang.org/x/net v0.26.0/go.mod h1:5YKkiSynbBIh3p6iOc/vibscux0x38BZDkn8sCUPxHE=
-golang.org/x/oauth2 v0.21.0 h1:tsimM75w1tF/uws5rbeHzIWxEqElMehnc+iW793zsZs=
-golang.org/x/oauth2 v0.21.0/go.mod h1:XYTD2NtWslqkgxebSiOHnXEap4TF09sJSc7H1sXbhtI=
+golang.org/x/net v0.32.0 h1:ZqPmj8Kzc+Y6e0+skZsuACbx+wzMgo5MQsJh9Qd6aYI=
+golang.org/x/net v0.32.0/go.mod h1:CwU0IoeOlnQQWJ6ioyFrfRuomB8GKF6KbYXZVyeXNfs=
+golang.org/x/oauth2 v0.24.0 h1:KTBBxWqUa0ykRPLtV69rRto9TLXcqYkeswu48x/gvNE=
+golang.org/x/oauth2 v0.24.0/go.mod h1:XYTD2NtWslqkgxebSiOHnXEap4TF09sJSc7H1sXbhtI=
 golang.org/x/sync v0.0.0-20190423024810-112230192c58/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
 golang.org/x/sync v0.0.0-20190911185100-cd5d95a43a6e/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
 golang.org/x/sync v0.0.0-20201020160332-67f06af15bc9/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
-golang.org/x/sync v0.7.0 h1:YsImfSBoP9QPYL0xyKJPq0gcaJdG3rInoqxTWbfQu9M=
-golang.org/x/sync v0.7.0/go.mod h1:Czt+wKu1gCyEFDUtn0jG5QVvpJ6rzVqr5aXyt9drQfk=
 golang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=
 golang.org/x/sys v0.0.0-20190412213103-97732733099d/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
 golang.org/x/sys v0.0.0-20200930185726-fdedc70b468f/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
 golang.org/x/sys v0.0.0-20220715151400-c0bba94af5f8/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
-golang.org/x/sys v0.21.0 h1:rF+pYz3DAGSQAxAu1CbC7catZg4ebC4UIeIhKxBZvws=
-golang.org/x/sys v0.21.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=
-golang.org/x/term v0.21.0 h1:WVXCp+/EBEHOj53Rvu+7KiT/iElMrO8ACK16SMZ3jaA=
-golang.org/x/term v0.21.0/go.mod h1:ooXLefLobQVslOqselCNF4SxFAaoS6KujMbsGzSDmX0=
+golang.org/x/sys v0.28.0 h1:Fksou7UEQUWlKvIdsqzJmUmCX3cZuD2+P3XyyzwMhlA=
+golang.org/x/sys v0.28.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=
+golang.org/x/term v0.27.0 h1:WP60Sv1nlK1T6SupCHbXzSaN0b9wUmsPoRS9b61A23Q=
+golang.org/x/term v0.27.0/go.mod h1:iMsnZpn0cago0GOrHO2+Y7u7JPn5AylBrcoWkElMTSM=
 golang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=
 golang.org/x/text v0.3.3/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=
-golang.org/x/text v0.16.0 h1:a94ExnEXNtEwYLGJSIUxnWoxoRz/ZcCsV63ROupILh4=
-golang.org/x/text v0.16.0/go.mod h1:GhwF1Be+LQoKShO3cGOHzqOgRrGaYc9AvblQOmPVHnI=
-golang.org/x/time v0.5.0 h1:o7cqy6amK/52YcAKIPlM3a+Fpj35zvRj2TP+e1xFSfk=
-golang.org/x/time v0.5.0/go.mod h1:3BpzKBy/shNhVucY/MWOyx10tF3SFh9QdLuxbVysPQM=
+golang.org/x/text v0.21.0 h1:zyQAAkrwaneQ066sspRyJaG9VNi/YJ1NfzcGB3hZ/qo=
+golang.org/x/text v0.21.0/go.mod h1:4IBbMaMmOPCJ8SecivzSH54+73PCFmPWxNTLm+vZkEQ=
+golang.org/x/time v0.8.0 h1:9i3RxcPv3PZnitoVGMPDKZSq1xW1gK1Xy3ArNOGZfEg=
+golang.org/x/time v0.8.0/go.mod h1:3BpzKBy/shNhVucY/MWOyx10tF3SFh9QdLuxbVysPQM=
 golang.org/x/tools v0.0.0-20180917221912-90fa682c2a6e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=
 golang.org/x/tools v0.0.0-20191119224855-298f0cb1881e/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=
 golang.org/x/tools v0.0.0-20200619180055-7c47624df98f/go.mod h1:EkVYQZoAsY45+roYkvgYkIh4xh/qjgUK9TdY2XT94GE=
 golang.org/x/tools v0.0.0-20210106214847-113979e3529a/go.mod h1:emZCQorbCU4vsT4fOWvOPXz4eW1wZW4PmDk9uLelYpA=
-golang.org/x/tools v0.22.0 h1:gqSGLZqv+AI9lIQzniJ0nZDRG5GBPsSi+DRNHWNz6yA=
-golang.org/x/tools v0.22.0/go.mod h1:aCwcsjqvq7Yqt6TNyX7QMU2enbQ/Gt0bo6krSeEri+c=
+golang.org/x/tools v0.28.0 h1:WuB6qZ4RPCQo5aP3WdKZS7i595EdWqWR8vqJTlwTVK8=
+golang.org/x/tools v0.28.0/go.mod h1:dcIOrVd3mfQKTgrDVQHqCPMWy6lnhfhtX3hLXYVLfRw=
 golang.org/x/xerrors v0.0.0-20190717185122-a985d3407aa7/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
 golang.org/x/xerrors v0.0.0-20191011141410-1b5146add898/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
 golang.org/x/xerrors v0.0.0-20191204190536-9bdfabe68543/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
 golang.org/x/xerrors v0.0.0-20200804184101-5ec99f83aff1/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
-google.golang.org/protobuf v1.34.2 h1:6xV6lTsCfpGD21XK49h7MhtcApnLqkfYgPcdHftf6hg=
-google.golang.org/protobuf v1.34.2/go.mod h1:qYOHts0dSfpeUzUFpOMr/WGzszTmLH+DiWniOlNbLDw=
+google.golang.org/protobuf v1.35.2 h1:8Ar7bF+apOIoThw1EdZl0p1oWvMqTHmpA2fRTyZO8io=
+google.golang.org/protobuf v1.35.2/go.mod h1:9fA7Ob0pmnwhb644+1+CVWFRbNajQ6iRojtC/QF5bRE=
 gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
 gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c h1:Hei/4ADfdWqJk1ZMxUNpqntNwaWcugrBjAiHlqqRiVk=
 gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c/go.mod h1:JHkPIbrfpd72SG/EVd6muEfDQjcINNoR0C8j2r3qZ4Q=
+gopkg.in/evanphx/json-patch.v4 v4.12.0 h1:n6jtcsulIzXPJaxegRbvFNNrZDjbij7ny3gmSPG+6V4=
+gopkg.in/evanphx/json-patch.v4 v4.12.0/go.mod h1:p8EYWUEYMpynmqDbY58zCKCFZw8pRWMG4EsWvDvM72M=
 gopkg.in/inf.v0 v0.9.1 h1:73M5CoZyi3ZLMOyDlQh031Cx6N9NDJ2Vvfl76EDAgDc=
 gopkg.in/inf.v0 v0.9.1/go.mod h1:cWUDdTG/fYaXco+Dcufb5Vnc6Gp2YChqWtbxRZE0mXw=
 gopkg.in/ini.v1 v1.67.0 h1:Dgnx+6+nfE+IfzjUEISNeydPJh9AXNNsWbGP9KzCsOA=
@@ -184,23 +180,21 @@ gopkg.in/yaml.v2 v2.4.0/go.mod h1:RDklbk79AGWmwhnvt/jBztapEOGDOx6ZbXqjP6csGnQ=
 gopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
 gopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=
 gopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
-k8s.io/api v0.31.1 h1:Xe1hX/fPW3PXYYv8BlozYqw63ytA92snr96zMW9gWTU=
-k8s.io/api v0.31.1/go.mod h1:sbN1g6eY6XVLeqNsZGLnI5FwVseTrZX7Fv3O26rhAaI=
-k8s.io/apimachinery v0.31.1 h1:mhcUBbj7KUjaVhyXILglcVjuS4nYXiwC+KKFBgIVy7U=
-k8s.io/apimachinery v0.31.1/go.mod h1:rsPdaZJfTfLsNJSQzNHQvYoTmxhoOEofxtOsF3rtsMo=
-k8s.io/client-go v0.31.1 h1:f0ugtWSbWpxHR7sjVpQwuvw9a3ZKLXX0u0itkFXufb0=
-k8s.io/client-go v0.31.1/go.mod h1:sKI8871MJN2OyeqRlmA4W4KM9KBdBUpDLu/43eGemCg=
+k8s.io/api v0.32.0 h1:OL9JpbvAU5ny9ga2fb24X8H6xQlVp+aJMFlgtQjR9CE=
+k8s.io/api v0.32.0/go.mod h1:4LEwHZEf6Q/cG96F3dqR965sYOfmPM7rq81BLgsE0p0=
+k8s.io/apimachinery v0.32.0 h1:cFSE7N3rmEEtv4ei5X6DaJPHHX0C+upp+v5lVPiEwpg=
+k8s.io/apimachinery v0.32.0/go.mod h1:GpHVgxoKlTxClKcteaeuF1Ul/lDVb74KpZcxcmLDElE=
+k8s.io/client-go v0.32.0 h1:DimtMcnN/JIKZcrSrstiwvvZvLjG0aSxy8PxN8IChp8=
+k8s.io/client-go v0.32.0/go.mod h1:boDWvdM1Drk4NJj/VddSLnx59X3OPgwrOo0vGbtq9+8=
 k8s.io/klog/v2 v2.130.1 h1:n9Xl7H1Xvksem4KFG4PYbdQCQxqc/tTUyrgXaOhHSzk=
 k8s.io/klog/v2 v2.130.1/go.mod h1:3Jpz1GvMt720eyJH1ckRHK1EDfpxISzJ7I9OYgaDtPE=
-k8s.io/kube-openapi v0.0.0-20240228011516-70dd3763d340 h1:BZqlfIlq5YbRMFko6/PM7FjZpUb45WallggurYhKGag=
-k8s.io/kube-openapi v0.0.0-20240228011516-70dd3763d340/go.mod h1:yD4MZYeKMBwQKVht279WycxKyM84kkAx2DPrTXaeb98=
-k8s.io/metrics v0.31.1 h1:h4I4dakgh/zKflWYAOQhwf0EXaqy8LxAIyE/GBvxqRc=
-k8s.io/metrics v0.31.1/go.mod h1:JuH1S9tJiH9q1VCY0yzSCawi7kzNLsDzlWDJN4xR+iA=
-k8s.io/utils v0.0.0-20240711033017-18e509b52bc8 h1:pUdcCO1Lk/tbT5ztQWOBi5HBgbBP1J8+AsQnQCKsi8A=
-k8s.io/utils v0.0.0-20240711033017-18e509b52bc8/go.mod h1:OLgZIPagt7ERELqWJFomSt595RzquPNLL48iOWgYOg0=
-sigs.k8s.io/json v0.0.0-20221116044647-bc3834ca7abd h1:EDPBXCAspyGV4jQlpZSudPeMmr1bNJefnuqLsRAsHZo=
-sigs.k8s.io/json v0.0.0-20221116044647-bc3834ca7abd/go.mod h1:B8JuhiUyNFVKdsE8h686QcCxMaH6HrOAZj4vswFpcB0=
-sigs.k8s.io/structured-merge-diff/v4 v4.4.1 h1:150L+0vs/8DA78h1u02ooW1/fFq/Lwr+sGiqlzvrtq4=
-sigs.k8s.io/structured-merge-diff/v4 v4.4.1/go.mod h1:N8hJocpFajUSSeSJ9bOZ77VzejKZaXsTtZo4/u7Io08=
+k8s.io/kube-openapi v0.0.0-20241212222426-2c72e554b1e7 h1:hcha5B1kVACrLujCKLbr8XWMxCxzQx42DY8QKYJrDLg=
+k8s.io/kube-openapi v0.0.0-20241212222426-2c72e554b1e7/go.mod h1:GewRfANuJ70iYzvn+i4lezLDAFzvjxZYK1gn1lWcfas=
+k8s.io/utils v0.0.0-20241210054802-24370beab758 h1:sdbE21q2nlQtFh65saZY+rRM6x6aJJI8IUa1AmH/qa0=
+k8s.io/utils v0.0.0-20241210054802-24370beab758/go.mod h1:OLgZIPagt7ERELqWJFomSt595RzquPNLL48iOWgYOg0=
+sigs.k8s.io/json v0.0.0-20241014173422-cfa47c3a1cc8 h1:gBQPwqORJ8d8/YNZWEjoZs7npUVDpVXUUOFfW6CgAqE=
+sigs.k8s.io/json v0.0.0-20241014173422-cfa47c3a1cc8/go.mod h1:mdzfpAEoE6DHQEN0uh9ZbOCuHbLK5wOm7dK4ctXE9Tg=
+sigs.k8s.io/structured-merge-diff/v4 v4.5.0 h1:nbCitCK2hfnhyiKo6uf2HxUPTCodY6Qaf85SbDIaMBk=
+sigs.k8s.io/structured-merge-diff/v4 v4.5.0/go.mod h1:N8f93tFZh9U6vpxwRArLiikrE5/2tiu1w1AGfACIGE4=
 sigs.k8s.io/yaml v1.4.0 h1:Mk1wCc2gy/F0THH0TAp1QYyJNzRm2KCLy3o5ASXVI5E=
 sigs.k8s.io/yaml v1.4.0/go.mod h1:Ejl7/uTz7PSA4eKMyQCUTnhZYNmLIl+5c2lQPGR2BPY=
diff --git a/main.go b/main.go
index 41acfe2..9031e29 100644
--- a/main.go
+++ b/main.go
@@ -23,7 +23,7 @@ import (
 	"github.com/spf13/cobra"
 	"github.com/spf13/viper"
 	"k8s.io/client-go/kubernetes"
-	metricsv "k8s.io/metrics/pkg/client/clientset/versioned"
+	"k8s.io/client-go/rest"
 
 	"github.com/vegacloud/kubernetes/metricsagent/pkg/agent"
 	"github.com/vegacloud/kubernetes/metricsagent/pkg/config"
@@ -107,24 +107,13 @@ func runRootCmd(cmd *cobra.Command, _ []string) error {
 		logrus.Debugf("Configuration: %+v", cfg)
 	}
 
-	ctx := context.Background()
-	k8sClientConfig, err := initializeKubernetesClients(ctx, cfg)
-	if err != nil {
-		return err
-	}
-
-	metricsClientset, err := metricsv.NewForConfig(k8sClientConfig.Config)
-	if err != nil {
-		return fmt.Errorf("failed to create metrics client: %w", err)
-	}
-
 	logger := logrus.WithFields(logrus.Fields{
 		"client_id":    cfg.VegaClientID,
 		"org_slug":     cfg.VegaOrgSlug,
 		"cluster_name": cfg.VegaClusterName,
 	})
 
-	if err := startMetricsAgent(cfg, k8sClientConfig.Clientset, metricsClientset, logger, *k8sClientConfig); err != nil {
+	if err := startMetricsAgent(cfg, logger); err != nil {
 		return err
 	}
 
@@ -143,47 +132,120 @@ func initializeLogging(cfg *config.Config) {
 	})
 }
 
-func initializeKubernetesClients(ctx context.Context, cfg *config.Config) (*utils.K8sClientConfig, error) {
-	clientConfig, err := utils.GetClientConfig(ctx, cfg)
+func initializeKubernetesClients(ctx context.Context, cfg *config.Config, logger *logrus.Entry) (*kubernetes.Clientset, *utils.K8sClientConfig, error) {
+	logger.Debug("Starting Kubernetes client initialization...")
+
+	// Get Kubernetes client configuration
+	logger.Debug("Getting Kubernetes client configuration...")
+	// Add functionality to verify we can access the service token
+	token, err := os.ReadFile(cfg.VegaBearerTokenPath)
+	if err != nil {
+		return nil, nil, fmt.Errorf("failed to read service token: %w", err)
+	}
+	logger.WithField("token", string(token)).Debug("Successfully read service token")
+
+	k8sConfig, err := utils.GetClientConfig(ctx, cfg)
 	if err != nil {
-		return nil, fmt.Errorf("failed to create Kubernetes client: %w", err)
+		logger.WithError(err).Error("Failed to get Kubernetes client configuration")
+		return nil, nil, fmt.Errorf("failed to get Kubernetes client config: %w", err)
 	}
-	return clientConfig, nil
+
+	logger.WithFields(logrus.Fields{
+		"host":              k8sConfig.Config.Host,
+		"bearer_token_path": k8sConfig.Config.BearerTokenFile,
+		"qps":               k8sConfig.Config.QPS,
+		"burst":             k8sConfig.Config.Burst,
+		"timeout":           k8sConfig.Config.Timeout,
+	}).Debug("Successfully obtained Kubernetes client configuration")
+
+	return k8sConfig.Clientset, k8sConfig, nil
 }
 
-func startMetricsAgent(cfg *config.Config,
-	clientset kubernetes.Interface,
-	metricsClientset *metricsv.Clientset,
-	logger *logrus.Entry,
-	k8sConfig utils.K8sClientConfig,
-) error {
-	metricsAgent, err := agent.NewMetricsAgent(cfg, clientset, metricsClientset, logger, k8sConfig)
+func startMetricsAgent(cfg *config.Config, logger *logrus.Entry) error {
+	logger.Debug("Initializing metrics agent...")
+
+	// Configure TLS transport
+	logger.Debug("Configuring TLS transport...")
+	transport, err := rest.TransportFor(&rest.Config{
+		TLSClientConfig: rest.TLSClientConfig{
+			Insecure: cfg.VegaInsecure,
+		},
+	})
+	if err != nil {
+		logger.WithError(err).Error("Failed to create TLS transport")
+		return fmt.Errorf("failed to create transport: %w", err)
+	}
+	logger.WithField("insecure", cfg.VegaInsecure).Debug("Successfully created TLS transport")
+
+	// Initialize Kubernetes clients
+	clientset, k8sConfig, err := initializeKubernetesClients(context.Background(), cfg, logger)
+	if err != nil {
+		return err
+	}
+
+	// Add detailed k8s config logging here
+	logger.WithFields(logrus.Fields{
+		"cluster_host_url":      k8sConfig.ClusterHostURL,
+		"cluster_uid":           k8sConfig.ClusterUID,
+		"cluster_version":       k8sConfig.ClusterVersion,
+		"namespace":             k8sConfig.Namespace,
+		"use_in_cluster_config": k8sConfig.UseInClusterConfig,
+		"qps":                   k8sConfig.Config.QPS,
+		"burst":                 k8sConfig.Config.Burst,
+		"timeout":               k8sConfig.Config.Timeout,
+		"bearer_token_file":     k8sConfig.Config.BearerTokenFile,
+		"ca_file":               k8sConfig.Config.CAFile,
+		"server_name":           k8sConfig.Config.ServerName,
+		"insecure_skip_verify":  k8sConfig.Config.Insecure,
+	}).Info("Kubernetes configuration details")
+
+	// Configure TLS for the REST client
+	if client, ok := clientset.CoreV1().RESTClient().(*rest.RESTClient); ok {
+		logger.Debug("Configuring TLS for kubelet client")
+		client.Client.Transport = transport
+		logger.WithFields(logrus.Fields{
+			"insecure":       cfg.VegaInsecure,
+			"transport_type": fmt.Sprintf("%T", transport),
+		}).Debug("Successfully configured TLS for kubelet client")
+	} else {
+		logger.Warn("Unable to configure TLS: unexpected REST client type")
+	}
+
+	// Create metrics agent
+	logger.Debug("Creating metrics agent...")
+	// os.Exit(1)
+	metricsAgent, err := agent.NewMetricsAgent(cfg, logger)
 	if err != nil {
+		logger.WithError(err).Error("Failed to create metrics agent")
 		return fmt.Errorf("failed to create metrics agent: %w", err)
 	}
+	logger.Debug("Successfully created metrics agent")
 
 	ctx, cancel := context.WithCancel(context.Background())
 	defer cancel()
 
+	// Start health check server
 	go func() {
-		logrus.Println("Starting liveness probe server on :80")
+		logger.Info("Starting liveness probe server on :80")
 		if err := health.ServerHealthCheck(ctx); err != nil {
-			logger.Error("Health check startup failed")
+			logger.WithError(err).Error("Health check startup failed")
 			cancel()
 		} else {
 			logger.Info("Health check successful")
 		}
 	}()
 
+	// Handle signals
 	sigs := make(chan os.Signal, 1)
 	signal.Notify(sigs, syscall.SIGINT, syscall.SIGTERM)
 
 	go func() {
 		sig := <-sigs
-		logger.Infof("Received signal %s, initiating shutdown", sig)
+		logger.WithField("signal", sig).Info("Received signal, initiating shutdown")
 		cancel()
 	}()
 
+	logger.Info("Starting metrics agent main loop")
 	metricsAgent.Run(ctx)
 
 	return nil
diff --git a/pkg/agent/agent.go b/pkg/agent/agent.go
index 48a0353..67ead83 100644
--- a/pkg/agent/agent.go
+++ b/pkg/agent/agent.go
@@ -18,16 +18,19 @@ import (
 	"fmt"
 	"io"
 	"net/http"
+	"os"
 	"sync"
 	"time"
 
 	"github.com/sirupsen/logrus"
-	"k8s.io/client-go/kubernetes"
-	metricsclientset "k8s.io/metrics/pkg/client/clientset/versioned"
+
+	"crypto/tls"
 
 	"github.com/vegacloud/kubernetes/metricsagent/pkg/collectors"
 	"github.com/vegacloud/kubernetes/metricsagent/pkg/config"
 	"github.com/vegacloud/kubernetes/metricsagent/pkg/utils"
+	"k8s.io/client-go/rest"
+	"k8s.io/client-go/transport"
 )
 
 // MetricsAgent is the main struct for the metrics agent
@@ -41,53 +44,50 @@ type MetricsAgent struct {
 
 // NewMetricsAgent creates a new MetricsAgent
 func NewMetricsAgent(cfg *config.Config,
-	clientset kubernetes.Interface,
-	metricsClientset metricsclientset.Interface,
 	logger *logrus.Entry,
-	k8sConfig utils.K8sClientConfig,
 ) (*MetricsAgent, error) {
 	logger = logger.WithField("function", "NewMetricsAgent")
 
-	k8sClientset, ok := clientset.(*kubernetes.Clientset)
-	if !ok {
-		return nil, fmt.Errorf("invalid clientset type: expected *kubernetes.Clientset, got %T", clientset)
-	}
-	// Create an empty map to hold the collectors
-	collectorsMap := make(map[string]collectors.Collector)
-	metricsClientsetImpl, ok := metricsClientset.(*metricsclientset.Clientset)
-	if !ok {
-		return nil, fmt.Errorf(
-			"invalid metrics clientset type: expected *metricsclientset.Clientset, got %T",
-			metricsClientset,
-		)
+	// Get existing client config
+	clientConfig, err := utils.GetExistingClientConfig()
+	if err != nil {
+		return nil, fmt.Errorf("failed to get existing client config: %w", err)
 	}
 
-	// Initialize the NodeCollector
-	nodeCollector, err := collectors.NewNodeCollector(k8sClientset, metricsClientsetImpl, cfg)
+	// Configure transport once for the clientset
+	token, err := os.ReadFile("/var/run/secrets/kubernetes.io/serviceaccount/token")
 	if err != nil {
-		logger.Fatalf("Failed to create NodeCollector: %v", err)
+		return nil, fmt.Errorf("failed to read service account token: %w", err)
+	}
+
+	clientConfig.Clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport = &http.Transport{
+		TLSClientConfig: &tls.Config{
+			InsecureSkipVerify: cfg.VegaInsecure,
+		},
 	}
-	collectorsMap["node"] = nodeCollector
 
-	// Initialize the PodCollector (assuming it doesn't return an error)
-	podCollector := collectors.NewPodCollector(k8sClientset, metricsClientsetImpl, cfg)
-	collectorsMap["pod"] = podCollector
+	clientConfig.Clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport = transport.NewBearerAuthRoundTripper(
+		string(token),
+		clientConfig.Clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport,
+	)
 
-	// Initialize the ClusterCollector (assuming it doesn't return an error)
-	clusterCollector := collectors.NewClusterCollector(k8sClientset, cfg, k8sConfig)
+	// Create collectors with the properly configured clientset
+	collectorsMap := make(map[string]collectors.Collector)
+	clusterCollector := collectors.NewClusterCollector(clientConfig.Clientset, cfg)
 	collectorsMap["cluster"] = clusterCollector
+	collectorsMap["node"] = collectors.NewNodeCollector(clientConfig.Clientset, cfg)
+	collectorsMap["pod"] = collectors.NewPodCollector(clientConfig.Clientset, cfg)
+	collectorsMap["pv"] = collectors.NewPersistentVolumeCollector(clientConfig.Clientset, cfg)
+	collectorsMap["namespace"] = collectors.NewNamespaceCollector(clientConfig.Clientset, cfg)
+	collectorsMap["workload"] = collectors.NewWorkloadCollector(clientConfig.Clientset, cfg)
+	collectorsMap["network"] = collectors.NewNetworkingCollector(clientConfig.Clientset, cfg)
+	collectorsMap["job"] = collectors.NewJobCollector(clientConfig.Clientset, cfg)
+	collectorsMap["cronjob"] = collectors.NewCronJobCollector(clientConfig.Clientset, cfg)
+	collectorsMap["hpa"] = collectors.NewHPACollector(clientConfig.Clientset, cfg)
+	collectorsMap["replicationController"] = collectors.NewReplicationControllerCollector(clientConfig.Clientset, cfg)
+	collectorsMap["storageclass"] = collectors.NewStorageClassCollector(clientConfig.Clientset, cfg)
+	collectorsMap["replicasets"] = collectors.NewReplicaSetCollector(clientConfig.Clientset, cfg)
 
-	// Continue initializing other collectors similarly
-	collectorsMap["pv"] = collectors.NewPersistentVolumeCollector(k8sClientset, cfg)
-	collectorsMap["namespace"] = collectors.NewNamespaceCollector(k8sClientset, cfg)
-	collectorsMap["workload"] = collectors.NewWorkloadCollector(k8sClientset, cfg)
-	collectorsMap["network"] = collectors.NewNetworkingCollector(k8sClientset, cfg)
-	collectorsMap["job"] = collectors.NewJobCollector(k8sClientset, cfg)
-	collectorsMap["cronjob"] = collectors.NewCronJobCollector(k8sClientset, cfg)
-	collectorsMap["HPA"] = collectors.NewHPACollector(k8sClientset, cfg)
-	collectorsMap["replicationController"] = collectors.NewReplicationControllerCollector(k8sClientset, cfg)
-
-	collectorsMap["replicasets"] = collectors.NewReplicaSetCollector(k8sClientset, cfg)
 	logger.Debugf("loaded %v collectors", len(collectorsMap))
 
 	// Initialize the uploader
@@ -95,18 +95,21 @@ func NewMetricsAgent(cfg *config.Config,
 	if err != nil {
 		return nil, fmt.Errorf("failed to create S3 uploader: %w", err)
 	}
+
 	ma := &MetricsAgent{
 		config:     cfg,
 		collectors: collectorsMap,
 		uploader:   uploader,
 		logger:     logger.WithField("component", "MetricsAgent"),
 		httpClient: &http.Client{
-			Timeout: 90 * time.Second, // Set a timeout for HTTP requests
+			Timeout: 90 * time.Second,
 		},
 	}
+
 	if cfg.ShouldAgentCheckIn {
-		logrus.Info("Checking in with the metrics server")
+		logger.Debug("Attempting to check in with the metrics server")
 		if err := ma.Checkin(context.Background()); err != nil {
+			logger.WithError(err).Debug("Checkin attempt failed with detailed error")
 			logrus.WithError(err).Error("Failed to check in with the metrics server")
 		}
 	}
@@ -174,7 +177,7 @@ func (ma *MetricsAgent) collectAndUploadMetrics(ctx context.Context) error {
 	startTime := time.Now()
 	if ma.config.ShouldAgentCheckIn {
 		go func() {
-			logrus.Info("Checking in with the metrics server")
+			logrus.Info("Starting Kubernetes Data Collection")
 			if err := ma.Checkin(ctx); err != nil {
 				logrus.WithError(err).Error("Failed to check in with the metrics server")
 			}
diff --git a/pkg/collectors/HPA.go b/pkg/collectors/HPA.go
deleted file mode 100644
index 65a0d0b..0000000
--- a/pkg/collectors/HPA.go
+++ /dev/null
@@ -1,91 +0,0 @@
-// Copyright 2024 Vega Cloud, Inc.
-//
-// Use of this software is governed by the Business Source License
-// included in the file licenses/BSL.txt.
-//
-// As of the Change Date specified in that file, in accordance with
-// the Business Source License, use of this software will be governed
-// by the Apache License, Version 2.0, included in the file
-// licenses/APL.txt.
-// File: pkg/collectors/hpa.go
-package collectors
-
-import (
-	"context"
-	"fmt"
-
-	"github.com/sirupsen/logrus"
-	autoscalingv1 "k8s.io/api/autoscaling/v1"
-	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
-	"k8s.io/client-go/kubernetes"
-
-	"github.com/vegacloud/kubernetes/metricsagent/pkg/config"
-	"github.com/vegacloud/kubernetes/metricsagent/pkg/models"
-)
-
-type HPACollector struct {
-	clientset *kubernetes.Clientset
-	config    *config.Config
-}
-
-func NewHPACollector(clientset *kubernetes.Clientset, cfg *config.Config) *HPACollector {
-	collector := &HPACollector{
-		clientset: clientset,
-		config:    cfg,
-	}
-	logrus.Debug("HPACollector created successfully")
-	return collector
-}
-
-func (hc *HPACollector) CollectMetrics(ctx context.Context) (interface{}, error) {
-	metrics, err := hc.CollectHPAMetrics(ctx)
-	if err != nil {
-		return nil, err
-	}
-	logrus.Debug("Successfully collected HPA metrics")
-	return metrics, nil
-}
-
-func (hc *HPACollector) CollectHPAMetrics(ctx context.Context) ([]models.HPAMetrics, error) {
-	hpas, err := hc.clientset.AutoscalingV1().HorizontalPodAutoscalers("").List(ctx, metav1.ListOptions{})
-	if err != nil {
-		return nil, fmt.Errorf("failed to list HPAs: %w", err)
-	}
-	logrus.Debugf("Successfully listed %d HPAs", len(hpas.Items))
-
-	metrics := make([]models.HPAMetrics, 0, len(hpas.Items))
-	for _, hpa := range hpas.Items {
-		metrics = append(metrics, hc.parseHPAMetrics(hpa))
-	}
-
-	logrus.Debugf("Collected metrics for %d HPAs", len(metrics))
-	return metrics, nil
-}
-
-func (hc *HPACollector) parseHPAMetrics(hpa autoscalingv1.HorizontalPodAutoscaler) models.HPAMetrics {
-	if hpa.Labels == nil {
-		hpa.Labels = make(map[string]string)
-	}
-	metrics := models.HPAMetrics{
-		Name:            hpa.Name,
-		Namespace:       hpa.Namespace,
-		CurrentReplicas: hpa.Status.CurrentReplicas,
-		DesiredReplicas: hpa.Status.DesiredReplicas,
-		Labels:          hpa.Labels,
-		CurrentCPUUtilizationPercentage: func() *int32 {
-			if hpa.Status.CurrentCPUUtilizationPercentage != nil {
-				return hpa.Status.CurrentCPUUtilizationPercentage
-			}
-			return nil
-		}(),
-		TargetCPUUtilizationPercentage: func() *int32 {
-			if hpa.Spec.TargetCPUUtilizationPercentage != nil {
-				return hpa.Spec.TargetCPUUtilizationPercentage
-			}
-			return nil
-		}(),
-	}
-
-	logrus.Debugf("Parsed HPA metrics for HPA %s/%s", hpa.Namespace, hpa.Name)
-	return metrics
-}
diff --git a/pkg/collectors/cluster.go b/pkg/collectors/cluster.go
index 018a0d6..9dfa3f2 100644
--- a/pkg/collectors/cluster.go
+++ b/pkg/collectors/cluster.go
@@ -7,71 +7,95 @@
 // the Business Source License, use of this software will be governed
 // by the Apache License, Version 2.0, included in the file
 // licenses/APL.txt.
+
+// Package collectors hosts the collection functions
 package collectors
 
 import (
 	"context"
+	// "crypto/tls"
 	"fmt"
+	// "net/http"
+	"os"
 
+	"github.com/sirupsen/logrus"
 	v1 "k8s.io/api/core/v1"
 	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
 	"k8s.io/client-go/kubernetes"
 
-	"github.com/sirupsen/logrus"
+	// "k8s.io/client-go/rest"
+	// "k8s.io/client-go/transport"
+
 	"github.com/vegacloud/kubernetes/metricsagent/pkg/config"
 	"github.com/vegacloud/kubernetes/metricsagent/pkg/models"
 	"github.com/vegacloud/kubernetes/metricsagent/pkg/utils"
 )
 
+// ClusterCollector collects cluster-wide metrics
 type ClusterCollector struct {
 	clientset *kubernetes.Clientset
 	config    *config.Config
-	k8sConfig utils.K8sClientConfig
 }
 
-func NewClusterCollector(
-	clientset *kubernetes.Clientset,
-	cfg *config.Config,
-	clientCFG utils.K8sClientConfig,
-) *ClusterCollector {
-	collector := &ClusterCollector{
+// NewClusterCollector creates a new cluster collector
+func NewClusterCollector(clientset *kubernetes.Clientset, cfg *config.Config) *ClusterCollector {
+	logrus.Debug("Starting ClusterCollector")
+	// Ensure we're using the in-cluster config token
+	// if token, err := os.ReadFile("/var/run/secrets/kubernetes.io/serviceaccount/token"); err == nil {
+	// 	clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport = &http.Transport{
+	// 		TLSClientConfig: &tls.Config{
+	// 			InsecureSkipVerify: cfg.VegaInsecure,
+	// 		},
+	// 	}
+	// 	clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport = transport.NewBearerAuthRoundTripper(
+	// 		string(token),
+	// 		clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport,
+	// 	)
+	// }
+	logrus.Debug("ClusterCollector initialized successfully")
+	return &ClusterCollector{
 		clientset: clientset,
 		config:    cfg,
-		k8sConfig: clientCFG,
 	}
-	logrus.Debug("ClusterCollector created successfully")
-	return collector
 }
 
+// CollectMetrics collects cluster-wide metrics
 func (cc *ClusterCollector) CollectMetrics(ctx context.Context) (interface{}, error) {
-	metrics, err := cc.CollectClusterMetrics(ctx)
-	if err != nil {
-		return nil, err
+	logger := logrus.WithField("collector", "ClusterCollector")
+
+	// Add debug logging for the client configuration
+	if token, err := os.ReadFile("/var/run/secrets/kubernetes.io/serviceaccount/token"); err == nil {
+		logger.Debugf("Service account token length: %d", len(string(token)))
 	}
-	logrus.Debug("Successfully collected cluster metrics")
-	return metrics, nil
-}
 
-func (cc *ClusterCollector) CollectClusterMetrics(ctx context.Context) (*models.ClusterMetrics, error) {
+	// Verify client identity before collecting metrics
+	if err := VerifyCollectorClient(ctx, cc.clientset, cc.config.VegaNamespace, "ClusterCollector"); err != nil {
+		return nil, err
+	}
+	version, err := cc.clientset.Discovery().ServerVersion()
+	if err != nil {
+		return nil, fmt.Errorf("failed to get server version: %w", err)
+	}
+	clusterVersion := version.String()
 	nodes, err := cc.clientset.CoreV1().Nodes().List(ctx, metav1.ListOptions{})
 	if err != nil {
 		return nil, fmt.Errorf("failed to list nodes: %w", err)
 	}
-	logrus.Debugf("Successfully listed %d nodes", len(nodes.Items))
+	logger.Debugf("Successfully listed %d nodes", len(nodes.Items))
 
 	pods, err := cc.clientset.CoreV1().Pods("").List(ctx, metav1.ListOptions{})
 	if err != nil {
 		return nil, fmt.Errorf("failed to list pods: %w", err)
 	}
-	logrus.Debugf("Successfully listed %d pods", len(pods.Items))
+	logger.Debugf("Successfully listed %d pods", len(pods.Items))
 
 	metrics := &models.ClusterMetrics{
-		KubernetesVersion: cc.k8sConfig.ClusterVersion,
+		KubernetesVersion: clusterVersion,
 		NodeCount:         len(nodes.Items),
 		PodCount:          len(pods.Items),
 		ContainerCount:    cc.countContainers(pods.Items),
-		NodeLabels:        make(map[string]map[string]string), // New field for node labels
-		PodLabels:         make(map[string]map[string]string), // New field for pod labels
+		NodeLabels:        make(map[string]map[string]string),
+		PodLabels:         make(map[string]map[string]string),
 	}
 
 	metrics.TotalCapacity = cc.calculateTotalCapacity(nodes.Items)
@@ -86,31 +110,35 @@ func (cc *ClusterCollector) CollectClusterMetrics(ctx context.Context) (*models.
 
 	// Collect labels for pods
 	for _, pod := range pods.Items {
-		podKey := fmt.Sprintf("%s/%s", pod.Namespace, pod.Name) // Unique key for pod (namespace/name)
+		podKey := fmt.Sprintf("%s/%s", pod.Namespace, pod.Name)
 		metrics.PodLabels[podKey] = pod.Labels
 	}
 
-	logrus.Debug("Successfully calculated cluster metrics")
 	return metrics, nil
 }
 
-func (cc *ClusterCollector) countContainers(pods []v1.Pod) int {
-	count := 0
-	for _, pod := range pods {
-		count += len(pod.Spec.Containers)
-	}
-	logrus.Debugf("Counted %d containers", count)
-	return count
-}
-
 func (cc *ClusterCollector) calculateTotalCapacity(nodes []v1.Node) models.ResourceMetrics {
 	total := models.ResourceMetrics{}
 	for _, node := range nodes {
 		total.CPU += node.Status.Capacity.Cpu().MilliValue()
 		total.Memory += node.Status.Capacity.Memory().Value()
-		total.Pods += node.Status.Capacity.Pods().Value()
+		total.Storage += node.Status.Allocatable.Storage().Value()
+		if ephemeral, ok := node.Status.Allocatable[v1.ResourceEphemeralStorage]; ok {
+			total.EphemeralStorage += ephemeral.Value()
+		}
+		total.Pods += node.Status.Allocatable.Pods().Value()
+
+		// Collect GPU metrics if available
+		if gpuQuantity, ok := node.Status.Capacity["nvidia.com/gpu"]; ok {
+			gpuMetric := models.GPUMetrics{
+				DeviceID:    fmt.Sprintf("node-%s-gpu", node.Name),
+				MemoryTotal: utils.SafeGPUMemory(gpuQuantity.Value()), // Convert to bytes
+			}
+			total.GPUDevices = append(total.GPUDevices, gpuMetric)
+		}
 	}
-	logrus.Debugf("Calculated total capacity: CPU=%d, Memory=%d, Pods=%d", total.CPU, total.Memory, total.Pods)
+	logrus.Debugf("Calculated total capacity: CPU=%d, Memory=%d, Storage=%d, EphemeralStorage=%d, Pods=%d, GPUs=%d",
+		total.CPU, total.Memory, total.Storage, total.EphemeralStorage, total.Pods, len(total.GPUDevices))
 	return total
 }
 
@@ -119,9 +147,23 @@ func (cc *ClusterCollector) calculateTotalAllocatable(nodes []v1.Node) models.Re
 	for _, node := range nodes {
 		total.CPU += node.Status.Allocatable.Cpu().MilliValue()
 		total.Memory += node.Status.Allocatable.Memory().Value()
+		total.Storage += node.Status.Allocatable.Storage().Value()
+		if ephemeral, ok := node.Status.Allocatable[v1.ResourceEphemeralStorage]; ok {
+			total.EphemeralStorage += ephemeral.Value()
+		}
 		total.Pods += node.Status.Allocatable.Pods().Value()
+
+		// Collect GPU metrics if available
+		if gpuQuantity, ok := node.Status.Allocatable["nvidia.com/gpu"]; ok {
+			gpuMetric := models.GPUMetrics{
+				DeviceID:    fmt.Sprintf("node-%s-gpu", node.Name),
+				MemoryTotal: utils.SafeGPUMemory(gpuQuantity.Value()), // Convert to bytes
+			}
+			total.GPUDevices = append(total.GPUDevices, gpuMetric)
+		}
 	}
-	logrus.Debugf("Calculated total allocatable: CPU=%d, Memory=%d, Pods=%d", total.CPU, total.Memory, total.Pods)
+	logrus.Debugf("Calculated total allocatable: CPU=%d, Memory=%d, Storage=%d, EphemeralStorage=%d, Pods=%d, GPUs=%d",
+		total.CPU, total.Memory, total.Storage, total.EphemeralStorage, total.Pods, len(total.GPUDevices))
 	return total
 }
 
@@ -131,10 +173,23 @@ func (cc *ClusterCollector) calculateTotalRequests(pods []v1.Pod) models.Resourc
 		for _, container := range pod.Spec.Containers {
 			total.CPU += container.Resources.Requests.Cpu().MilliValue()
 			total.Memory += container.Resources.Requests.Memory().Value()
+			if ephemeral, ok := container.Resources.Requests[v1.ResourceEphemeralStorage]; ok {
+				total.EphemeralStorage += ephemeral.Value()
+			}
+
+			// Sum GPU requests if available
+			if gpuQuantity, ok := container.Resources.Requests["nvidia.com/gpu"]; ok {
+				gpuMetric := models.GPUMetrics{
+					DeviceID:    fmt.Sprintf("pod-%s-%s-gpu", pod.Namespace, pod.Name),
+					MemoryTotal: utils.SafeGPUMemory(gpuQuantity.Value()), // Convert to bytes
+				}
+				total.GPUDevices = append(total.GPUDevices, gpuMetric)
+			}
 		}
 	}
 	total.Pods = int64(len(pods))
-	logrus.Debugf("Calculated total requests: CPU=%d, Memory=%d, Pods=%d", total.CPU, total.Memory, total.Pods)
+	logrus.Debugf("Calculated total requests: CPU=%d, Memory=%d, EphemeralStorage=%d, Pods=%d, GPUs=%d",
+		total.CPU, total.Memory, total.EphemeralStorage, total.Pods, len(total.GPUDevices))
 	return total
 }
 
@@ -144,9 +199,32 @@ func (cc *ClusterCollector) calculateTotalLimits(pods []v1.Pod) models.ResourceM
 		for _, container := range pod.Spec.Containers {
 			total.CPU += container.Resources.Limits.Cpu().MilliValue()
 			total.Memory += container.Resources.Limits.Memory().Value()
+			if ephemeral, ok := container.Resources.Limits[v1.ResourceEphemeralStorage]; ok {
+				total.EphemeralStorage += ephemeral.Value()
+			}
+
+			// Sum GPU limits if available
+			if gpuQuantity, ok := container.Resources.Limits["nvidia.com/gpu"]; ok {
+				gpuMetric := models.GPUMetrics{
+					DeviceID:    fmt.Sprintf("pod-%s-%s-gpu", pod.Namespace, pod.Name),
+					MemoryTotal: utils.SafeGPUMemory(gpuQuantity.Value()),
+				}
+				total.GPUDevices = append(total.GPUDevices, gpuMetric)
+			}
 		}
 	}
 	total.Pods = int64(len(pods))
-	logrus.Debugf("Calculated total limits: CPU=%d, Memory=%d, Pods=%d", total.CPU, total.Memory, total.Pods)
+	logrus.Debugf("Calculated total limits: CPU=%d, Memory=%d, EphemeralStorage=%d, Pods=%d, GPUs=%d",
+		total.CPU, total.Memory, total.EphemeralStorage, total.Pods, len(total.GPUDevices))
 	return total
 }
+
+func (cc *ClusterCollector) countContainers(pods []v1.Pod) int {
+	count := 0
+	for _, pod := range pods {
+		count += len(pod.Spec.Containers)
+		count += len(pod.Spec.InitContainers)
+		count += len(pod.Spec.EphemeralContainers)
+	}
+	return count
+}
diff --git a/pkg/collectors/collector.go b/pkg/collectors/collector.go
index 0953b1f..0bfe162 100644
--- a/pkg/collectors/collector.go
+++ b/pkg/collectors/collector.go
@@ -8,10 +8,20 @@
 // by the Apache License, Version 2.0, included in the file
 // licenses/APL.txt.
 // File: pkg/collectors/collector.go
+
+// Package collectors hosts the collection functions
 package collectors
 
 import (
+	"bytes"
 	"context"
+	"fmt"
+
+	dto "github.com/prometheus/client_model/go"
+	"github.com/prometheus/common/expfmt"
+	"github.com/sirupsen/logrus"
+	"github.com/vegacloud/kubernetes/metricsagent/pkg/utils"
+	"k8s.io/client-go/kubernetes"
 )
 
 // Collector defines the interface for all metric collectors
@@ -20,3 +30,60 @@ type Collector interface {
 	// which can be type-asserted to the specific metrics type for each collector
 	CollectMetrics(ctx context.Context) (interface{}, error)
 }
+
+// FetchMetricsViaKubelet fetches and parses metrics from kubelet
+func FetchMetricsViaKubelet(ctx context.Context, clientset *kubernetes.Clientset, nodeName, metricsPath string) (map[string]*dto.MetricFamily, error) {
+	kubeletClient := clientset.CoreV1().RESTClient().Get().
+		Resource("nodes").
+		Name(nodeName).
+		SubResource("proxy").
+		Suffix(metricsPath)
+
+	rawMetrics, err := kubeletClient.Do(ctx).Raw()
+	if err != nil {
+		return nil, fmt.Errorf("failed to fetch metrics: %w", err)
+	}
+
+	var parser expfmt.TextParser
+	return parser.TextToMetricFamilies(bytes.NewReader(rawMetrics))
+}
+
+// FetchRawStatsViaKubelet fetches raw stats from kubelet
+func FetchRawStatsViaKubelet(ctx context.Context, clientset *kubernetes.Clientset, nodeName, statsPath string) ([]byte, error) {
+	kubeletClient := clientset.CoreV1().RESTClient().Get().
+		Resource("nodes").
+		Name(nodeName).
+		SubResource("proxy").
+		Suffix(statsPath)
+
+	rawStats, err := kubeletClient.Do(ctx).Raw()
+	if err != nil {
+		return nil, fmt.Errorf("failed to fetch stats: %w", err)
+	}
+
+	return rawStats, nil
+}
+
+// FetchRawMetricsFromKubelet fetches raw metrics from kubelet
+func FetchRawMetricsFromKubelet(ctx context.Context, clientset *kubernetes.Clientset, nodeName, metricsPath string) ([]byte, error) {
+	kubeletClient := clientset.CoreV1().RESTClient().Get().
+		Resource("nodes").
+		Name(nodeName).
+		SubResource("proxy").
+		Suffix(metricsPath)
+
+	return kubeletClient.Do(ctx).Raw()
+}
+
+// VerifyCollectorClient verifies that the collector is using the correct client
+func VerifyCollectorClient(ctx context.Context, clientset *kubernetes.Clientset, namespace string, collectorName string) error {
+	logger := logrus.WithField("collector", collectorName)
+
+	if err := utils.VerifyClientIdentity(ctx, clientset, namespace); err != nil {
+		logger.WithError(err).Error("Collector using incorrect client identity")
+		return fmt.Errorf("%s collector: %w", collectorName, err)
+	}
+
+	logger.Debug("Verified collector client identity")
+	return nil
+}
diff --git a/pkg/collectors/cronJob.go b/pkg/collectors/cronJob.go
index 9ca0d3a..f4cd96e 100644
--- a/pkg/collectors/cronJob.go
+++ b/pkg/collectors/cronJob.go
@@ -7,6 +7,8 @@
 // the Business Source License, use of this software will be governed
 // by the Apache License, Version 2.0, included in the file
 // licenses/APL.txt.
+
+// Package collectors hosts the collection functions
 package collectors
 
 import (
@@ -14,39 +16,49 @@ import (
 	"fmt"
 	"time"
 
+	"github.com/robfig/cron/v3"
 	"github.com/sirupsen/logrus"
 	batchv1 "k8s.io/api/batch/v1"
+	v1 "k8s.io/api/core/v1"
 	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
 	"k8s.io/client-go/kubernetes"
 
 	"github.com/vegacloud/kubernetes/metricsagent/pkg/config"
 	"github.com/vegacloud/kubernetes/metricsagent/pkg/models"
+	"github.com/vegacloud/kubernetes/metricsagent/pkg/utils"
 )
 
+// CronJobCollector collects metrics from Kubernetes cron jobs.
 type CronJobCollector struct {
 	clientset *kubernetes.Clientset
 	config    *config.Config
+	parser    cron.Parser
 }
 
+// NewCronJobCollector creates a new CronJobCollector.
 func NewCronJobCollector(clientset *kubernetes.Clientset, cfg *config.Config) *CronJobCollector {
-	collector := &CronJobCollector{
+	// if token, err := os.ReadFile("/var/run/secrets/kubernetes.io/serviceaccount/token"); err == nil {
+	// 	logrus.Debug("CronJobCollector initialized successfully")
+	// 	clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport = &http.Transport{
+	// 		TLSClientConfig: &tls.Config{
+	// 			InsecureSkipVerify: cfg.VegaInsecure,
+	// 		},
+	// 	}
+	// 	clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport = transport.NewBearerAuthRoundTripper(
+	// 		string(token),
+	// 		clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport,
+	// 	)
+	// }
+	logrus.Debug("CronJobCollector initialized successfully")
+	return &CronJobCollector{
 		clientset: clientset,
 		config:    cfg,
+		parser:    cron.NewParser(cron.Second | cron.Minute | cron.Hour | cron.Dom | cron.Month | cron.Dow | cron.Descriptor),
 	}
-	logrus.Debug("CronJobCollector created successfully")
-	return collector
 }
 
+// CollectMetrics collects metrics from Kubernetes cron jobs.
 func (cjc *CronJobCollector) CollectMetrics(ctx context.Context) (interface{}, error) {
-	metrics, err := cjc.CollectCronJobMetrics(ctx)
-	if err != nil {
-		return nil, err
-	}
-	logrus.Debug("Successfully collected cron job metrics")
-	return metrics, nil
-}
-
-func (cjc *CronJobCollector) CollectCronJobMetrics(ctx context.Context) ([]models.CronJobMetrics, error) {
 	cronJobs, err := cjc.clientset.BatchV1().CronJobs("").List(ctx, metav1.ListOptions{})
 	if err != nil {
 		return nil, fmt.Errorf("failed to list cron jobs: %w", err)
@@ -54,36 +66,228 @@ func (cjc *CronJobCollector) CollectCronJobMetrics(ctx context.Context) ([]model
 	logrus.Debugf("Successfully listed %d cron jobs", len(cronJobs.Items))
 
 	metrics := make([]models.CronJobMetrics, 0, len(cronJobs.Items))
-
 	for _, cj := range cronJobs.Items {
-		metrics = append(metrics, cjc.parseCronJobMetrics(cj))
+		metric, err := cjc.parseCronJobMetrics(ctx, cj)
+		if err != nil {
+			logrus.Warnf("Failed to parse metrics for cronjob %s/%s: %v", cj.Namespace, cj.Name, err)
+			continue
+		}
+		metrics = append(metrics, metric)
 	}
 
 	logrus.Debugf("Collected metrics for %d cron jobs", len(metrics))
 	return metrics, nil
 }
 
-func (cjc *CronJobCollector) parseCronJobMetrics(cj batchv1.CronJob) models.CronJobMetrics {
-	// Create an empty cj.Labels if cj.Labels is nil
-	if cj.Labels == nil {
-		cj.Labels = make(map[string]string)
-	}
+// parseCronJobMetrics parses metrics from a Kubernetes cron job.
+func (cjc *CronJobCollector) parseCronJobMetrics(ctx context.Context, cj batchv1.CronJob) (models.CronJobMetrics, error) {
 	metrics := models.CronJobMetrics{
-		Name:       cj.Name,
-		Namespace:  cj.Namespace,
-		Schedule:   cj.Spec.Schedule,
-		Suspend:    cj.Spec.Suspend != nil && *cj.Spec.Suspend,
-		ActiveJobs: len(cj.Status.Active),
-		Labels:     cj.Labels,
-		LastScheduleTime: func() *time.Time {
-			if cj.Status.LastScheduleTime != nil {
-				t := cj.Status.LastScheduleTime.Time
-				return &t
-			}
-			return nil
-		}(),
+		Name:      cj.Name,
+		Namespace: cj.Namespace,
+		Schedule:  cj.Spec.Schedule,
+		Labels:    cj.Labels,
+		Status: models.CronJobStatus{
+			Active:             len(cj.Status.Active),
+			LastScheduleTime:   cjc.convertTime(cj.Status.LastScheduleTime),
+			LastSuccessfulTime: cjc.convertTime(cj.Status.LastSuccessfulTime),
+		},
+		Spec: models.CronJobSpec{
+			Suspend:                    cjc.getBoolValue(cj.Spec.Suspend),
+			Concurrency:                string(cj.Spec.ConcurrencyPolicy),
+			StartingDeadlineSeconds:    cjc.getInt64Value(cj.Spec.StartingDeadlineSeconds),
+			SuccessfulJobsHistoryLimit: cjc.getInt32Value(cj.Spec.SuccessfulJobsHistoryLimit),
+			FailedJobsHistoryLimit:     cjc.getInt32Value(cj.Spec.FailedJobsHistoryLimit),
+		},
+	}
+
+	// Calculate next scheduled time
+	nextSchedule, err := cjc.calculateNextSchedule(cj)
+	if err != nil {
+		logrus.Warnf("Failed to calculate next schedule for cronjob %s/%s: %v", cj.Namespace, cj.Name, err)
+	} else {
+		metrics.Status.NextScheduledTime = nextSchedule
+	}
+
+	// Get associated jobs metrics
+	jobs, err := cjc.getAssociatedJobsMetrics(ctx, cj)
+	if err != nil {
+		logrus.Warnf("Failed to get associated jobs for cronjob %s/%s: %v", cj.Namespace, cj.Name, err)
+	} else {
+		metrics.JobMetrics = jobs
+		metrics.Status.SuccessRate = cjc.calculateSuccessRate(jobs)
+	}
+
+	return metrics, nil
+}
+
+// getAssociatedJobsMetrics collects metrics from jobs associated with the CronJob
+func (cjc *CronJobCollector) getAssociatedJobsMetrics(ctx context.Context, cj batchv1.CronJob) ([]models.JobMetrics, error) {
+	selector := metav1.ListOptions{
+		LabelSelector: fmt.Sprintf("job-name=%s", cj.Name),
 	}
 
-	logrus.Debugf("Parsed cron job metrics for cron job %s/%s", cj.Namespace, cj.Name)
+	jobs, err := cjc.clientset.BatchV1().Jobs(cj.Namespace).List(ctx, selector)
+	if err != nil {
+		return nil, fmt.Errorf("failed to list jobs: %w", err)
+	}
+
+	metrics := make([]models.JobMetrics, 0, len(jobs.Items))
+	for _, job := range jobs.Items {
+		metrics = append(metrics, cjc.parseJobMetrics(job))
+	}
+
+	return metrics, nil
+}
+
+// parseJobMetrics parses metrics from a Kubernetes job
+func (cjc *CronJobCollector) parseJobMetrics(job batchv1.Job) models.JobMetrics {
+	metrics := models.JobMetrics{
+		Name:           job.Name,
+		Namespace:      job.Namespace,
+		StartTime:      cjc.convertTime(job.Status.StartTime),
+		CompletionTime: cjc.convertTime(job.Status.CompletionTime),
+		Active:         job.Status.Active,
+		Succeeded:      job.Status.Succeeded,
+		Failed:         job.Status.Failed,
+		Status:         cjc.getJobStatus(job.Status),
+		Duration:       cjc.calculateJobDuration(job.Status),
+		Labels:         job.Labels,
+		Resources:      cjc.getJobResourceMetrics(job),
+	}
 	return metrics
 }
+
+// Helper functions
+
+func (cjc *CronJobCollector) calculateNextSchedule(cj batchv1.CronJob) (*time.Time, error) {
+	schedule, err := cjc.parser.Parse(cj.Spec.Schedule)
+	if err != nil {
+		return nil, fmt.Errorf("failed to parse schedule: %w", err)
+	}
+
+	var baseTime time.Time
+	if cj.Status.LastScheduleTime != nil {
+		baseTime = cj.Status.LastScheduleTime.Time
+	} else {
+		baseTime = time.Now()
+	}
+
+	next := schedule.Next(baseTime)
+	return &next, nil
+}
+
+func (cjc *CronJobCollector) calculateSuccessRate(jobs []models.JobMetrics) float64 {
+	if len(jobs) == 0 {
+		return 0
+	}
+
+	completed := 0
+	succeeded := 0
+	for _, job := range jobs {
+		if job.CompletionTime != nil {
+			completed++
+			if job.Succeeded > 0 {
+				succeeded++
+			}
+		}
+	}
+
+	if completed == 0 {
+		return 0
+	}
+
+	return float64(succeeded) / float64(completed) * 100
+}
+func (cjc *CronJobCollector) getJobResourceMetrics(job batchv1.Job) models.ResourceMetrics {
+	resources := models.ResourceMetrics{}
+
+	if job.Spec.Template.Spec.Containers == nil {
+		return resources
+	}
+
+	for _, container := range job.Spec.Template.Spec.Containers {
+		if container.Resources.Requests != nil {
+			resources.CPU += container.Resources.Requests.Cpu().MilliValue()
+			resources.Memory += container.Resources.Requests.Memory().Value()
+			if storage, ok := container.Resources.Requests[v1.ResourceStorage]; ok {
+				resources.Storage += storage.Value()
+			}
+			if ephemeral, ok := container.Resources.Requests[v1.ResourceEphemeralStorage]; ok {
+				resources.EphemeralStorage += ephemeral.Value()
+			}
+		}
+
+		if gpuQuantity, ok := container.Resources.Requests["nvidia.com/gpu"]; ok {
+			gpuMetric := models.GPUMetrics{
+				DeviceID:    fmt.Sprintf("job-%s-%s-gpu", job.Namespace, job.Name),
+				MemoryTotal: utils.SafeGPUMemory(gpuQuantity.Value()),
+				MemoryUsed:  0,
+				OptMetrics: models.OptionalMetrics{
+					DutyCycle:   0,
+					Temperature: 0,
+					PowerUsage:  0,
+				},
+			}
+			resources.GPUDevices = append(resources.GPUDevices, gpuMetric)
+		}
+	}
+
+	return resources
+}
+
+// Utility functions for handling nullable values
+func (cjc *CronJobCollector) convertTime(t *metav1.Time) *time.Time {
+	if t == nil {
+		return nil
+	}
+	converted := t.Time
+	return &converted
+}
+
+func (cjc *CronJobCollector) getBoolValue(b *bool) bool {
+	if b == nil {
+		return false
+	}
+	return *b
+}
+
+func (cjc *CronJobCollector) getInt64Value(i *int64) int64 {
+	if i == nil {
+		return 0
+	}
+	return *i
+}
+
+func (cjc *CronJobCollector) getInt32Value(i *int32) int32 {
+	if i == nil {
+		return 0
+	}
+	return *i
+}
+
+func (cjc *CronJobCollector) getJobStatus(status batchv1.JobStatus) string {
+	switch {
+	case status.Succeeded > 0:
+		return "Succeeded"
+	case status.Failed > 0:
+		return "Failed"
+	case status.Active > 0:
+		return "Active"
+	default:
+		return "Pending"
+	}
+}
+
+func (cjc *CronJobCollector) calculateJobDuration(status batchv1.JobStatus) *time.Duration {
+	if status.StartTime == nil {
+		return nil
+	}
+
+	endTime := time.Now()
+	if status.CompletionTime != nil {
+		endTime = status.CompletionTime.Time
+	}
+
+	duration := endTime.Sub(status.StartTime.Time)
+	return &duration
+}
diff --git a/pkg/collectors/daemonsets.go b/pkg/collectors/daemonsets.go
index 4f91828..afc1c18 100644
--- a/pkg/collectors/daemonsets.go
+++ b/pkg/collectors/daemonsets.go
@@ -8,6 +8,8 @@
 // by the Apache License, Version 2.0, included in the file
 // licenses/APL.txt.
 // File: pkg/collectors/daemonsets.go
+
+// Package collectors hosts the collection functions
 package collectors
 
 import (
@@ -31,47 +33,55 @@ type DaemonSetCollector struct {
 
 // NewDaemonSetCollector creates a new DaemonSetCollector instance
 func NewDaemonSetCollector(clientset *kubernetes.Clientset, cfg *config.Config) *DaemonSetCollector {
-	collector := &DaemonSetCollector{
+	logrus.Debug("Starting DaemonSetCollector")
+	// if token, err := os.ReadFile("/var/run/secrets/kubernetes.io/serviceaccount/token"); err == nil {
+	// 	clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport = &http.Transport{
+	// 		TLSClientConfig: &tls.Config{
+	// 			InsecureSkipVerify: cfg.VegaInsecure,
+	// 		},
+	// 	}
+	// 	clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport = transport.NewBearerAuthRoundTripper(
+	// 		string(token),
+	// 		clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport,
+	// 	)
+	// }
+	logrus.Debug("DaemonSetCollector created successfully")
+	return &DaemonSetCollector{
 		clientset: clientset,
 		config:    cfg,
 	}
-	logrus.Debug("DaemonSetCollector created successfully")
-	return collector
 }
 
 // CollectMetrics collects metrics for daemon sets
-func (dsc *DaemonSetCollector) CollectMetrics(ctx context.Context) (interface{}, error) {
-	metrics, err := dsc.CollectDaemonSetMetrics(ctx)
-	if err != nil {
-		return nil, err
-	}
-	logrus.Debug("Successfully collected daemon set metrics")
-	return metrics, nil
-}
-
-// CollectDaemonSetMetrics collects metrics for all daemon sets in the cluster
-func (dsc *DaemonSetCollector) CollectDaemonSetMetrics(ctx context.Context) ([]models.DaemonSetMetrics, error) {
-	daemonSets, err := dsc.clientset.AppsV1().DaemonSets("").List(ctx, metav1.ListOptions{})
+func (dc *DaemonSetCollector) CollectMetrics(ctx context.Context) (interface{}, error) {
+	daemonsets, err := dc.clientset.AppsV1().DaemonSets("").List(ctx, metav1.ListOptions{})
 	if err != nil {
-		return nil, fmt.Errorf("failed to list daemon sets: %w", err)
+		return nil, fmt.Errorf("failed to list daemonsets: %w", err)
 	}
-	logrus.Debugf("Successfully listed %d daemon sets", len(daemonSets.Items))
 
-	metrics := make([]models.DaemonSetMetrics, 0, len(daemonSets.Items))
-
-	for _, ds := range daemonSets.Items {
-		metrics = append(metrics, dsc.parseDaemonSetMetrics(ds))
+	metrics := make([]models.DaemonSetMetrics, 0, len(daemonsets.Items))
+	for _, ds := range daemonsets.Items {
+		metric := dc.convertDaemonSetToMetrics(&ds)
+		metrics = append(metrics, metric)
 	}
 
-	logrus.Debugf("Collected metrics for %d daemon sets", len(metrics))
+	logrus.Debugf("Collected metrics for %d daemonsets", len(metrics))
 	return metrics, nil
 }
 
-// parseDaemonSetMetrics parses metrics for a single daemon set
-func (dsc *DaemonSetCollector) parseDaemonSetMetrics(ds appsv1.DaemonSet) models.DaemonSetMetrics {
-	if ds.Labels == nil {
-		ds.Labels = make(map[string]string)
+// convertDaemonSetToMetrics converts a Kubernetes DaemonSet to metrics
+func (dc *DaemonSetCollector) convertDaemonSetToMetrics(ds *appsv1.DaemonSet) models.DaemonSetMetrics {
+	conditions := make([]models.DaemonSetCondition, 0, len(ds.Status.Conditions))
+	for _, condition := range ds.Status.Conditions {
+		conditions = append(conditions, models.DaemonSetCondition{
+			Type:               string(condition.Type),
+			Status:             string(condition.Status),
+			LastTransitionTime: &condition.LastTransitionTime.Time,
+			Reason:             condition.Reason,
+			Message:            condition.Message,
+		})
 	}
+
 	return models.DaemonSetMetrics{
 		Name:                   ds.Name,
 		Namespace:              ds.Namespace,
@@ -80,6 +90,15 @@ func (dsc *DaemonSetCollector) parseDaemonSetMetrics(ds appsv1.DaemonSet) models
 		NumberReady:            ds.Status.NumberReady,
 		UpdatedNumberScheduled: ds.Status.UpdatedNumberScheduled,
 		NumberAvailable:        ds.Status.NumberAvailable,
+		NumberUnavailable:      ds.Status.NumberUnavailable,
+		NumberMisscheduled:     ds.Status.NumberMisscheduled,
 		Labels:                 ds.Labels,
+		Annotations:            ds.Annotations,
+		CreationTimestamp:      &ds.CreationTimestamp.Time,
+		CollisionCount:         ds.Status.CollisionCount,
+		Status: models.DaemonSetStatus{
+			ObservedGeneration: ds.Status.ObservedGeneration,
+		},
+		Conditions: conditions,
 	}
 }
diff --git a/pkg/collectors/hpa.go b/pkg/collectors/hpa.go
new file mode 100644
index 0000000..221222b
--- /dev/null
+++ b/pkg/collectors/hpa.go
@@ -0,0 +1,125 @@
+// Copyright 2024 Vega Cloud, Inc.
+//
+// Use of this software is governed by the Business Source License
+// included in the file licenses/BSL.txt.
+//
+// As of the Change Date specified in that file, in accordance with
+// the Business Source License, use of this software will be governed
+// by the Apache License, Version 2.0, included in the file
+// licenses/APL.txt.
+// File: pkg/collectors/hpa.go
+
+// Package collectors hosts the collection functions
+package collectors
+
+import (
+	"context"
+	"fmt"
+	"time"
+
+	"github.com/sirupsen/logrus"
+	autoscalingv1 "k8s.io/api/autoscaling/v1"
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	"k8s.io/client-go/kubernetes"
+
+	"github.com/vegacloud/kubernetes/metricsagent/pkg/config"
+	"github.com/vegacloud/kubernetes/metricsagent/pkg/models"
+)
+
+// HPACollector collects metrics from Kubernetes horizontal pod autoscalers.
+type HPACollector struct {
+	clientset *kubernetes.Clientset
+	config    *config.Config
+}
+
+// NewHPACollector creates a new HPACollector.
+func NewHPACollector(clientset *kubernetes.Clientset, cfg *config.Config) *HPACollector {
+	// logrus.Debug("Starting HPACollector")
+	// if token, err := os.ReadFile("/var/run/secrets/kubernetes.io/serviceaccount/token"); err == nil {
+	// 	clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport = &http.Transport{
+	// 		TLSClientConfig: &tls.Config{
+	// 			InsecureSkipVerify: cfg.VegaInsecure,
+	// 		},
+	// 	}
+	// 	clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport = transport.NewBearerAuthRoundTripper(
+	// 		string(token),
+	// 		clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport,
+	// 	)
+	// }
+	logrus.Debug("HPACollector created successfully")
+	return &HPACollector{
+		clientset: clientset,
+		config:    cfg,
+	}
+
+}
+
+// CollectMetrics collects metrics from Kubernetes horizontal pod autoscalers.
+func (hc *HPACollector) CollectMetrics(ctx context.Context) (interface{}, error) {
+	metrics, err := hc.CollectHPAMetrics(ctx)
+	if err != nil {
+		return nil, err
+	}
+	logrus.Debug("Successfully collected HPA metrics")
+	return metrics, nil
+}
+
+// CollectHPAMetrics collects metrics from Kubernetes horizontal pod autoscalers.
+func (hc *HPACollector) CollectHPAMetrics(ctx context.Context) ([]models.HPAMetrics, error) {
+	hpas, err := hc.clientset.AutoscalingV1().HorizontalPodAutoscalers("").List(ctx, metav1.ListOptions{})
+	if err != nil {
+		return nil, fmt.Errorf("failed to list HPAs: %w", err)
+	}
+	logrus.Debugf("Successfully listed %d HPAs", len(hpas.Items))
+
+	metrics := make([]models.HPAMetrics, 0, len(hpas.Items))
+	for _, hpa := range hpas.Items {
+		metrics = append(metrics, hc.parseHPAMetrics(hpa))
+	}
+
+	logrus.Debugf("Collected metrics for %d HPAs", len(metrics))
+	return metrics, nil
+}
+
+func (hc *HPACollector) parseHPAMetrics(hpa autoscalingv1.HorizontalPodAutoscaler) models.HPAMetrics {
+	metrics := models.HPAMetrics{
+		Name:      hpa.Name,
+		Namespace: hpa.Namespace,
+		ScaleTargetRef: models.ScaleTargetRef{
+			Kind:       hpa.Spec.ScaleTargetRef.Kind,
+			Name:       hpa.Spec.ScaleTargetRef.Name,
+			APIVersion: hpa.Spec.ScaleTargetRef.APIVersion,
+		},
+		MinReplicas:                     hpa.Spec.MinReplicas,
+		MaxReplicas:                     hpa.Spec.MaxReplicas,
+		CurrentReplicas:                 hpa.Status.CurrentReplicas,
+		DesiredReplicas:                 hpa.Status.DesiredReplicas,
+		CurrentCPUUtilizationPercentage: hpa.Status.CurrentCPUUtilizationPercentage,
+		TargetCPUUtilizationPercentage:  hpa.Spec.TargetCPUUtilizationPercentage,
+		LastScaleTime: func() *time.Time {
+			if hpa.Status.LastScaleTime != nil {
+				t := hpa.Status.LastScaleTime.Time
+				return &t
+			}
+			return nil
+		}(),
+		ObservedGeneration: hpa.Status.ObservedGeneration,
+		Labels:             hpa.Labels,
+		Annotations:        hpa.Annotations,
+		Status: models.HPAStatus{
+			CurrentReplicas:                 hpa.Status.CurrentReplicas,
+			DesiredReplicas:                 hpa.Status.DesiredReplicas,
+			CurrentCPUUtilizationPercentage: hpa.Status.CurrentCPUUtilizationPercentage,
+			LastScaleTime: func() *time.Time {
+				if hpa.Status.LastScaleTime != nil {
+					t := hpa.Status.LastScaleTime.Time
+					return &t
+				}
+				return nil
+			}(),
+		},
+	}
+
+	logrus.Debugf("Parsed HPA metrics for HPA %s/%s", hpa.Namespace, hpa.Name)
+	return metrics
+}
diff --git a/pkg/collectors/job.go b/pkg/collectors/job.go
index 0254ae6..a1a19bb 100644
--- a/pkg/collectors/job.go
+++ b/pkg/collectors/job.go
@@ -8,6 +8,8 @@
 // by the Apache License, Version 2.0, included in the file
 // licenses/APL.txt.
 // File: pkg/collectors/job.go
+
+// Package collectors hosts the collection functions
 package collectors
 
 import (
@@ -16,6 +18,7 @@ import (
 	"time"
 
 	"github.com/sirupsen/logrus"
+	batchv1 "k8s.io/api/batch/v1"
 	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
 	"k8s.io/client-go/kubernetes"
 
@@ -23,20 +26,35 @@ import (
 	"github.com/vegacloud/kubernetes/metricsagent/pkg/models"
 )
 
+// JobCollector collects metrics from Kubernetes jobs.
 type JobCollector struct {
 	clientset *kubernetes.Clientset
 	config    *config.Config
 }
 
+// NewJobCollector creates a new JobCollector.
 func NewJobCollector(clientset *kubernetes.Clientset, cfg *config.Config) *JobCollector {
-	collector := &JobCollector{
+	// logrus.Debug("Starting JobCollector")
+	// if token, err := os.ReadFile("/var/run/secrets/kubernetes.io/serviceaccount/token"); err == nil {
+	// 	clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport = &http.Transport{
+	// 		TLSClientConfig: &tls.Config{
+	// 			InsecureSkipVerify: cfg.VegaInsecure,
+	// 		},
+	// 	}
+	// 	clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport = transport.NewBearerAuthRoundTripper(
+	// 		string(token),
+	// 		clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport,
+	// 	)
+	// }
+	logrus.Debug("JobCollector created successfully")
+	return &JobCollector{
 		clientset: clientset,
 		config:    cfg,
 	}
-	logrus.Debug("JobCollector created successfully")
-	return collector
+
 }
 
+// CollectMetrics collects metrics from Kubernetes jobs.
 func (jc *JobCollector) CollectMetrics(ctx context.Context) (interface{}, error) {
 	metrics, err := jc.CollectJobMetrics(ctx)
 	if err != nil {
@@ -46,8 +64,8 @@ func (jc *JobCollector) CollectMetrics(ctx context.Context) (interface{}, error)
 	return metrics, nil
 }
 
+// CollectJobMetrics collects metrics from Kubernetes jobs.
 func (jc *JobCollector) CollectJobMetrics(ctx context.Context) ([]models.JobMetrics, error) {
-
 	jobs, err := jc.clientset.BatchV1().Jobs("").List(ctx, metav1.ListOptions{})
 	if err != nil {
 		return nil, fmt.Errorf("failed to list jobs: %w", err)
@@ -60,41 +78,110 @@ func (jc *JobCollector) CollectJobMetrics(ctx context.Context) ([]models.JobMetr
 		if job.Labels == nil {
 			job.Labels = make(map[string]string)
 		}
+
+		var duration *time.Duration
+		if job.Status.CompletionTime != nil && job.Status.StartTime != nil {
+			d := job.Status.CompletionTime.Sub(job.Status.StartTime.Time)
+			duration = &d
+		}
+
+		status := calculateJobStatus(&job)
+
 		metrics := models.JobMetrics{
-			Name:      job.Name,
-			Namespace: job.Namespace,
-			Labels:    job.Labels,
-			Completions: func() *int32 {
-				if job.Spec.Completions != nil {
-					return job.Spec.Completions
-				}
-				return nil
-			}(),
-			Parallelism: func() *int32 {
-				if job.Spec.Parallelism != nil {
-					return job.Spec.Parallelism
-				}
-				return nil
-			}(),
-			Active:    job.Status.Active,
-			Succeeded: job.Status.Succeeded,
-			Failed:    job.Status.Failed,
-			StartTime: func() time.Time {
-				if job.Status.StartTime != nil {
-					return job.Status.StartTime.Time
-				}
-				return time.Time{}
-			}(),
-			CompletionTime: func() time.Time {
-				if job.Status.CompletionTime != nil {
-					return job.Status.CompletionTime.Time
-				}
-				return time.Time{}
-			}(),
+			Name:            job.Name,
+			Namespace:       job.Namespace,
+			Labels:          job.Labels,
+			Active:          job.Status.Active,
+			Succeeded:       job.Status.Succeeded,
+			Failed:          job.Status.Failed,
+			Status:          status,
+			StartTime:       timePtr(job.Status.StartTime),
+			CompletionTime:  timePtr(job.Status.CompletionTime),
+			Duration:        duration,
+			Parallelism:     job.Spec.Parallelism,
+			Completions:     job.Spec.Completions,
+			BackoffLimit:    job.Spec.BackoffLimit,
+			Suspended:       job.Spec.Suspend != nil && *job.Spec.Suspend,
+			CreationTime:    &job.CreationTimestamp.Time,
+			Conditions:      convertJobConditions(job.Status.Conditions),
+			ResourceMetrics: jc.collectJobResourceMetrics(ctx, &job),
 		}
+
 		jobMetrics = append(jobMetrics, metrics)
 		logrus.Debugf("Collected metrics for job %s/%s", job.Namespace, job.Name)
 	}
 
 	return jobMetrics, nil
 }
+
+// Helper functions
+
+func timePtr(t *metav1.Time) *time.Time {
+	if t == nil {
+		return nil
+	}
+	tt := t.Time
+	return &tt
+}
+
+func calculateJobStatus(job *batchv1.Job) string {
+	if job.Status.Succeeded > 0 {
+		return "Completed"
+	}
+	if job.Status.Failed > 0 {
+		return "Failed"
+	}
+	if job.Status.Active > 0 {
+		return "Active"
+	}
+	if job.Spec.Suspend != nil && *job.Spec.Suspend {
+		return "Suspended"
+	}
+	return "Pending"
+}
+
+func convertJobConditions(conditions []batchv1.JobCondition) []models.JobCondition {
+	result := make([]models.JobCondition, 0, len(conditions))
+	for _, c := range conditions {
+		condition := models.JobCondition{
+			Type:               string(c.Type),
+			Status:             string(c.Status),
+			LastProbeTime:      timePtr(&c.LastProbeTime),
+			LastTransitionTime: timePtr(&c.LastTransitionTime),
+			Reason:             c.Reason,
+			Message:            c.Message,
+		}
+		result = append(result, condition)
+	}
+	return result
+}
+
+func (jc *JobCollector) collectJobResourceMetrics(ctx context.Context, job *batchv1.Job) models.ResourceMetrics {
+	selector := metav1.LabelSelector{MatchLabels: job.Spec.Selector.MatchLabels}
+	labelSelector, err := metav1.LabelSelectorAsSelector(&selector)
+	if err != nil {
+		logrus.Errorf("Failed to create selector for job %s/%s: %v", job.Namespace, job.Name, err)
+		return models.ResourceMetrics{}
+	}
+
+	pods, err := jc.clientset.CoreV1().Pods(job.Namespace).List(ctx, metav1.ListOptions{
+		LabelSelector: labelSelector.String(),
+	})
+	if err != nil {
+		logrus.Errorf("Failed to list pods for job %s/%s: %v", job.Namespace, job.Name, err)
+		return models.ResourceMetrics{}
+	}
+
+	metrics := models.ResourceMetrics{}
+	for _, pod := range pods.Items {
+		for _, container := range pod.Spec.Containers {
+			metrics.CPU += container.Resources.Requests.Cpu().MilliValue()
+			metrics.Memory += container.Resources.Requests.Memory().Value()
+			if container.Resources.Requests.Storage() != nil {
+				metrics.Storage += container.Resources.Requests.Storage().Value()
+			}
+		}
+	}
+
+	return metrics
+}
diff --git a/pkg/collectors/namespace.go b/pkg/collectors/namespace.go
index 6e09191..19fb2f3 100644
--- a/pkg/collectors/namespace.go
+++ b/pkg/collectors/namespace.go
@@ -8,11 +8,14 @@
 // by the Apache License, Version 2.0, included in the file
 // licenses/APL.txt.
 // File: pkg/collectors/namespace.go
+
+// Package collectors hosts the collection functions
 package collectors
 
 import (
 	"context"
 	"fmt"
+	"strings"
 
 	"github.com/sirupsen/logrus"
 	v1 "k8s.io/api/core/v1"
@@ -23,12 +26,26 @@ import (
 	"github.com/vegacloud/kubernetes/metricsagent/pkg/models"
 )
 
+// NamespaceCollector collects metrics from Kubernetes namespaces.
 type NamespaceCollector struct {
 	clientset *kubernetes.Clientset
 	config    *config.Config
 }
 
+// NewNamespaceCollector creates a new NamespaceCollector.
 func NewNamespaceCollector(clientset *kubernetes.Clientset, cfg *config.Config) *NamespaceCollector {
+	// logrus.Debug("Starting NamespaceCollector")
+	// if token, err := os.ReadFile("/var/run/secrets/kubernetes.io/serviceaccount/token"); err == nil {
+	// 	clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport = &http.Transport{
+	// 		TLSClientConfig: &tls.Config{
+	// 			InsecureSkipVerify: cfg.VegaInsecure,
+	// 		},
+	// 	}
+	// 	clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport = transport.NewBearerAuthRoundTripper(
+	// 		string(token),
+	// 		clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport,
+	// 	)
+	// }
 	logrus.Debug("Creating new NamespaceCollector")
 	return &NamespaceCollector{
 		clientset: clientset,
@@ -36,11 +53,13 @@ func NewNamespaceCollector(clientset *kubernetes.Clientset, cfg *config.Config)
 	}
 }
 
+// CollectMetrics collects metrics from Kubernetes namespaces.
 func (nc *NamespaceCollector) CollectMetrics(ctx context.Context) (interface{}, error) {
 	logrus.Debug("Collecting namespace metrics")
 	return nc.CollectNamespaceMetrics(ctx)
 }
 
+// CollectNamespaceMetrics collects metrics from Kubernetes namespaces.
 func (nc *NamespaceCollector) CollectNamespaceMetrics(ctx context.Context) ([]models.NamespaceMetrics, error) {
 	namespaces, err := nc.clientset.CoreV1().Namespaces().List(ctx, metav1.ListOptions{})
 	if err != nil {
@@ -63,6 +82,7 @@ func (nc *NamespaceCollector) CollectNamespaceMetrics(ctx context.Context) ([]mo
 	return namespaceMetrics, nil
 }
 
+// collectSingleNamespaceMetrics collects metrics from a single Kubernetes namespace.
 func (nc *NamespaceCollector) collectSingleNamespaceMetrics(
 	ctx context.Context,
 	ns v1.Namespace,
@@ -70,10 +90,33 @@ func (nc *NamespaceCollector) collectSingleNamespaceMetrics(
 	if ns.Labels == nil {
 		ns.Labels = make(map[string]string)
 	}
+
 	metrics := models.NamespaceMetrics{
-		Name:   ns.Name,
-		Status: string(ns.Status.Phase),
-		Labels: ns.Labels,
+		Name:              ns.Name,
+		Status:            string(ns.Status.Phase),
+		Phase:             string(ns.Status.Phase),
+		CreationTimestamp: ns.CreationTimestamp.Time,
+		DeletionTimestamp: nil,
+		Finalizers:        ns.Finalizers,
+		Labels:            ns.Labels,
+		Annotations:       ns.Annotations,
+	}
+
+	// Set deletion timestamp if exists
+	if ns.DeletionTimestamp != nil {
+		deletionTime := ns.DeletionTimestamp.Time
+		metrics.DeletionTimestamp = &deletionTime
+	}
+
+	// Convert conditions
+	for _, condition := range ns.Status.Conditions {
+		metrics.Conditions = append(metrics.Conditions, models.NamespaceCondition{
+			Type:               string(condition.Type),
+			Status:             string(condition.Status),
+			LastTransitionTime: &condition.LastTransitionTime.Time,
+			Reason:             condition.Reason,
+			Message:            condition.Message,
+		})
 	}
 
 	// Collect ResourceQuotas
@@ -107,11 +150,13 @@ func (nc *NamespaceCollector) collectSingleNamespaceMetrics(
 	return metrics, nil
 }
 
+// parseResourceQuota parses metrics from a Kubernetes resource quota.
 func (nc *NamespaceCollector) parseResourceQuota(quota v1.ResourceQuota) models.ResourceQuotaMetrics {
 	metrics := models.ResourceQuotaMetrics{
 		Name: quota.Name,
 	}
 
+	// Parse basic resources
 	for resourceName, hard := range quota.Status.Hard {
 		used := quota.Status.Used[resourceName]
 		metrics.Resources = append(metrics.Resources, models.ResourceMetric{
@@ -121,10 +166,49 @@ func (nc *NamespaceCollector) parseResourceQuota(quota v1.ResourceQuota) models.
 		})
 	}
 
-	logrus.Debugf("Parsed resource quota %s", quota.Name)
+	// Parse quota scopes
+	if len(quota.Spec.Scopes) > 0 {
+		for _, scope := range quota.Spec.Scopes {
+			scopeMetric := models.QuotaScopeMetrics{
+				ScopeName: string(scope),
+			}
+			if quota.Spec.ScopeSelector != nil && quota.Spec.ScopeSelector.MatchExpressions != nil {
+				for _, expr := range quota.Spec.ScopeSelector.MatchExpressions {
+					scopeMetric.MatchScopes = append(scopeMetric.MatchScopes, string(expr.Operator))
+				}
+			}
+			metrics.Scopes = append(metrics.Scopes, scopeMetric)
+		}
+	}
+
+	// Parse priority class quotas
+	for resourceName, hard := range quota.Status.Hard {
+		if isPriorityClassResource(string(resourceName)) {
+			priorityClass := extractPriorityClass(string(resourceName))
+			metrics.PriorityQuotas = append(metrics.PriorityQuotas, models.PriorityClassQuotaMetrics{
+				PriorityClass: priorityClass,
+				Hard:          map[string]string{string(resourceName): hard.ToUnstructured().(string)},
+				Used:          map[string]string{string(resourceName): quota.Status.Used[resourceName].ToUnstructured().(string)},
+			})
+		}
+	}
+
 	return metrics
 }
 
+// Helper functions
+func isPriorityClassResource(resource string) bool {
+	return strings.HasPrefix(resource, "count/pods.") && strings.Contains(resource, "priorityclass")
+}
+
+func extractPriorityClass(resource string) string {
+	parts := strings.Split(resource, ".")
+	if len(parts) > 1 {
+		return parts[len(parts)-1]
+	}
+	return ""
+}
+
 func (nc *NamespaceCollector) parseLimitRange(lr v1.LimitRange) models.LimitRangeMetrics {
 	metrics := models.LimitRangeMetrics{
 		Name: lr.Name,
diff --git a/pkg/collectors/networking.go b/pkg/collectors/networking.go
index d28695f..284c029 100644
--- a/pkg/collectors/networking.go
+++ b/pkg/collectors/networking.go
@@ -8,6 +8,8 @@
 // by the Apache License, Version 2.0, included in the file
 // licenses/APL.txt.
 // File: pkg/collectors/networking.go
+
+// Package collectors hosts the collection functions
 package collectors
 
 import (
@@ -24,20 +26,35 @@ import (
 	"github.com/vegacloud/kubernetes/metricsagent/pkg/models"
 )
 
+// NetworkingCollector collects metrics from Kubernetes networking resources.
 type NetworkingCollector struct {
 	clientset *kubernetes.Clientset
 	config    *config.Config
 }
 
+// NewNetworkingCollector creates a new NetworkingCollector.
 func NewNetworkingCollector(clientset *kubernetes.Clientset, cfg *config.Config) *NetworkingCollector {
-	collector := &NetworkingCollector{
+	// logrus.Debug("Starting NetworkingCollector")
+	// if token, err := os.ReadFile("/var/run/secrets/kubernetes.io/serviceaccount/token"); err == nil {
+	// 	clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport = &http.Transport{
+	// 		TLSClientConfig: &tls.Config{
+	// 			InsecureSkipVerify: cfg.VegaInsecure,
+	// 		},
+	// 	}
+	// 	clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport = transport.NewBearerAuthRoundTripper(
+	// 		string(token),
+	// 		clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport,
+	// 	)
+	// }
+	logrus.Debug("NetworkingCollector created successfully")
+	return &NetworkingCollector{
 		clientset: clientset,
 		config:    cfg,
 	}
-	logrus.Debug("NetworkingCollector created successfully")
-	return collector
+
 }
 
+// CollectMetrics collects metrics from Kubernetes networking resources.
 func (nc *NetworkingCollector) CollectMetrics(ctx context.Context) (interface{}, error) {
 	metrics, err := nc.CollectNetworkingMetrics(ctx)
 	if err != nil {
@@ -47,6 +64,7 @@ func (nc *NetworkingCollector) CollectMetrics(ctx context.Context) (interface{},
 	return metrics, nil
 }
 
+// CollectNetworkingMetrics collects metrics from Kubernetes networking resources.
 func (nc *NetworkingCollector) CollectNetworkingMetrics(ctx context.Context) (*models.NetworkingMetrics, error) {
 	metrics := &models.NetworkingMetrics{}
 
@@ -65,6 +83,13 @@ func (nc *NetworkingCollector) CollectNetworkingMetrics(ctx context.Context) (*m
 		logrus.Debug("Successfully collected ingress metrics")
 	}
 
+	metrics.NetworkPolicies, err = nc.collectNetworkPolicyMetrics(ctx)
+	if err != nil {
+		logrus.Warnf("Failed to collect network policy metrics: %v", err)
+	} else {
+		logrus.Debug("Successfully collected network policy metrics")
+	}
+
 	return metrics, nil
 }
 
@@ -85,31 +110,57 @@ func (nc *NetworkingCollector) collectServiceMetrics(ctx context.Context) ([]mod
 }
 
 func (nc *NetworkingCollector) parseServiceMetrics(svc corev1.Service) models.ServiceMetrics {
-	if svc.Labels == nil {
-		svc.Labels = make(map[string]string)
-	}
 	metrics := models.ServiceMetrics{
-		Name:       svc.Name,
-		Namespace:  svc.Namespace,
-		Type:       string(svc.Spec.Type),
-		ClusterIP:  svc.Spec.ClusterIP,
-		ExternalIP: svc.Spec.ExternalIPs,
-		Labels:     svc.Labels,
+		Name:                  svc.Name,
+		Namespace:             svc.Namespace,
+		Type:                  string(svc.Spec.Type),
+		ClusterIP:             svc.Spec.ClusterIP,
+		ExternalIP:            svc.Spec.ExternalIPs,
+		Labels:                svc.Labels,
+		Annotations:           svc.Annotations,
+		SessionAffinity:       string(svc.Spec.SessionAffinity),
+		ExternalTrafficPolicy: string(svc.Spec.ExternalTrafficPolicy),
+		HealthCheckNodePort:   svc.Spec.HealthCheckNodePort,
+		IPFamilies:            make([]string, 0),
+		IPFamilyPolicy: func() string {
+			if svc.Spec.IPFamilyPolicy != nil {
+				return string(*svc.Spec.IPFamilyPolicy)
+			}
+			return ""
+		}(),
+	}
+
+	for _, family := range svc.Spec.IPFamilies {
+		metrics.IPFamilies = append(metrics.IPFamilies, string(family))
 	}
 
 	if svc.Spec.LoadBalancerIP != "" {
 		metrics.LoadBalancerIP = svc.Spec.LoadBalancerIP
 	}
 
-	for _, ingress := range svc.Status.LoadBalancer.Ingress {
-		if ingress.IP != "" {
-			metrics.LoadBalancerIngress = append(metrics.LoadBalancerIngress, ingress.IP)
-		}
-		if ingress.Hostname != "" {
-			metrics.LoadBalancerIngress = append(metrics.LoadBalancerIngress, ingress.Hostname)
+	// Parse status
+	metrics.Status.LoadBalancer = models.LoadBalancerStatus{
+		Ingress: make([]models.LoadBalancerIngress, len(svc.Status.LoadBalancer.Ingress)),
+	}
+
+	for i, ing := range svc.Status.LoadBalancer.Ingress {
+		metrics.Status.LoadBalancer.Ingress[i] = models.LoadBalancerIngress{
+			IP:       ing.IP,
+			Hostname: ing.Hostname,
 		}
 	}
 
+	// Parse conditions
+	for _, cond := range svc.Status.Conditions {
+		metrics.Status.Conditions = append(metrics.Status.Conditions, models.ServiceCondition{
+			Type:               string(cond.Type),
+			Status:             string(cond.Status),
+			LastTransitionTime: &cond.LastTransitionTime.Time,
+			Reason:             cond.Reason,
+			Message:            cond.Message,
+		})
+	}
+
 	for _, port := range svc.Spec.Ports {
 		metrics.Ports = append(metrics.Ports, models.ServicePort{
 			Name:       port.Name,
@@ -143,9 +194,6 @@ func (nc *NetworkingCollector) collectIngressMetrics(ctx context.Context) ([]mod
 }
 
 func (nc *NetworkingCollector) parseIngressMetrics(ing networkingv1.Ingress) models.IngressMetrics {
-	if ing.Labels == nil {
-		ing.Labels = make(map[string]string)
-	}
 	metrics := models.IngressMetrics{
 		Name:      ing.Name,
 		Namespace: ing.Namespace,
@@ -155,7 +203,19 @@ func (nc *NetworkingCollector) parseIngressMetrics(ing networkingv1.Ingress) mod
 			}
 			return ""
 		}(),
-		Labels: ing.Labels,
+		Labels:            ing.Labels,
+		Annotations:       ing.Annotations,
+		CreationTimestamp: &ing.CreationTimestamp.Time,
+	}
+
+	// Parse status
+	for _, ing := range ing.Status.LoadBalancer.Ingress {
+		if ing.IP != "" {
+			metrics.LoadBalancerIngress = append(metrics.LoadBalancerIngress, ing.IP)
+		}
+		if ing.Hostname != "" {
+			metrics.LoadBalancerIngress = append(metrics.LoadBalancerIngress, ing.Hostname)
+		}
 	}
 
 	for _, rule := range ing.Spec.Rules {
@@ -188,15 +248,105 @@ func (nc *NetworkingCollector) parseIngressMetrics(ing networkingv1.Ingress) mod
 		})
 	}
 
-	for _, ing := range ing.Status.LoadBalancer.Ingress {
-		if ing.IP != "" {
-			metrics.LoadBalancerIngress = append(metrics.LoadBalancerIngress, ing.IP)
+	logrus.Debugf("Parsed ingress metrics for ingress %s/%s", ing.Namespace, ing.Name)
+	return metrics
+}
+
+func (nc *NetworkingCollector) collectNetworkPolicyMetrics(ctx context.Context) ([]models.NetworkPolicyMetrics, error) {
+	networkPolicies, err := nc.clientset.NetworkingV1().NetworkPolicies("").List(ctx, metav1.ListOptions{})
+	if err != nil {
+		return nil, fmt.Errorf("failed to list network policies: %w", err)
+	}
+	logrus.Debugf("Successfully listed %d network policies", len(networkPolicies.Items))
+
+	metrics := make([]models.NetworkPolicyMetrics, 0, len(networkPolicies.Items))
+
+	for _, policy := range networkPolicies.Items {
+		metrics = append(metrics, nc.parseNetworkPolicyMetrics(policy))
+	}
+
+	return metrics, nil
+}
+
+func (nc *NetworkingCollector) parseNetworkPolicyMetrics(policy networkingv1.NetworkPolicy) models.NetworkPolicyMetrics {
+	metrics := models.NetworkPolicyMetrics{
+		Name:        policy.Name,
+		Namespace:   policy.Namespace,
+		Labels:      policy.Labels,
+		Annotations: policy.Annotations,
+		PodSelector: policy.Spec.PodSelector.MatchLabels,
+	}
+
+	// Parse policy types
+	for _, pType := range policy.Spec.PolicyTypes {
+		metrics.PolicyTypes = append(metrics.PolicyTypes, string(pType))
+	}
+
+	// Parse ingress rules
+	for _, rule := range policy.Spec.Ingress {
+		ingressRule := models.NetworkPolicyIngressRule{}
+
+		// Parse ports
+		for _, port := range rule.Ports {
+			ingressRule.Ports = append(ingressRule.Ports, models.NetworkPolicyPort{
+				Protocol: string(*port.Protocol),
+				Port:     port.Port.IntVal,
+			})
 		}
-		if ing.Hostname != "" {
-			metrics.LoadBalancerIngress = append(metrics.LoadBalancerIngress, ing.Hostname)
+
+		// Parse from rules
+		for _, from := range rule.From {
+			peer := models.NetworkPolicyPeer{}
+			if from.PodSelector != nil {
+				peer.PodSelector = from.PodSelector.MatchLabels
+			}
+			if from.NamespaceSelector != nil {
+				peer.NamespaceSelector = from.NamespaceSelector.MatchLabels
+			}
+			if from.IPBlock != nil {
+				peer.IPBlock = &models.IPBlock{
+					CIDR:   from.IPBlock.CIDR,
+					Except: from.IPBlock.Except,
+				}
+			}
+			ingressRule.From = append(ingressRule.From, peer)
 		}
+
+		metrics.Ingress = append(metrics.Ingress, ingressRule)
+	}
+
+	// Parse egress rules
+	for _, rule := range policy.Spec.Egress {
+		egressRule := models.NetworkPolicyEgressRule{}
+
+		// Parse ports
+		for _, port := range rule.Ports {
+			egressRule.Ports = append(egressRule.Ports, models.NetworkPolicyPort{
+				Protocol: string(*port.Protocol),
+				Port:     port.Port.IntVal,
+			})
+		}
+
+		// Parse to rules
+		for _, to := range rule.To {
+			peer := models.NetworkPolicyPeer{}
+			if to.PodSelector != nil {
+				peer.PodSelector = to.PodSelector.MatchLabels
+			}
+			if to.NamespaceSelector != nil {
+				peer.NamespaceSelector = to.NamespaceSelector.MatchLabels
+			}
+			if to.IPBlock != nil {
+				peer.IPBlock = &models.IPBlock{
+					CIDR:   to.IPBlock.CIDR,
+					Except: to.IPBlock.Except,
+				}
+			}
+			egressRule.To = append(egressRule.To, peer)
+		}
+
+		metrics.Egress = append(metrics.Egress, egressRule)
 	}
 
-	logrus.Debugf("Parsed ingress metrics for ingress %s/%s", ing.Namespace, ing.Name)
 	return metrics
 }
diff --git a/pkg/collectors/node.go b/pkg/collectors/node.go
index 7f55868..45d8dcc 100644
--- a/pkg/collectors/node.go
+++ b/pkg/collectors/node.go
@@ -7,352 +7,1051 @@
 // the Business Source License, use of this software will be governed
 // by the Apache License, Version 2.0, included in the file
 // licenses/APL.txt.
+
 // Package collectors hosts the collection functions
 package collectors
 
 import (
 	"context"
-	"crypto/tls"
-	"errors"
+	"sync"
+
+	// "crypto/tls"
+	"encoding/json"
+	// "errors"
 	"fmt"
-	"net/http"
-	"os"
+	"math"
+
+	// "os"
+	"strconv"
 	"strings"
-	"sync"
-	"time"
+
+	// "sync"
+	// "time"
 
 	dto "github.com/prometheus/client_model/go"
-	"github.com/prometheus/common/expfmt"
 	"github.com/sirupsen/logrus"
-	"golang.org/x/sync/errgroup"
 	v1 "k8s.io/api/core/v1"
 	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
 	"k8s.io/client-go/kubernetes"
-	metricsv1beta1 "k8s.io/metrics/pkg/apis/metrics/v1beta1"
-	metricsclientset "k8s.io/metrics/pkg/client/clientset/versioned"
 
 	"github.com/vegacloud/kubernetes/metricsagent/pkg/config"
 	"github.com/vegacloud/kubernetes/metricsagent/pkg/models"
 )
 
+// NodeCollector collects metrics from Kubernetes nodes.
 type NodeCollector struct {
-	clientset        *kubernetes.Clientset
-	metricsClientset *metricsclientset.Clientset
-	config           *config.Config
-	bearerToken      string
-	httpClient       *http.Client
+	clientset *kubernetes.Clientset
+	config    *config.Config
 }
 
 // NewNodeCollector initializes a new NodeCollector.
 func NewNodeCollector(
 	clientset *kubernetes.Clientset,
-	metricsClientset *metricsclientset.Clientset,
 	cfg *config.Config,
-) (*NodeCollector, error) {
-	nc := &NodeCollector{
-		clientset:        clientset,
-		metricsClientset: metricsClientset,
-		config:           cfg,
+) *NodeCollector {
+	// Log that we are starting the NodeCollector
+	// logrus.Debug("Starting NodeCollector")
+
+	// // Check if we have a token
+	// if token, err := os.ReadFile("/var/run/secrets/kubernetes.io/serviceaccount/token"); err == nil {
+	// 	clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport = &http.Transport{
+	// 		TLSClientConfig: &tls.Config{
+	// 			InsecureSkipVerify: cfg.VegaInsecure,
+	// 		},
+	// 	}
+	// 	clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport = transport.NewBearerAuthRoundTripper(
+	// 		string(token),
+	// 		clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport,
+	// 	)
+	// }
+	logrus.Debug("NodeCollector initialized successfully")
+	return &NodeCollector{
+		clientset: clientset,
+		config:    cfg,
 	}
+}
 
-	// Get bearer token
-	token, err := nc.getBearerToken()
+// CollectMetrics collects metrics for all nodes
+func (nc *NodeCollector) CollectMetrics(ctx context.Context) (interface{}, error) {
+	nodes, err := nc.clientset.CoreV1().Nodes().List(ctx, metav1.ListOptions{})
 	if err != nil {
-		return nil, fmt.Errorf("failed to get bearer token: %w", err)
+		return nil, err
 	}
-	nc.bearerToken = token
-	logrus.Debug("Successfully retrieved bearer token")
 
-	// Create HTTP client
-	transport := &http.Transport{
-		TLSClientConfig: &tls.Config{
-			InsecureSkipVerify: cfg.VegaInsecure, //#nosec this is only off for local testing and will be true in prod.
-		},
+	var metrics []models.EnhancedNodeMetrics
+	for _, node := range nodes.Items {
+		nodeMetrics, err := nc.CollectNodeMetrics(ctx, &node)
+		if err != nil {
+			continue
+		}
+		metrics = append(metrics, *nodeMetrics)
 	}
+	return metrics, nil
+}
 
-	nc.httpClient = &http.Client{
-		Timeout:   10 * time.Second,
-		Transport: transport,
+// Update collectCPUMetrics to use the new method
+func (nc *NodeCollector) collectCPUMetrics(ctx context.Context, node *v1.Node) (models.CPUMetrics, error) {
+	metricFamilies, err := FetchMetricsViaKubelet(ctx, nc.clientset, node.Name, "metrics/cadvisor")
+	if err != nil {
+		return models.CPUMetrics{}, err
 	}
-	logrus.Debug("HTTP client created successfully")
 
-	return nc, nil
+	cpuMetrics := models.CPUMetrics{
+		PerCoreUsage: make(map[string]uint64),
+		Throttling:   models.ThrottlingMetrics{},
+	}
+
+	// Total CPU usage (equivalent to metrics-server CPU usage)
+	if cpuUsage, ok := metricFamilies["container_cpu_usage_seconds_total"]; ok {
+		for _, metric := range cpuUsage.Metric {
+			if metric.Counter != nil {
+				value := metric.Counter.GetValue() * 1e9
+				if value >= 0 {
+					cpuMetrics.UsageTotal = uint64(value)
+				}
+			}
+		}
+	}
+
+	// Additional detailed CPU metrics
+	if cpuUser, ok := metricFamilies["container_cpu_user_seconds_total"]; ok {
+		for _, metric := range cpuUser.Metric {
+			if metric.Counter != nil {
+				value := metric.Counter.GetValue() * 1e9
+				if value >= 0 {
+					cpuMetrics.UserTime = uint64(value)
+				}
+			}
+		}
+	}
+
+	if cpuSystem, ok := metricFamilies["container_cpu_system_seconds_total"]; ok {
+		for _, metric := range cpuSystem.Metric {
+			if metric.Counter != nil {
+				value := metric.Counter.GetValue() * 1e9
+				if value >= 0 {
+					cpuMetrics.SystemTime = uint64(value)
+				}
+			}
+		}
+	}
+
+	return cpuMetrics, nil
 }
 
-// CollectMetrics collects enhanced node metrics.
-func (nc *NodeCollector) CollectMetrics(ctx context.Context) (interface{}, error) {
-	return nc.collectEnhancedNodeMetrics(ctx)
+// collectRuntimeMetrics collects runtime metrics
+func (nc *NodeCollector) collectRuntimeMetrics(ctx context.Context, node *v1.Node) (models.RuntimeMetrics, error) {
+	metricFamilies, err := FetchMetricsViaKubelet(ctx, nc.clientset, node.Name, "metrics/probes")
+	if err != nil {
+		return models.RuntimeMetrics{}, err
+	}
+
+	runtimeMetrics := models.RuntimeMetrics{
+		Operations: make(map[string]models.RuntimeOperation),
+	}
+
+	// Parse runtime operations
+	if operations, ok := metricFamilies["runtime_operations_total"]; ok {
+		for _, metric := range operations.Metric {
+			if metric.Counter != nil {
+				opType := getLabel(metric, "operation_type")
+				if opType != "" {
+					runtimeMetrics.Operations[opType] = models.RuntimeOperation{
+						Count: uint64(metric.Counter.GetValue()),
+						Type:  opType,
+					}
+				}
+			}
+		}
+	}
+
+	// Parse runtime errors
+	if errors, ok := metricFamilies["runtime_operations_errors_total"]; ok {
+		for _, metric := range errors.Metric {
+			if metric.Counter != nil {
+				opType := getLabel(metric, "operation_type")
+				if op, exists := runtimeMetrics.Operations[opType]; exists {
+					op.Errors = uint64(metric.Counter.GetValue())
+					runtimeMetrics.Operations[opType] = op
+				}
+			}
+		}
+	}
+
+	return runtimeMetrics, nil
 }
 
-// collectEnhancedNodeMetrics collects metrics for all nodes.
-func (nc *NodeCollector) collectEnhancedNodeMetrics(ctx context.Context) ([]models.EnhancedNodeMetrics, error) {
-	// Fetch node details
-	nodes, err := nc.clientset.CoreV1().Nodes().List(ctx, metav1.ListOptions{})
+// Update collectMemoryMetrics
+func (nc *NodeCollector) collectMemoryMetrics(ctx context.Context, node *v1.Node) (models.MemoryMetrics, error) {
+	metricFamilies, err := FetchMetricsViaKubelet(ctx, nc.clientset, node.Name, "metrics/cadvisor")
 	if err != nil {
-		return nil, fmt.Errorf("failed to list nodes: %w", err)
+		return models.MemoryMetrics{}, err
+	}
+
+	memoryMetrics := models.MemoryMetrics{}
+
+	// Total memory usage (equivalent to metrics-server memory usage)
+	if memUsage, ok := metricFamilies["container_memory_usage_bytes"]; ok {
+		for _, metric := range memUsage.Metric {
+			if metric.Gauge != nil {
+				memoryMetrics.Used = uint64(metric.Gauge.GetValue())
+			}
+		}
+	}
+
+	// Additional detailed memory metrics
+	if memWorkingSet, ok := metricFamilies["container_memory_working_set_bytes"]; ok {
+		for _, metric := range memWorkingSet.Metric {
+			if metric.Gauge != nil {
+				memoryMetrics.WorkingSet = uint64(metric.Gauge.GetValue())
+			}
+		}
 	}
-	logrus.Debugf("Successfully listed %d nodes", len(nodes.Items))
 
-	// Fetch node metrics from the Metrics Server
-	nodeMetricsList, err := nc.metricsClientset.MetricsV1beta1().NodeMetricses().List(ctx, metav1.ListOptions{})
+	if memRSS, ok := metricFamilies["container_memory_rss"]; ok {
+		for _, metric := range memRSS.Metric {
+			if metric.Gauge != nil {
+				memoryMetrics.RSS = uint64(metric.Gauge.GetValue())
+			}
+		}
+	}
+
+	return memoryMetrics, nil
+}
+
+// // getBearerToken retrieves the token from a file or environment variable.
+// func (nc *NodeCollector) getBearerToken() (string, error) {
+// 	// First, try to read from file
+// 	if nc.config.VegaBearerTokenPath != "" {
+// 		tokenBytes, err := os.ReadFile(nc.config.VegaBearerTokenPath)
+// 		if err == nil {
+// 			token := strings.TrimSpace(string(tokenBytes))
+// 			logrus.Debug("Successfully read Service Account bearer token from file")
+// 			return token, nil
+// 		}
+// 		logrus.Printf("Failed to read bearer token from file, defaulting to BEARER_TOKEN environment variable: %v", err)
+// 	}
+
+// 	// If file read failed or no file path was provided, try environment variable
+// 	token := strings.TrimSpace(os.Getenv("BEARER_TOKEN"))
+// 	if token != "" {
+// 		logrus.Debug("Successfully read bearer token from environment variable")
+// 		return token, nil
+// 	}
+
+// 	// If both file and environment variable failed, return an error
+// 	return "", errors.New("bearer token not found in file or environment")
+// }
+
+func (nc *NodeCollector) collectNetworkMetrics(ctx context.Context, node *v1.Node) (models.NetworkMetrics, error) {
+	metricFamilies, err := FetchMetricsViaKubelet(ctx, nc.clientset, node.Name, "metrics/cadvisor")
 	if err != nil {
-		return nil, fmt.Errorf("failed to fetch node metrics from metrics server: %w", err)
+		return models.NetworkMetrics{}, fmt.Errorf("failed to fetch network metrics: %w", err)
 	}
-	logrus.Debugf("Successfully fetched node metrics from metrics server")
 
-	metricsMap := nc.mapNodeMetrics(nodeMetricsList)
+	networkMetrics := models.NetworkMetrics{
+		Interfaces: make([]models.InterfaceStats, 0),
+		Summary:    models.NetworkSummary{},
+	}
 
-	var (
-		enhancedNodeMetrics []models.EnhancedNodeMetrics
-		mu                  sync.Mutex
-	)
+	interfaceMap := make(map[string]*models.InterfaceStats)
 
-	g, ctx := errgroup.WithContext(ctx)
-	semaphore := make(chan struct{}, nc.config.VegaMaxConcurrency) // Configurable concurrency
+	// Process node network metrics for detailed interface stats
+	for name, family := range metricFamilies {
+		if !strings.HasPrefix(name, "node_network_") {
+			continue
+		}
 
-	for _, node := range nodes.Items {
-		node := node // capture variable
-		g.Go(func() error {
-			select {
-			case semaphore <- struct{}{}:
-				defer func() { <-semaphore }()
-			case <-ctx.Done():
-				return ctx.Err()
+		for _, metric := range family.Metric {
+			interfaceName := getLabel(metric, "device")
+			if interfaceName == "" {
+				continue
 			}
 
-			nodeMetric, found := metricsMap[node.Name]
-			if !found {
-				nodeMetric = metricsv1beta1.NodeMetrics{}
+			if _, exists := interfaceMap[interfaceName]; !exists {
+				interfaceMap[interfaceName] = &models.InterfaceStats{
+					InterfaceName: interfaceName,
+				}
 			}
 
-			metrics, err := nc.collectSingleNodeMetrics(ctx, node, nodeMetric)
-			if err != nil {
-				logrus.Warnf("Failed to collect metrics for node %s: %v", node.Name, err)
-				return nil
+			if metric.Counter != nil {
+				value := uint64(metric.Counter.GetValue())
+				switch name {
+				case "node_network_receive_bytes_total":
+					interfaceMap[interfaceName].RxBytes = value
+					networkMetrics.Summary.RxBytesTotal += value
+				case "node_network_transmit_bytes_total":
+					interfaceMap[interfaceName].TxBytes = value
+					networkMetrics.Summary.TxBytesTotal += value
+				case "node_network_receive_packets_total":
+					interfaceMap[interfaceName].RxPackets = value
+				case "node_network_transmit_packets_total":
+					interfaceMap[interfaceName].TxPackets = value
+				case "node_network_receive_errs_total":
+					interfaceMap[interfaceName].RxErrors = value
+				case "node_network_transmit_errs_total":
+					interfaceMap[interfaceName].TxErrors = value
+				}
 			}
+		}
+	}
 
-			mu.Lock()
-			enhancedNodeMetrics = append(enhancedNodeMetrics, metrics)
-			mu.Unlock()
-			logrus.Debugf("Successfully collected metrics for node %s", node.Name)
-			return nil
-		})
+	// Convert interface map to slice
+	for _, v := range interfaceMap {
+		networkMetrics.Interfaces = append(networkMetrics.Interfaces, *v)
 	}
 
-	if err := g.Wait(); err != nil {
-		return nil, err
+	// Also collect container network metrics if they provide additional information
+	if rxBytes, ok := metricFamilies["container_network_receive_bytes_total"]; ok {
+		for _, metric := range rxBytes.Metric {
+			if metric.Counter != nil {
+				networkMetrics.Summary.ContainerRxBytesTotal += uint64(metric.Counter.GetValue())
+			}
+		}
 	}
 
-	logrus.Debug("Successfully collected enhanced node metrics for all nodes")
-	return enhancedNodeMetrics, nil
+	if txBytes, ok := metricFamilies["container_network_transmit_bytes_total"]; ok {
+		for _, metric := range txBytes.Metric {
+			if metric.Counter != nil {
+				networkMetrics.Summary.ContainerTxBytesTotal += uint64(metric.Counter.GetValue())
+			}
+		}
+	}
+
+	return networkMetrics, nil
 }
 
-// mapNodeMetrics creates a map of node metrics for easier lookup by node name.
-func (nc *NodeCollector) mapNodeMetrics(
-	nodeMetricsList *metricsv1beta1.NodeMetricsList,
-) map[string]metricsv1beta1.NodeMetrics {
-	metricsMap := make(map[string]metricsv1beta1.NodeMetrics)
-	for _, nodeMetric := range nodeMetricsList.Items {
-		metricsMap[nodeMetric.Name] = nodeMetric
+func (nc *NodeCollector) collectDiskMetrics(ctx context.Context, node *v1.Node) (models.DiskMetrics, error) {
+	metricFamilies, err := FetchMetricsViaKubelet(ctx, nc.clientset, node.Name, "metrics/cadvisor")
+	if err != nil {
+		return models.DiskMetrics{}, fmt.Errorf("failed to fetch disk metrics: %w", err)
 	}
-	logrus.Debug("Successfully mapped node metrics")
-	return metricsMap
-}
 
-// collectSingleNodeMetrics collects metrics for a single node.
-func (nc *NodeCollector) collectSingleNodeMetrics(
-	ctx context.Context,
-	node v1.Node,
-	nodeMetrics metricsv1beta1.NodeMetrics,
-) (models.EnhancedNodeMetrics, error) {
-	if node.Labels == nil {
-		node.Labels = make(map[string]string)
-	}
-	metrics := models.EnhancedNodeMetrics{
-		Name:   node.Name,
-		Labels: node.Labels,
-		Capacity: models.ResourceMetrics{
-			CPU:     node.Status.Capacity.Cpu().MilliValue(),
-			Memory:  node.Status.Capacity.Memory().Value(),
-			Pods:    node.Status.Capacity.Pods().Value(),
-			Storage: node.Status.Capacity.StorageEphemeral().Value(),
-		},
-		Allocatable: models.ResourceMetrics{
-			CPU:     node.Status.Allocatable.Cpu().MilliValue(),
-			Memory:  node.Status.Allocatable.Memory().Value(),
-			Pods:    node.Status.Allocatable.Pods().Value(),
-			Storage: node.Status.Allocatable.StorageEphemeral().Value(),
-		},
+	diskMetrics := models.DiskMetrics{
+		Devices: make([]models.NodeDiskStats, 0),
 	}
 
-	// Add node conditions
-	for _, condition := range node.Status.Conditions {
-		switch condition.Type {
-		case v1.NodeReady:
-			metrics.Conditions.Ready = condition.Status == v1.ConditionTrue
-		case v1.NodeMemoryPressure:
-			metrics.Conditions.MemoryPressure = condition.Status == v1.ConditionTrue
-		case v1.NodeDiskPressure:
-			metrics.Conditions.DiskPressure = condition.Status == v1.ConditionTrue
-		case v1.NodePIDPressure:
-			metrics.Conditions.PIDPressure = condition.Status == v1.ConditionTrue
+	deviceMap := make(map[string]*models.NodeDiskStats)
+
+	// Process per-device metrics
+	for name, family := range metricFamilies {
+		if !strings.HasPrefix(name, "node_disk_") {
+			continue
+		}
+
+		for _, metric := range family.Metric {
+			device := getLabel(metric, "device")
+			if device == "" {
+				continue
+			}
+
+			if _, exists := deviceMap[device]; !exists {
+				deviceMap[device] = &models.NodeDiskStats{
+					Device: device,
+				}
+			}
+
+			if metric.Counter != nil {
+				value := uint64(metric.Counter.GetValue())
+				switch name {
+				case "node_disk_read_bytes_total":
+					deviceMap[device].ReadBytes = value
+					diskMetrics.ReadBytes += safeInt64(value)
+				case "node_disk_written_bytes_total":
+					deviceMap[device].WriteBytes = value
+					diskMetrics.WriteBytes += safeInt64(value)
+				case "node_disk_reads_completed_total":
+					deviceMap[device].ReadOps = value
+				case "node_disk_writes_completed_total":
+					deviceMap[device].WriteOps = value
+				}
+			}
 		}
 	}
 
-	// Fetch resource usage from the Kubernetes Metrics API
-	if nodeMetrics.Name != "" {
-		metrics.Usage.CPU = nodeMetrics.Usage.Cpu().MilliValue()
-		metrics.Usage.Memory = nodeMetrics.Usage.Memory().Value()
+	// Convert device map to slice
+	for _, device := range deviceMap {
+		diskMetrics.Devices = append(diskMetrics.Devices, *device)
 	}
 
-	// Fetch additional usage metrics from cAdvisor
-	cAdvisorUsage, err := nc.collectNodeUsageMetrics(ctx, node)
+	return diskMetrics, nil
+}
+
+func (nc *NodeCollector) collectProcessMetrics(ctx context.Context, node *v1.Node) (models.ProcessMetrics, error) {
+	metricFamilies, err := FetchMetricsViaKubelet(ctx, nc.clientset, node.Name, "metrics")
 	if err != nil {
-		return metrics, fmt.Errorf("failed to collect cAdvisor metrics for node %s: %w", node.Name, err)
+		return models.ProcessMetrics{}, fmt.Errorf("failed to fetch process metrics: %w", err)
 	}
 
-	// Combine the metrics from cAdvisor with those from Metrics API
-	metrics.Usage.Storage = cAdvisorUsage.Storage
+	var processMetrics models.ProcessMetrics
 
-	logrus.Debugf("Successfully collected metrics for node %s", node.Name)
-	return metrics, nil
+	if procCount, ok := metricFamilies["process_cpu_seconds_total"]; ok {
+		processMetrics.ProcessCount = len(procCount.Metric)
+	}
+
+	return processMetrics, nil
 }
 
-// getBearerToken retrieves the token from a file or environment variable.
-func (nc *NodeCollector) getBearerToken() (string, error) {
-	// First, try to read from file
-	if nc.config.VegaBearerTokenPath != "" {
-		tokenBytes, err := os.ReadFile(nc.config.VegaBearerTokenPath)
-		if err == nil {
-			token := strings.TrimSpace(string(tokenBytes))
-			logrus.Debug("Successfully read Service Account bearer token from file")
-			return token, nil
-		}
-		logrus.Printf("Failed to read bearer token from file, defaulting to BEARER_TOKEN environment variable: %v", err)
+// parseResourceList converts Kubernetes resource list to ResourceMetrics
+func (nc *NodeCollector) parseResourceList(rl v1.ResourceList) models.ResourceMetrics {
+	metrics := models.ResourceMetrics{}
+
+	// CPU is reported in cores, convert to millicores
+	if cpu, ok := rl[v1.ResourceCPU]; ok {
+		metrics.CPU = cpu.MilliValue()
+	}
+
+	// Memory is reported in bytes
+	if memory, ok := rl[v1.ResourceMemory]; ok {
+		metrics.Memory = memory.Value()
 	}
 
-	// If file read failed or no file path was provided, try environment variable
-	token := strings.TrimSpace(os.Getenv("BEARER_TOKEN"))
-	if token != "" {
-		logrus.Debug("Successfully read bearer token from environment variable")
-		return token, nil
+	// Persistent Storage
+	if storage, ok := rl[v1.ResourceStorage]; ok {
+		metrics.Storage = storage.Value()
 	}
 
-	// If both file and environment variable failed, return an error
-	return "", errors.New("bearer token not found in file or environment")
+	// Ephemeral storage should be tracked separately from persistent storage
+	if ephemeralStorage, ok := rl[v1.ResourceEphemeralStorage]; ok {
+		metrics.EphemeralStorage = ephemeralStorage.Value()
+	}
+
+	// Add pods resource if available
+	if pods, ok := rl[v1.ResourcePods]; ok {
+		metrics.Pods = pods.Value()
+	}
+
+	return metrics
 }
 
-// collectNodeUsageMetrics collects cAdvisor metrics for a node.
-func (nc *NodeCollector) collectNodeUsageMetrics(ctx context.Context, node v1.Node) (models.ResourceMetrics, error) {
-	var nodeAddress string
-	for _, address := range node.Status.Addresses {
-		if address.Type == v1.NodeInternalIP {
-			nodeAddress = address.Address
-			break
-		}
+func (nc *NodeCollector) collectFilesystemMetrics(ctx context.Context, node *v1.Node) (models.FSMetrics, error) {
+	rawStats, err := FetchRawStatsViaKubelet(ctx, nc.clientset, node.Name, "stats/summary")
+	if err != nil {
+		return models.FSMetrics{}, fmt.Errorf("failed to fetch filesystem stats: %w", err)
 	}
-	if nodeAddress == "" {
-		return models.ResourceMetrics{}, fmt.Errorf("no valid IP found for node %s", node.Name)
+
+	var stats struct {
+		Node struct {
+			Fs struct {
+				CapacityBytes  uint64 `json:"capacityBytes"`
+				UsedBytes      uint64 `json:"usedBytes"`
+				AvailableBytes uint64 `json:"availableBytes"`
+				InodesFree     uint64 `json:"inodesFree"`
+				InodesUsed     uint64 `json:"inodesUsed"`
+			} `json:"fs"`
+		} `json:"node"`
 	}
 
-	metricsURL := fmt.Sprintf("https://%s:%d/metrics/cadvisor", nodeAddress, 10250)
+	if err := json.Unmarshal(rawStats, &stats); err != nil {
+		return models.FSMetrics{}, fmt.Errorf("failed to unmarshal filesystem stats: %w", err)
+	}
 
-	req, err := http.NewRequestWithContext(ctx, "GET", metricsURL, nil)
+	return models.FSMetrics{
+		TotalBytes:     stats.Node.Fs.CapacityBytes,
+		UsedBytes:      stats.Node.Fs.UsedBytes,
+		AvailableBytes: stats.Node.Fs.AvailableBytes,
+		Inodes: models.InodeStats{
+			Free:  stats.Node.Fs.InodesFree,
+			Used:  stats.Node.Fs.InodesUsed,
+			Total: stats.Node.Fs.InodesFree + stats.Node.Fs.InodesUsed,
+		},
+	}, nil
+}
+
+func (nc *NodeCollector) collectContainerStats(ctx context.Context, node *v1.Node) (models.ContainerStats, error) {
+	metricFamilies, err := FetchMetricsViaKubelet(ctx, nc.clientset, node.Name, "metrics/cadvisor")
 	if err != nil {
-		return models.ResourceMetrics{}, fmt.Errorf("failed to create request: %w", err)
+		return models.ContainerStats{}, fmt.Errorf("failed to fetch container metrics: %w", err)
+	}
+
+	stats := models.ContainerStats{
+		PerContainer: make(map[string]models.ContainerMetrics),
 	}
 
-	// Add the bearer token to the request header
-	req.Header.Add("Authorization", "Bearer "+nc.bearerToken)
+	// Basic container count from running containers
+	if count, ok := metricFamilies["container_last_seen"]; ok {
+		stats.RunningCount = len(count.Metric)
+	}
+
+	return stats, nil
+}
 
-	resp, err := nc.httpClient.Do(req)
+func (nc *NodeCollector) collectSystemMetrics(ctx context.Context, node *v1.Node) (models.SystemMetrics, error) {
+	metricFamilies, err := FetchMetricsViaKubelet(ctx, nc.clientset, node.Name, "metrics")
 	if err != nil {
-		return models.ResourceMetrics{}, fmt.Errorf("failed to fetch cAdvisor metrics: %w", err)
+		return models.SystemMetrics{}, fmt.Errorf("failed to fetch system metrics: %w", err)
+	}
+
+	var systemMetrics models.SystemMetrics
+
+	// Parse system metrics
+	if nodeUptimeMetric, ok := metricFamilies["node_boot_time_seconds"]; ok {
+		for _, metric := range nodeUptimeMetric.Metric {
+			if metric.Gauge != nil {
+				systemMetrics.BootTimeSeconds = float64(metric.Gauge.GetValue())
+			}
+		}
 	}
-	defer resp.Body.Close()
 
-	if resp.StatusCode != http.StatusOK {
-		return models.ResourceMetrics{}, fmt.Errorf("cAdvisor returned non-200 status: %d", resp.StatusCode)
+	// Add additional system information
+	cpuValue := node.Status.Capacity.Cpu().Value()
+	if cpuValue >= 0 {
+		systemMetrics.NumCPUs = cpuValue
+	}
+	systemMetrics.KernelVersion = node.Status.NodeInfo.KernelVersion
+	systemMetrics.OSVersion = node.Status.NodeInfo.OSImage
+	systemMetrics.MachineID = node.Status.NodeInfo.MachineID
+	systemMetrics.SystemUUID = node.Status.NodeInfo.SystemUUID
+
+	return systemMetrics, nil
+}
+
+// safeInt64 converts uint64 to int64 safely
+func safeInt64(val uint64) int64 {
+	if val > math.MaxInt64 {
+		return math.MaxInt64
+	}
+	return int64(val)
+}
+
+// // safeInt32 converts uint64 to int32 safely
+// func safeInt32(val uint64) int32 {
+// 	if val > math.MaxInt32 {
+// 		return math.MaxInt32
+// 	}
+// 	return int32(val)
+// }
+
+// // safeUint64 converts int64 to uint64 safely
+// func safeUint64(val int64) uint64 {
+// 	if val < 0 {
+// 		return 0
+// 	}
+// 	return uint64(val)
+// }
+
+func (nc *NodeCollector) parseLatencyMetric(family *dto.MetricFamily) models.LatencyMetric {
+	latencyMetric := models.LatencyMetric{}
+
+	for _, metric := range family.Metric {
+		if metric.Summary != nil {
+			for _, quantile := range metric.Summary.Quantile {
+				switch *quantile.Quantile {
+				case 0.50:
+					latencyMetric.P50 = *quantile.Value
+				case 0.90:
+					latencyMetric.P90 = *quantile.Value
+				case 0.99:
+					latencyMetric.P99 = *quantile.Value
+				}
+			}
+			if metric.Summary.SampleCount != nil {
+				count := *metric.Summary.SampleCount
+				latencyMetric.Count = safeInt64(count)
+			}
+		}
 	}
 
-	var parser expfmt.TextParser
-	metricFamilies, err := parser.TextToMetricFamilies(resp.Body)
+	return latencyMetric
+}
+
+func (nc *NodeCollector) collectVolumeMetrics(ctx context.Context, node *v1.Node) (models.VolumeMetrics, error) {
+	metricFamilies, err := FetchMetricsViaKubelet(ctx, nc.clientset, node.Name, "metrics")
 	if err != nil {
-		return models.ResourceMetrics{}, fmt.Errorf("failed to parse cAdvisor metrics: %w", err)
+		return models.VolumeMetrics{}, fmt.Errorf("failed to fetch volume metrics: %w", err)
+	}
+
+	volumeMetrics := models.VolumeMetrics{
+		OperationLatency: make(map[string]models.LatencyMetric),
+	}
+
+	// Parse volume metrics
+	if attachCount, ok := metricFamilies["volume_manager_total_volumes"]; ok {
+		for _, metric := range attachCount.Metric {
+			if metric.Gauge != nil {
+				volumeMetrics.InUseCount = uint64(metric.Gauge.GetValue())
+			}
+		}
 	}
 
-	logrus.Debugf("Successfully fetched and parsed cAdvisor metrics for node %s", node.Name)
-	return nc.extractNodeUsageMetrics(metricFamilies), nil
+	// Parse operation latency metrics if available
+	if latencyMetrics, ok := metricFamilies["storage_operation_duration_seconds"]; ok {
+		for _, metric := range latencyMetrics.Metric {
+			if metric.Summary != nil {
+				opType := getLabel(metric, "operation_name")
+				if opType != "" {
+					volumeMetrics.OperationLatency[opType] = nc.parseLatencyMetric(latencyMetrics)
+				}
+			}
+		}
+	}
+
+	return volumeMetrics, nil
 }
 
-// extractNodeUsageMetrics extracts usage metrics from metric families.
-func (nc *NodeCollector) extractNodeUsageMetrics(metricFamilies map[string]*dto.MetricFamily) models.ResourceMetrics {
-	usage := models.ResourceMetrics{}
+func (nc *NodeCollector) collectNodeInfo(node *v1.Node) models.NodeInfo {
+	return models.NodeInfo{
+		Architecture:            node.Status.NodeInfo.Architecture,
+		ContainerRuntimeVersion: node.Status.NodeInfo.ContainerRuntimeVersion,
+		KernelVersion:           node.Status.NodeInfo.KernelVersion,
+		OSImage:                 node.Status.NodeInfo.OSImage,
+		KubeletVersion:          node.Status.NodeInfo.KubeletVersion,
+	}
+}
+
+func (nc *NodeCollector) collectHardwareTopology(ctx context.Context, node *v1.Node) (*models.HardwareTopology, error) {
+	cpuValue := node.Status.Capacity.Cpu().Value()
 
-	if cpuMetric, ok := metricFamilies["node_cpu_seconds_total"]; ok {
-		usage.CPU = nc.parseCPUUsage(cpuMetric)
+	// Since cpuValue is already an int64, this check is unnecessary
+	// You can remove the check entirely or if you want to be extra careful:
+	if cpuValue < 0 {
+		return nil, fmt.Errorf("invalid negative CPU value: %d", cpuValue)
 	}
 
-	if memoryMetric, ok := metricFamilies["node_memory_Active_bytes"]; ok {
-		usage.Memory = nc.parseMemoryUsage(memoryMetric)
+	topology := &models.HardwareTopology{
+		Sockets: cpuValue,
+		Cores:   cpuValue,
 	}
 
-	if storageMetric, ok := metricFamilies["node_filesystem_size_bytes"]; ok {
-		usage.Storage = nc.parseStorageUsage(storageMetric)
+	metricFamilies, err := FetchMetricsViaKubelet(ctx, nc.clientset, node.Name, "metrics/cadvisor")
+	if err != nil {
+		return topology, nil // Return basic topology even if detailed info fails
 	}
 
-	logrus.Debug("Successfully extracted node usage metrics")
-	return usage
+	// Parse NUMA information if available
+	if numaInfo, ok := metricFamilies["node_numa_memory_bytes"]; ok {
+		for _, metric := range numaInfo.Metric {
+			if nodeIDStr := getLabel(metric, "numa_node"); nodeIDStr != "" {
+				nodeID, err := strconv.ParseInt(nodeIDStr, 10, 32)
+				if err != nil {
+					continue
+				}
+				topology.NUMANodes = append(topology.NUMANodes, models.NUMANode{
+					ID:     int32(nodeID),
+					Memory: uint64(metric.Gauge.GetValue()),
+				})
+			}
+		}
+	}
+
+	return topology, nil
 }
 
-// parseCPUUsage parses CPU usage from the metric family.
-func (nc *NodeCollector) parseCPUUsage(family *dto.MetricFamily) int64 {
-	var totalCPUUsage float64
-	for _, metric := range family.Metric {
-		mode := ""
-		for _, label := range metric.Label {
-			if label.GetName() == "mode" {
-				mode = label.GetValue()
+func (nc *NodeCollector) collectPowerMetrics(ctx context.Context, node *v1.Node) (*models.PowerMetrics, error) {
+	metricFamilies, err := FetchMetricsViaKubelet(ctx, nc.clientset, node.Name, "metrics/cadvisor")
+	if err != nil {
+		return nil, fmt.Errorf("failed to fetch power metrics: %w", err)
+	}
+
+	powerMetrics := &models.PowerMetrics{}
+
+	if power, ok := metricFamilies["node_power_watts"]; ok {
+		for _, metric := range power.Metric {
+			if metric.Gauge != nil {
+				powerMetrics.CurrentWatts = metric.Gauge.GetValue()
 				break
 			}
 		}
-		if mode == "user" || mode == "system" {
-			if metric.Counter != nil && metric.Counter.Value != nil {
-				totalCPUUsage += metric.Counter.GetValue()
-			}
+	}
+
+	return powerMetrics, nil
+}
+
+func (nc *NodeCollector) collectNodeTaintsAndTolerations(node v1.Node) []models.NodeTaint {
+	taints := make([]models.NodeTaint, 0, len(node.Spec.Taints))
+	for _, taint := range node.Spec.Taints {
+		nodeTaint := models.NodeTaint{
+			Key:    taint.Key,
+			Value:  taint.Value,
+			Effect: string(taint.Effect),
 		}
+		if taint.TimeAdded != nil {
+			timeAdded := taint.TimeAdded.Time
+			nodeTaint.TimeAdded = &timeAdded
+		}
+		taints = append(taints, nodeTaint)
 	}
-	logrus.Debug("Successfully parsed CPU usage metrics")
-	return int64(totalCPUUsage * 1000) // Convert to millicores
+	return taints
 }
 
-// parseMemoryUsage parses memory usage from the metric family.
-func (nc *NodeCollector) parseMemoryUsage(family *dto.MetricFamily) int64 {
-	if len(family.Metric) > 0 && family.Metric[0].Gauge != nil && family.Metric[0].Gauge.Value != nil {
-		logrus.Debug("Successfully parsed memory usage metrics")
-		return int64(family.Metric[0].Gauge.GetValue())
+func (nc *NodeCollector) collectNodeLease(ctx context.Context, nodeName string) (*models.NodeLease, error) {
+	// If anything fails, log it and return empty lease
+	if nc == nil || nc.clientset == nil {
+		logrus.Warn("node collector or clientset is nil")
+		return &models.NodeLease{}, nil
+	}
+
+	lease, err := nc.clientset.CoordinationV1().Leases("kube-node-lease").Get(ctx, nodeName, metav1.GetOptions{})
+	if err != nil {
+		logrus.WithError(err).Debug("failed to get node lease")
+		return &models.NodeLease{}, nil
 	}
-	return 0
+
+	result := &models.NodeLease{}
+
+	// Safely get values, defaulting to empty/zero values if nil
+	if lease.Spec.HolderIdentity != nil {
+		result.HolderIdentity = *lease.Spec.HolderIdentity
+	}
+	if lease.Spec.LeaseDurationSeconds != nil {
+		result.LeaseDurationSeconds = *lease.Spec.LeaseDurationSeconds
+	}
+	if lease.Spec.AcquireTime != nil {
+		result.AcquireTime = &lease.Spec.AcquireTime.Time
+	}
+	if lease.Spec.RenewTime != nil {
+		result.RenewTime = &lease.Spec.RenewTime.Time
+	}
+
+	return result, nil
 }
 
-// parseStorageUsage parses storage usage from the metric family.
-func (nc *NodeCollector) parseStorageUsage(family *dto.MetricFamily) int64 {
-	var totalStorage int64
-	for _, metric := range family.Metric {
-		device := ""
-		for _, label := range metric.Label {
-			if label.GetName() == "mountpoint" {
-				device = label.GetValue()
-				break
+func (nc *NodeCollector) collectExtendedResources(node v1.Node) map[string]models.ExtendedResource {
+	resources := make(map[string]models.ExtendedResource)
+
+	for resourceName, quantity := range node.Status.Capacity {
+		if !isStandardResource(resourceName) {
+			capUnstructured := quantity.ToUnstructured()
+			allocUnstructured := node.Status.Allocatable[resourceName].ToUnstructured()
+
+			resources[string(resourceName)] = models.ExtendedResource{
+				Name:        string(resourceName),
+				Capacity:    fmt.Sprintf("%v", capUnstructured),
+				Allocatable: fmt.Sprintf("%v", allocUnstructured),
 			}
 		}
-		if device == "/" {
-			if metric.Gauge != nil && metric.Gauge.Value != nil {
-				totalStorage += int64(metric.Gauge.GetValue())
+	}
+
+	return resources
+}
+
+// Helper function to identify standard resources
+func isStandardResource(resourceName v1.ResourceName) bool {
+	standardResources := []v1.ResourceName{
+		v1.ResourceCPU,
+		v1.ResourceMemory,
+		v1.ResourceStorage,
+		v1.ResourceEphemeralStorage,
+		v1.ResourcePods,
+	}
+
+	for _, std := range standardResources {
+		if resourceName == std {
+			return true
+		}
+	}
+	return false
+}
+
+func (nc *NodeCollector) collectVolumeHealthMetrics(ctx context.Context, node *v1.Node) ([]models.VolumeHealthMetrics, error) {
+	pods, err := nc.clientset.CoreV1().Pods("").List(ctx, metav1.ListOptions{
+		FieldSelector: fmt.Sprintf("spec.nodeName=%s", node.Name),
+	})
+	if err != nil {
+		return nil, err
+	}
+
+	var volumeHealthMetrics []models.VolumeHealthMetrics
+
+	for _, pod := range pods.Items {
+		for _, volume := range pod.Spec.Volumes {
+			if volume.PersistentVolumeClaim != nil {
+				pvc, err := nc.clientset.CoreV1().PersistentVolumeClaims(pod.Namespace).Get(ctx, volume.PersistentVolumeClaim.ClaimName, metav1.GetOptions{})
+				if err != nil {
+					continue
+				}
+
+				metric := models.VolumeHealthMetrics{
+					VolumeName: volume.Name,
+					PodName:    pod.Name,
+					Namespace:  pod.Namespace,
+					State:      string(pvc.Status.Phase),
+				}
+
+				volumeHealthMetrics = append(volumeHealthMetrics, metric)
 			}
-			break
 		}
 	}
-	logrus.Debug("Successfully parsed storage usage metrics")
-	return totalStorage
+
+	return volumeHealthMetrics, nil
+}
+
+func (nc *NodeCollector) collectVolumeAttachmentMetrics(ctx context.Context, nodeName string) ([]models.VolumeAttachmentMetrics, error) {
+	// Get all volume attachments and filter by node name in memory
+	attachments, err := nc.clientset.StorageV1().VolumeAttachments().List(ctx, metav1.ListOptions{})
+	if err != nil {
+		return nil, err
+	}
+
+	var metrics []models.VolumeAttachmentMetrics
+
+	for _, attachment := range attachments.Items {
+		// Filter for attachments matching our node
+		if attachment.Spec.NodeName != nodeName {
+			continue
+		}
+
+		metric := models.VolumeAttachmentMetrics{
+			VolumeName: func() string {
+				if attachment.Spec.Source.PersistentVolumeName != nil {
+					return *attachment.Spec.Source.PersistentVolumeName
+				}
+				return ""
+			}(),
+			AttachmentState: func() string {
+				if attachment.Status.Attached {
+					return "attached"
+				}
+				return "detached"
+			}(),
+			AttachTime: attachment.CreationTimestamp.Time,
+		}
+
+		metrics = append(metrics, metric)
+	}
+
+	return metrics, nil
+}
+
+// CollectNodeMetrics collects all metrics for a node
+func (nc *NodeCollector) CollectNodeMetrics(ctx context.Context, node *v1.Node) (*models.EnhancedNodeMetrics, error) {
+	metrics := &models.EnhancedNodeMetrics{
+		Name:        node.Name,
+		Conditions:  nc.getNodeConditions(node),
+		Labels:      node.Labels,
+		Annotations: node.Annotations,
+		NodeInfo:    nc.collectNodeInfo(node),
+	}
+
+	// Collect resource metrics
+	metrics.Capacity = nc.parseResourceList(node.Status.Capacity)
+	metrics.Allocatable = nc.parseResourceList(node.Status.Allocatable)
+
+	// Collect all detailed metrics in parallel using goroutines
+	var wg sync.WaitGroup
+	var errChan = make(chan error, 15) // Buffer for potential errors
+	var mu sync.Mutex
+
+	detailedMetrics := models.NodeDetailedMetrics{}
+
+	// Helper function to safely update metrics
+	safeUpdate := func(f func()) {
+		mu.Lock()
+		defer mu.Unlock()
+		f()
+	}
+
+	// CPU Metrics
+	wg.Add(1)
+	go func() {
+		defer wg.Done()
+		if cpuMetrics, err := nc.collectCPUMetrics(ctx, node); err == nil {
+			safeUpdate(func() { detailedMetrics.CPU = cpuMetrics })
+		} else {
+			errChan <- fmt.Errorf("CPU metrics: %w", err)
+		}
+	}()
+
+	// Memory Metrics
+	wg.Add(1)
+	go func() {
+		defer wg.Done()
+		if memMetrics, err := nc.collectMemoryMetrics(ctx, node); err == nil {
+			safeUpdate(func() { detailedMetrics.Memory = memMetrics })
+		} else {
+			errChan <- fmt.Errorf("memory metrics: %w", err)
+		}
+	}()
+
+	// Network Metrics
+	wg.Add(1)
+	go func() {
+		defer wg.Done()
+		if netMetrics, err := nc.collectNetworkMetrics(ctx, node); err == nil {
+			safeUpdate(func() { detailedMetrics.Network = netMetrics })
+		} else {
+			errChan <- fmt.Errorf("network metrics: %w", err)
+		}
+	}()
+
+	// Disk Metrics
+	wg.Add(1)
+	go func() {
+		defer wg.Done()
+		if diskMetrics, err := nc.collectDiskMetrics(ctx, node); err == nil {
+			safeUpdate(func() { detailedMetrics.DiskIO = diskMetrics })
+		} else {
+			errChan <- fmt.Errorf("disk metrics: %w", err)
+		}
+	}()
+
+	// Filesystem Metrics
+	wg.Add(1)
+	go func() {
+		defer wg.Done()
+		if fsMetrics, err := nc.collectFilesystemMetrics(ctx, node); err == nil {
+			safeUpdate(func() { detailedMetrics.Filesystem = fsMetrics })
+		} else {
+			errChan <- fmt.Errorf("filesystem metrics: %w", err)
+		}
+	}()
+
+	// Process Metrics
+	wg.Add(1)
+	go func() {
+		defer wg.Done()
+		if procMetrics, err := nc.collectProcessMetrics(ctx, node); err == nil {
+			safeUpdate(func() { detailedMetrics.Process = procMetrics })
+		} else {
+			errChan <- fmt.Errorf("process metrics: %w", err)
+		}
+	}()
+
+	// Runtime Metrics
+	wg.Add(1)
+	go func() {
+		defer wg.Done()
+		if runtimeMetrics, err := nc.collectRuntimeMetrics(ctx, node); err == nil {
+			safeUpdate(func() { detailedMetrics.Runtime = runtimeMetrics })
+		} else {
+			errChan <- fmt.Errorf("runtime metrics: %w", err)
+		}
+	}()
+
+	// System Metrics
+	wg.Add(1)
+	go func() {
+		defer wg.Done()
+		if sysMetrics, err := nc.collectSystemMetrics(ctx, node); err == nil {
+			safeUpdate(func() { detailedMetrics.System = sysMetrics })
+		} else {
+			errChan <- fmt.Errorf("system metrics: %w", err)
+		}
+	}()
+
+	// Volume Metrics
+	wg.Add(1)
+	go func() {
+		defer wg.Done()
+		if volMetrics, err := nc.collectVolumeMetrics(ctx, node); err == nil {
+			safeUpdate(func() { detailedMetrics.Volume = volMetrics })
+		} else {
+			errChan <- fmt.Errorf("volume metrics: %w", err)
+		}
+	}()
+
+	// Container Stats
+	wg.Add(1)
+	go func() {
+		defer wg.Done()
+		if containerStats, err := nc.collectContainerStats(ctx, node); err == nil {
+			safeUpdate(func() { detailedMetrics.Container = containerStats })
+		} else {
+			errChan <- fmt.Errorf("container stats: %w", err)
+		}
+	}()
+
+	// Hardware Topology
+	wg.Add(1)
+	go func() {
+		defer wg.Done()
+		if topology, err := nc.collectHardwareTopology(ctx, node); err == nil {
+			safeUpdate(func() { metrics.HardwareTopology = topology })
+		} else {
+			errChan <- fmt.Errorf("hardware topology: %w", err)
+		}
+	}()
+
+	// Power Metrics
+	wg.Add(1)
+	go func() {
+		defer wg.Done()
+		if powerMetrics, err := nc.collectPowerMetrics(ctx, node); err == nil {
+			safeUpdate(func() { metrics.PowerMetrics = powerMetrics })
+		} else {
+			errChan <- fmt.Errorf("power metrics: %w", err)
+		}
+	}()
+
+	// Volume Health
+	wg.Add(1)
+	go func() {
+		defer wg.Done()
+		if volumeHealth, err := nc.collectVolumeHealthMetrics(ctx, node); err == nil {
+			safeUpdate(func() { metrics.VolumeHealth = volumeHealth })
+		} else {
+			errChan <- fmt.Errorf("volume health: %w", err)
+		}
+	}()
+
+	// Volume Attachments
+	wg.Add(1)
+	go func() {
+		defer wg.Done()
+		if attachments, err := nc.collectVolumeAttachmentMetrics(ctx, node.Name); err == nil {
+			safeUpdate(func() { metrics.VolumeAttachments = attachments })
+		} else {
+			errChan <- fmt.Errorf("volume attachments: %w", err)
+		}
+	}()
+
+	// Node Lease
+	wg.Add(1)
+	go func() {
+		defer wg.Done()
+		lease, _ := nc.collectNodeLease(ctx, node.Name) // Ignore error as we always return a value
+		safeUpdate(func() {
+			metrics.Lease = lease
+		})
+	}()
+
+	// Wait for all collectors to complete
+	wg.Wait()
+	close(errChan)
+
+	// Collect any errors
+	var errors []string
+	for err := range errChan {
+		errors = append(errors, err.Error())
+	}
+
+	// Add non-parallel collections
+	metrics.Taints = nc.collectNodeTaintsAndTolerations(*node)
+	metrics.ExtendedResources = nc.collectExtendedResources(*node)
+	metrics.DetailedMetrics = detailedMetrics
+
+	// If there were any errors, log them but don't fail the entire collection
+	if len(errors) > 0 {
+		logrus.WithField("errors", strings.Join(errors, "; ")).
+			Warn("Some metrics collections failed")
+	}
+
+	return metrics, nil
+}
+
+func (nc *NodeCollector) getNodeConditions(node *v1.Node) models.NodeConditions {
+	conditions := models.NodeConditions{}
+	for _, condition := range node.Status.Conditions {
+		switch condition.Type {
+		case v1.NodeReady:
+			conditions.Ready = condition.Status == v1.ConditionTrue
+		case v1.NodeMemoryPressure:
+			conditions.MemoryPressure = condition.Status == v1.ConditionTrue
+		case v1.NodeDiskPressure:
+			conditions.DiskPressure = condition.Status == v1.ConditionTrue
+		case v1.NodePIDPressure:
+			conditions.PIDPressure = condition.Status == v1.ConditionTrue
+		}
+	}
+	return conditions
+}
+
+// Helper function to get label value
+func getLabel(metric *dto.Metric, name string) string {
+	for _, label := range metric.Label {
+		if label.GetName() == name {
+			return label.GetValue()
+		}
+	}
+	return ""
 }
diff --git a/pkg/collectors/persistentvolume.go b/pkg/collectors/persistentvolume.go
index 7183ce4..7cf36d8 100644
--- a/pkg/collectors/persistentvolume.go
+++ b/pkg/collectors/persistentvolume.go
@@ -8,122 +8,106 @@
 // by the Apache License, Version 2.0, included in the file
 // licenses/APL.txt.
 // File: pkg/collectors/persistentvolume.go
+
+// Package collectors hosts the collection functions
 package collectors
 
 import (
 	"context"
+	// "crypto/tls"
 	"fmt"
+	// "net/http"
+	// "os"
 
 	v1 "k8s.io/api/core/v1"
 	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
 	"k8s.io/client-go/kubernetes"
 
+	// "k8s.io/client-go/rest"
+	// "k8s.io/client-go/transport"
+
 	"github.com/sirupsen/logrus"
 	"github.com/vegacloud/kubernetes/metricsagent/pkg/config"
 	"github.com/vegacloud/kubernetes/metricsagent/pkg/models"
 )
 
+// PersistentVolumeCollector collects metrics from Kubernetes persistent volumes.
 type PersistentVolumeCollector struct {
 	clientset *kubernetes.Clientset
 	config    *config.Config
 }
 
+// NewPersistentVolumeCollector creates a new PersistentVolumeCollector.
 func NewPersistentVolumeCollector(clientset *kubernetes.Clientset, cfg *config.Config) *PersistentVolumeCollector {
-	collector := &PersistentVolumeCollector{
+
+	logrus.Debug("PersistentVolumeCollector created successfully")
+	return &PersistentVolumeCollector{
 		clientset: clientset,
 		config:    cfg,
 	}
-	logrus.Debug("PersistentVolumeCollector created successfully")
-	return collector
-}
 
-func (pvc *PersistentVolumeCollector) CollectMetrics(ctx context.Context) (interface{}, error) {
-	metrics, err := pvc.CollectPersistentVolumeMetrics(ctx)
-	if err != nil {
-		return nil, err
-	}
-	logrus.Debug("Successfully collected persistent volume metrics")
-	return metrics, nil
 }
 
-func (pvc *PersistentVolumeCollector) CollectPersistentVolumeMetrics(
-	ctx context.Context,
-) (*models.PersistentVolumeMetrics, error) {
+// CollectMetrics collects metrics from Kubernetes persistent volumes.
+func (pvc *PersistentVolumeCollector) CollectMetrics(ctx context.Context) (interface{}, error) {
 	pvs, err := pvc.clientset.CoreV1().PersistentVolumes().List(ctx, metav1.ListOptions{})
 	if err != nil {
 		return nil, fmt.Errorf("failed to list persistent volumes: %w", err)
 	}
 	logrus.Debugf("Successfully listed %d persistent volumes", len(pvs.Items))
 
-	pvcs, err := pvc.clientset.CoreV1().PersistentVolumeClaims("").List(ctx, metav1.ListOptions{})
-	if err != nil {
-		return nil, fmt.Errorf("failed to list persistent volume claims: %w", err)
-	}
-	logrus.Debugf("Successfully listed %d persistent volume claims", len(pvcs.Items))
-
-	metrics := &models.PersistentVolumeMetrics{
-		PVs:  pvc.collectPVMetrics(pvs.Items),
-		PVCs: pvc.collectPVCMetrics(pvcs.Items),
-	}
-
-	logrus.Debug("Successfully calculated persistent volume metrics")
+	metrics := pvc.collectPVMetrics(pvs.Items)
+	logrus.Debug("Successfully collected persistent volume metrics")
 	return metrics, nil
 }
 
+// collectPVMetrics collects metrics from Kubernetes persistent volumes.
 func (pvc *PersistentVolumeCollector) collectPVMetrics(pvs []v1.PersistentVolume) []models.PVMetric {
-
 	pvMetrics := make([]models.PVMetric, 0, len(pvs))
 
 	for _, pv := range pvs {
 		if pv.Labels == nil {
 			pv.Labels = make(map[string]string)
 		}
-		metric := models.PVMetric{
-			Name:         pv.Name,
-			Capacity:     pv.Spec.Capacity.Storage().Value(),
-			Phase:        string(pv.Status.Phase),
-			StorageClass: pv.Spec.StorageClassName,
-			Labels:       pv.Labels,
-		}
 
-		if pv.Spec.ClaimRef != nil {
-			metric.BoundPVC = fmt.Sprintf("%s/%s", pv.Spec.ClaimRef.Namespace, pv.Spec.ClaimRef.Name)
+		accessModes := make([]string, 0)
+		for _, mode := range pv.Spec.AccessModes {
+			accessModes = append(accessModes, string(mode))
 		}
 
-		pvMetrics = append(pvMetrics, metric)
-	}
-
-	logrus.Debugf("Collected metrics for %d persistent volumes", len(pvMetrics))
-	return pvMetrics
-}
-
-func (pvc *PersistentVolumeCollector) collectPVCMetrics(pvcs []v1.PersistentVolumeClaim) []models.PVCMetric {
-
-	pvcMetrics := make([]models.PVCMetric, 0, len(pvcs))
-
-	for _, pvc := range pvcs {
-		if pvc.Labels == nil {
-			pvc.Labels = make(map[string]string)
+		metric := models.PVMetric{
+			Name:          pv.Name,
+			Capacity:      pv.Spec.Capacity.Storage().Value(),
+			Phase:         string(pv.Status.Phase),
+			StorageClass:  pv.Spec.StorageClassName,
+			Labels:        pv.Labels,
+			AccessModes:   accessModes,
+			ReclaimPolicy: string(pv.Spec.PersistentVolumeReclaimPolicy),
+			VolumeMode:    string(*pv.Spec.VolumeMode),
+			Status: models.PVStatus{
+				Phase:   string(pv.Status.Phase),
+				Message: pv.Status.Message,
+				Reason:  pv.Status.Reason,
+			},
+			MountOptions: pv.Spec.MountOptions,
 		}
-		metric := models.PVCMetric{
-			Name:      pvc.Name,
-			Namespace: pvc.Namespace,
-			Phase:     string(pvc.Status.Phase),
-			Capacity:  pvc.Status.Capacity.Storage().Value(),
-			Labels:    pvc.Labels,
+
+		if sc, err := pvc.clientset.StorageV1().StorageClasses().Get(context.Background(), pv.Spec.StorageClassName, metav1.GetOptions{}); err == nil {
+			metric.VolumeBindingMode = string(*sc.VolumeBindingMode)
 		}
 
-		if pvc.Spec.StorageClassName != nil {
-			metric.StorageClass = *pvc.Spec.StorageClassName
+		if pv.Spec.ClaimRef != nil {
+			metric.BoundPVC = fmt.Sprintf("%s/%s", pv.Spec.ClaimRef.Namespace, pv.Spec.ClaimRef.Name)
 		}
 
-		if pvc.Spec.VolumeName != "" {
-			metric.BoundPV = pvc.Spec.VolumeName
+		if ann := pv.Annotations; ann != nil {
+			if provisioner, ok := ann["pv.kubernetes.io/provisioned-by"]; ok {
+				metric.StorageProvisioner = provisioner
+			}
 		}
 
-		pvcMetrics = append(pvcMetrics, metric)
+		pvMetrics = append(pvMetrics, metric)
 	}
 
-	logrus.Debugf("Collected metrics for %d persistent volume claims", len(pvcMetrics))
-	return pvcMetrics
+	return pvMetrics
 }
diff --git a/pkg/collectors/persistentvolumeclaim.go b/pkg/collectors/persistentvolumeclaim.go
new file mode 100644
index 0000000..17854e1
--- /dev/null
+++ b/pkg/collectors/persistentvolumeclaim.go
@@ -0,0 +1,142 @@
+package collectors
+
+import (
+	"context"
+	"fmt"
+
+	v1 "k8s.io/api/core/v1"
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	"k8s.io/client-go/kubernetes"
+
+	"github.com/sirupsen/logrus"
+	"github.com/vegacloud/kubernetes/metricsagent/pkg/config"
+	"github.com/vegacloud/kubernetes/metricsagent/pkg/models"
+)
+
+// PersistentVolumeClaimCollector collects metrics from Kubernetes persistent volume claims.
+type PersistentVolumeClaimCollector struct {
+	clientset *kubernetes.Clientset
+	config    *config.Config
+}
+
+// NewPersistentVolumeClaimCollector creates a new PersistentVolumeClaimCollector.
+func NewPersistentVolumeClaimCollector(clientset *kubernetes.Clientset, cfg *config.Config) *PersistentVolumeClaimCollector {
+	// logrus.Debug("Starting PersistentVolumeClaimCollector")
+	// if token, err := os.ReadFile("/var/run/secrets/kubernetes.io/serviceaccount/token"); err == nil {
+	// 	clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport = &http.Transport{
+	// 		TLSClientConfig: &tls.Config{
+	// 			InsecureSkipVerify: cfg.VegaInsecure,
+	// 		},
+	// 	}
+	// 	clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport = transport.NewBearerAuthRoundTripper(
+	// 		string(token),
+	// 		clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport,
+	// 	)
+	// }
+	logrus.Debug("PersistentVolumeClaimCollector created successfully")
+	return &PersistentVolumeClaimCollector{
+		clientset: clientset,
+		config:    cfg,
+	}
+
+}
+
+// CollectMetrics collects metrics from Kubernetes persistent volume claims.
+func (pvcc *PersistentVolumeClaimCollector) CollectMetrics(ctx context.Context) (interface{}, error) {
+	pvcs, err := pvcc.clientset.CoreV1().PersistentVolumeClaims("").List(ctx, metav1.ListOptions{})
+	if err != nil {
+		return nil, fmt.Errorf("failed to list persistent volume claims: %w", err)
+	}
+	logrus.Debugf("Successfully listed %d persistent volume claims", len(pvcs.Items))
+
+	metrics := pvcc.collectPVCMetrics(pvcs.Items)
+	logrus.Debug("Successfully collected persistent volume claim metrics")
+	return metrics, nil
+}
+
+// collectPVCMetrics collects metrics from Kubernetes persistent volume claims.
+func (pvcc *PersistentVolumeClaimCollector) collectPVCMetrics(pvcs []v1.PersistentVolumeClaim) []models.PVCMetric {
+	pvcMetrics := make([]models.PVCMetric, 0, len(pvcs))
+
+	for _, claim := range pvcs {
+		if claim.Labels == nil {
+			claim.Labels = make(map[string]string)
+		}
+
+		accessModes := make([]string, 0)
+		for _, mode := range claim.Spec.AccessModes {
+			accessModes = append(accessModes, string(mode))
+		}
+
+		conditions := make([]models.PVCCondition, 0)
+		for _, cond := range claim.Status.Conditions {
+			conditions = append(conditions, models.PVCCondition{
+				Type:               string(cond.Type),
+				Status:             string(cond.Status),
+				Reason:             cond.Reason,
+				Message:            cond.Message,
+				LastTransitionTime: &cond.LastTransitionTime.Time,
+			})
+		}
+
+		metric := models.PVCMetric{
+			Name:             claim.Name,
+			Namespace:        claim.Namespace,
+			Phase:            string(claim.Status.Phase),
+			Capacity:         claim.Status.Capacity.Storage().Value(),
+			RequestedStorage: claim.Spec.Resources.Requests.Storage().Value(),
+			Labels:           claim.Labels,
+			AccessModes:      accessModes,
+			VolumeMode:       string(*claim.Spec.VolumeMode),
+			VolumeName:       claim.Spec.VolumeName,
+			Status: models.PVCStatus{
+				Phase:      string(claim.Status.Phase),
+				Conditions: conditions,
+			},
+		}
+
+		if claim.Spec.StorageClassName != nil {
+			metric.StorageClass = *claim.Spec.StorageClassName
+			if sc, err := pvcc.clientset.StorageV1().StorageClasses().Get(context.Background(), *claim.Spec.StorageClassName, metav1.GetOptions{}); err == nil {
+				metric.VolumeBindingMode = string(*sc.VolumeBindingMode)
+			}
+		}
+
+		if claim.Spec.VolumeName != "" {
+			metric.BoundPV = claim.Spec.VolumeName
+			if boundPV, err := pvcc.clientset.CoreV1().PersistentVolumes().Get(context.Background(), claim.Spec.VolumeName, metav1.GetOptions{}); err == nil {
+				metric.MountOptions = boundPV.Spec.MountOptions
+			}
+		}
+
+		if ann := claim.Annotations; ann != nil {
+			if provisioner, ok := ann["volume.kubernetes.io/storage-provisioner"]; ok {
+				metric.StorageProvisioner = provisioner
+			}
+		}
+
+		if claim.Spec.Resources.Requests.Storage() != nil {
+			expansion := &models.VolumeExpansionMetrics{
+				CurrentSize:   claim.Status.Capacity.Storage().Value(),
+				RequestedSize: claim.Spec.Resources.Requests.Storage().Value(),
+				InProgress:    false,
+			}
+
+			for _, condition := range claim.Status.Conditions {
+				if condition.Type == v1.PersistentVolumeClaimResizing {
+					expansion.InProgress = true
+					expansion.LastResizeTime = &condition.LastTransitionTime.Time
+					expansion.ResizeStatus = string(condition.Status)
+					expansion.FailureMessage = condition.Message
+					break
+				}
+			}
+
+			metric.Expansion = expansion
+		}
+
+		pvcMetrics = append(pvcMetrics, metric)
+	}
+
+	return pvcMetrics
+}
diff --git a/pkg/collectors/pod.go b/pkg/collectors/pod.go
index e67cd48..89a7013 100644
--- a/pkg/collectors/pod.go
+++ b/pkg/collectors/pod.go
@@ -8,18 +8,24 @@
 // by the Apache License, Version 2.0, included in the file
 // licenses/APL.txt.
 // File: pkg/collectors/pod.go
+
+// Package collectors hosts the collection functions
 package collectors
 
 import (
 	"context"
+	"crypto/tls"
 	"fmt"
+	"net/http"
+	"os"
+	"time"
 
+	"github.com/prometheus/common/expfmt"
 	"github.com/sirupsen/logrus"
 	v1 "k8s.io/api/core/v1"
 	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	"k8s.io/apimachinery/pkg/labels"
 	"k8s.io/client-go/kubernetes"
-	metricsv1beta1 "k8s.io/metrics/pkg/apis/metrics/v1beta1"
-	metricsclientset "k8s.io/metrics/pkg/client/clientset/versioned"
 
 	"github.com/vegacloud/kubernetes/metricsagent/pkg/config"
 	"github.com/vegacloud/kubernetes/metricsagent/pkg/models"
@@ -27,28 +33,41 @@ import (
 
 // PodCollector collects metrics for all pods in the cluster
 type PodCollector struct {
-	clientset     *kubernetes.Clientset
-	metricsClient *metricsclientset.Clientset
-	config        *config.Config
+	clientset  *kubernetes.Clientset
+	config     *config.Config
+	httpClient *http.Client
 }
 
-// NewPodCollector creates a new PodCollector instance
-func NewPodCollector(
-	clientset *kubernetes.Clientset,
-	metricsClient *metricsclientset.Clientset,
-	cfg *config.Config,
-) *PodCollector {
-	collector := &PodCollector{
-		clientset:     clientset,
-		metricsClient: metricsClient,
-		config:        cfg,
+// NewPodCollector creates a new PodCollector
+func NewPodCollector(clientset *kubernetes.Clientset, cfg *config.Config) *PodCollector {
+	logrus.Debug("Starting PodCollector")
+
+	// Initialize HTTP client with reasonable defaults
+	httpClient := &http.Client{
+		Timeout: time.Second * 10,
+		Transport: &http.Transport{
+			TLSClientConfig: &tls.Config{
+				InsecureSkipVerify: cfg.VegaInsecure,
+			},
+		},
 	}
+
 	logrus.Debug("PodCollector created successfully")
-	return collector
+	return &PodCollector{
+		clientset:  clientset,
+		config:     cfg,
+		httpClient: httpClient,
+	}
 }
 
 // CollectMetrics collects metrics for all pods in the cluster
 func (pc *PodCollector) CollectMetrics(ctx context.Context) (interface{}, error) {
+	logrus.WithField("collector", "PodCollector")
+
+	// Verify client identity before collecting metrics
+	if err := VerifyCollectorClient(ctx, pc.clientset, pc.config.VegaNamespace, "PodCollector"); err != nil {
+		return nil, err
+	}
 	metrics, err := pc.CollectEnhancedPodMetrics(ctx)
 	if err != nil {
 		return nil, err
@@ -57,6 +76,7 @@ func (pc *PodCollector) CollectMetrics(ctx context.Context) (interface{}, error)
 	return metrics, nil
 }
 
+// CollectEnhancedPodMetrics collects metrics from Kubernetes pods.
 func (pc *PodCollector) CollectEnhancedPodMetrics(ctx context.Context) ([]models.EnhancedPodMetrics, error) {
 	pods, err := pc.clientset.CoreV1().Pods("").List(ctx, metav1.ListOptions{})
 	if err != nil {
@@ -83,6 +103,7 @@ func (pc *PodCollector) collectSinglePodMetrics(ctx context.Context, pod v1.Pod)
 	if pod.Labels == nil {
 		pod.Labels = make(map[string]string)
 	}
+
 	metrics := models.EnhancedPodMetrics{
 		PodMetrics: models.PodMetrics{
 			Name:      pod.Name,
@@ -91,8 +112,45 @@ func (pc *PodCollector) collectSinglePodMetrics(ctx context.Context, pod v1.Pod)
 			Labels:    pod.Labels,
 		},
 		QoSClass: string(pod.Status.QOSClass),
+		StartTime: func() *time.Time {
+			if pod.Status.StartTime != nil {
+				t := pod.Status.StartTime.Time
+				return &t
+			}
+			return nil
+		}(),
+		Priority:          pod.Spec.Priority,
+		PriorityClassName: pod.Spec.PriorityClassName,
+		NodeName:          pod.Spec.NodeName,
+		HostIP:            pod.Status.HostIP,
+		NominatedNodeName: pod.Status.NominatedNodeName,
+	}
+
+	// Collect Pod IPs
+	podIPs := make([]string, 0, len(pod.Status.PodIPs))
+	for _, ip := range pod.Status.PodIPs {
+		podIPs = append(podIPs, ip.IP)
+	}
+	metrics.PodIPs = podIPs
+
+	// Collect Readiness Gates
+	readinessGates := make([]models.PodReadinessGate, 0, len(pod.Spec.ReadinessGates))
+	for _, gate := range pod.Spec.ReadinessGates {
+		status := false
+		for _, condition := range pod.Status.Conditions {
+			if string(condition.Type) == string(gate.ConditionType) {
+				status = condition.Status == v1.ConditionTrue
+				break
+			}
+		}
+		readinessGates = append(readinessGates, models.PodReadinessGate{
+			ConditionType: string(gate.ConditionType),
+			Status:        status,
+		})
 	}
+	metrics.ReadinessGates = readinessGates
 
+	// Collect Pod Conditions
 	for _, condition := range pod.Status.Conditions {
 		switch condition.Type {
 		case v1.PodScheduled:
@@ -106,6 +164,7 @@ func (pc *PodCollector) collectSinglePodMetrics(ctx context.Context, pod v1.Pod)
 		}
 	}
 
+	// Collect Resource Metrics
 	for _, container := range pod.Spec.Containers {
 		metrics.Requests.CPU += container.Resources.Requests.Cpu().MilliValue()
 		metrics.Requests.Memory += container.Resources.Requests.Memory().Value()
@@ -113,10 +172,8 @@ func (pc *PodCollector) collectSinglePodMetrics(ctx context.Context, pod v1.Pod)
 		metrics.Limits.Memory += container.Resources.Limits.Memory().Value()
 	}
 
-	podMetrics, err := pc.metricsClient.
-		MetricsV1beta1().
-		PodMetricses(pod.Namespace).
-		Get(ctx, pod.Name, metav1.GetOptions{})
+	// Get Pod Metrics from Kubelet
+	podMetrics, err := pc.getPodMetrics(ctx, &pod)
 	if err != nil {
 		return metrics, fmt.Errorf("failed to get pod metrics: %w", err)
 	}
@@ -124,16 +181,233 @@ func (pc *PodCollector) collectSinglePodMetrics(ctx context.Context, pod v1.Pod)
 	metrics.Containers = pc.extractContainerMetrics(pod, podMetrics)
 	metrics.TotalRestarts = pc.getTotalRestarts(pod)
 
+	// Set completion time for completed pods
+	if pod.Status.Phase == v1.PodSucceeded || pod.Status.Phase == v1.PodFailed {
+		for _, containerStatus := range pod.Status.ContainerStatuses {
+			if containerStatus.State.Terminated != nil {
+				metrics.CompletionTime = &containerStatus.State.Terminated.FinishedAt.Time
+				break
+			}
+		}
+	}
+
+	// Add annotations
+	metrics.Annotations = pod.Annotations
+
+	// Add volume mounts
+	metrics.VolumeMounts = make([]models.VolumeMountMetrics, 0)
+	for _, container := range pod.Spec.Containers {
+		for _, volumeMount := range container.VolumeMounts {
+			metrics.VolumeMounts = append(metrics.VolumeMounts, models.VolumeMountMetrics{
+				Name:        volumeMount.Name,
+				MountPath:   volumeMount.MountPath,
+				ReadOnly:    volumeMount.ReadOnly,
+				SubPath:     volumeMount.SubPath,
+				SubPathExpr: volumeMount.SubPathExpr,
+				MountPropagation: func() string {
+					if volumeMount.MountPropagation != nil {
+						return string(*volumeMount.MountPropagation)
+					}
+					return ""
+				}(),
+			})
+		}
+	}
+
+	// Add image pull policy
+	imagePullPolicies := make([]string, 0)
+	for _, container := range pod.Spec.Containers {
+		imagePullPolicies = append(imagePullPolicies, string(container.ImagePullPolicy))
+	}
+	// Use the most common pull policy, or "Mixed" if there are different policies
+	if len(imagePullPolicies) > 0 {
+		allSame := true
+		for i := 1; i < len(imagePullPolicies); i++ {
+			if imagePullPolicies[i] != imagePullPolicies[0] {
+				allSame = false
+				break
+			}
+		}
+		if allSame {
+			metrics.ImagePullPolicy = imagePullPolicies[0]
+		} else {
+			metrics.ImagePullPolicy = "Mixed"
+		}
+	}
+
+	// Add service account information
+	metrics.ServiceAccountName = pod.Spec.ServiceAccountName
+
+	// Collect PDB metrics
+	pdbMetrics, err := pc.collectPodDisruptionBudget(ctx, pod)
+	if err != nil {
+		logrus.Warnf("Failed to collect PDB metrics for pod %s/%s: %v", pod.Namespace, pod.Name, err)
+	} else {
+		metrics.DisruptionBudget = pdbMetrics
+	}
+
+	// Collect topology spread constraints
+	metrics.TopologySpread = pc.collectTopologySpread(pod)
+
+	// Collect pod overhead
+	if pod.Spec.Overhead != nil {
+		metrics.Overhead = &models.PodOverheadMetrics{
+			CPU:    pod.Spec.Overhead.Cpu().String(),
+			Memory: pod.Spec.Overhead.Memory().String(),
+		}
+	}
+
+	// Collect scheduling gates
+	for _, gate := range pod.Spec.SchedulingGates {
+		metrics.SchedulingGates = append(metrics.SchedulingGates, models.PodSchedulingGate{
+			Name:   gate.Name,
+			Active: true,
+		})
+	}
+
+	// Collect security context
+	metrics.SecurityContext = pc.collectSecurityContext(pod)
+
+	// Collect QoS details
+	metrics.QoSDetails = pc.collectQoSDetails(pod)
+
 	logrus.Debugf("Successfully collected metrics for pod %s/%s", pod.Namespace, pod.Name)
 	return metrics, nil
 }
 
+func (pc *PodCollector) getPodMetrics(ctx context.Context, pod *v1.Pod) (*models.PodMetrics, error) {
+	// Initialize empty metrics structure
+	metrics := &models.PodMetrics{
+		Name:       pod.Name,
+		Namespace:  pod.Namespace,
+		Usage:      models.ResourceMetrics{},
+		Containers: make([]models.ContainerMetrics, 0),
+	}
+
+	// Skip metrics collection if pod is not running on a node
+	if pod.Spec.NodeName == "" {
+		logrus.Debugf("Pod %s/%s is not scheduled on any node, skipping metrics collection", pod.Namespace, pod.Name)
+		return metrics, nil
+	}
+
+	// Get node internal IP where the pod is running
+	node, err := pc.clientset.CoreV1().Nodes().Get(ctx, pod.Spec.NodeName, metav1.GetOptions{})
+	if err != nil {
+		logrus.Warnf("Failed to get node info for pod %s/%s: %v", pod.Namespace, pod.Name, err)
+		return metrics, nil
+	}
+
+	var nodeAddress string
+	for _, addr := range node.Status.Addresses {
+		if addr.Type == v1.NodeInternalIP {
+			nodeAddress = addr.Address
+			break
+		}
+	}
+
+	if nodeAddress == "" {
+		logrus.Warnf("No internal IP found for node %s", pod.Spec.NodeName)
+		return metrics, nil
+	}
+
+	// Ensure HTTP client exists
+	if pc.httpClient == nil {
+		logrus.Warn("HTTP client not initialized, creating default client")
+		pc.httpClient = &http.Client{
+			Timeout: time.Second * 10,
+			Transport: &http.Transport{
+				TLSClientConfig: &tls.Config{
+					InsecureSkipVerify: pc.config.VegaInsecure,
+				},
+			},
+		}
+	}
+
+	// Construct URL for kubelet metrics
+	metricsURL := fmt.Sprintf("https://%s:10250/metrics/resource", nodeAddress)
+
+	// Create request
+	req, err := http.NewRequestWithContext(ctx, "GET", metricsURL, nil)
+	if err != nil {
+		logrus.Warnf("Failed to create request for pod %s/%s: %v", pod.Namespace, pod.Name, err)
+		return metrics, nil
+	}
+
+	// Get bearer token from service account
+	token, err := os.ReadFile("/var/run/secrets/kubernetes.io/serviceaccount/token")
+	if err != nil {
+		logrus.Warnf("Failed to read service account token: %v", err)
+		return metrics, nil
+	}
+	req.Header.Set("Authorization", "Bearer "+string(token))
+
+	// Make request
+	resp, err := pc.httpClient.Do(req)
+	if err != nil {
+		logrus.Warnf("Failed to get metrics from kubelet for pod %s/%s: %v", pod.Namespace, pod.Name, err)
+		return metrics, nil
+	}
+	defer resp.Body.Close()
+
+	// Parse metrics
+	var parser expfmt.TextParser
+	metricFamilies, err := parser.TextToMetricFamilies(resp.Body)
+	if err != nil {
+		logrus.Warnf("Failed to parse metrics for pod %s/%s: %v", pod.Namespace, pod.Name, err)
+		return metrics, nil
+	}
+
+	containerMetrics := make(map[string]*models.ContainerMetrics)
+
+	// Parse container metrics for the pod
+	for _, family := range metricFamilies {
+		for _, metric := range family.Metric {
+			labels := make(map[string]string)
+			for _, label := range metric.Label {
+				labels[*label.Name] = *label.Value
+			}
+
+			// Match metrics for this specific pod
+			if labels["pod"] == pod.Name && labels["namespace"] == pod.Namespace {
+				containerName := labels["container"]
+
+				// Initialize container metrics if not exists
+				if _, exists := containerMetrics[containerName]; !exists {
+					containerMetrics[containerName] = &models.ContainerMetrics{
+						Name: containerName,
+					}
+				}
+
+				// Update container metrics based on metric type
+				switch family.GetName() {
+				case "container_cpu_usage_seconds_total":
+					value := int64(*metric.Counter.Value * 1000) // Convert to millicores
+					containerMetrics[containerName].UsageNanos = value
+					metrics.Usage.CPU += value
+				case "container_memory_working_set_bytes":
+					value := int64(*metric.Gauge.Value)
+					containerMetrics[containerName].UsageBytes = value
+					metrics.Usage.Memory += value
+					containerMetrics[containerName].Memory.WorkingSet = uint64(*metric.Gauge.Value)
+				}
+			}
+		}
+	}
+
+	// Convert map to slice
+	for _, cm := range containerMetrics {
+		metrics.Containers = append(metrics.Containers, *cm)
+	}
+
+	return metrics, nil
+}
+
 func (pc *PodCollector) extractContainerMetrics(
 	pod v1.Pod,
-	podMetrics *metricsv1beta1.PodMetrics,
+	podMetrics *models.PodMetrics,
 ) []models.ContainerMetrics {
 
-	containerMetrics := make([]models.ContainerMetrics, 0, len(podMetrics.Containers))
+	containerMetrics := make([]models.ContainerMetrics, 0, len(pod.Status.ContainerStatuses))
 
 	for _, container := range pod.Status.ContainerStatuses {
 		metrics := models.ContainerMetrics{
@@ -144,13 +418,14 @@ func (pc *PodCollector) extractContainerMetrics(
 		}
 
 		if container.LastTerminationState.Terminated != nil {
-			metrics.LastTerminated = container.LastTerminationState.Terminated.Reason
+			metrics.LastTerminationReason = container.LastTerminationState.Terminated.Reason
 		}
 
-		for _, c := range podMetrics.Containers {
-			if c.Name == container.Name {
-				metrics.CPU = c.Usage.Cpu().MilliValue()
-				metrics.Memory = c.Usage.Memory().Value()
+		// Match container usage from podMetrics
+		for _, containerName := range podMetrics.Containers {
+			if containerName.Name == container.Name {
+				metrics.CPU = containerName.CPU
+				metrics.Memory = containerName.Memory
 				break
 			}
 		}
@@ -183,3 +458,85 @@ func getContainerState(state v1.ContainerState) string {
 	}
 	return "Unknown"
 }
+
+func (pc *PodCollector) collectPodDisruptionBudget(ctx context.Context, pod v1.Pod) (*models.PodDisruptionBudgetMetrics, error) {
+	pdbs, err := pc.clientset.PolicyV1().PodDisruptionBudgets(pod.Namespace).List(ctx, metav1.ListOptions{})
+	if err != nil {
+		return nil, err
+	}
+
+	for _, pdb := range pdbs.Items {
+		selector, err := metav1.LabelSelectorAsSelector(pdb.Spec.Selector)
+		if err != nil {
+			continue
+		}
+		if selector.Matches(labels.Set(pod.Labels)) {
+			return &models.PodDisruptionBudgetMetrics{
+				MinAvailable:       pdb.Spec.MinAvailable.String(),
+				MaxUnavailable:     pdb.Spec.MaxUnavailable.String(),
+				CurrentHealthy:     pdb.Status.CurrentHealthy,
+				DesiredHealthy:     pdb.Status.DesiredHealthy,
+				DisruptionsAllowed: pdb.Status.DisruptionsAllowed,
+				ExpectedPods:       pdb.Status.ExpectedPods,
+			}, nil
+		}
+	}
+	return nil, nil
+}
+
+func (pc *PodCollector) collectTopologySpread(pod v1.Pod) []models.TopologySpreadConstraint {
+	var constraints []models.TopologySpreadConstraint
+	for _, constraint := range pod.Spec.TopologySpreadConstraints {
+		constraints = append(constraints, models.TopologySpreadConstraint{
+			MaxSkew:           constraint.MaxSkew,
+			TopologyKey:       constraint.TopologyKey,
+			WhenUnsatisfiable: string(constraint.WhenUnsatisfiable),
+			LabelSelector:     constraint.LabelSelector.String(),
+			MinDomains:        constraint.MinDomains,
+		})
+	}
+	return constraints
+}
+
+func (pc *PodCollector) collectSecurityContext(pod v1.Pod) *models.SecurityContextMetrics {
+	if pod.Spec.SecurityContext == nil {
+		return nil
+	}
+
+	return &models.SecurityContextMetrics{
+		RunAsUser:    pod.Spec.SecurityContext.RunAsUser,
+		RunAsGroup:   pod.Spec.SecurityContext.RunAsGroup,
+		FSGroup:      pod.Spec.SecurityContext.FSGroup,
+		RunAsNonRoot: pod.Spec.SecurityContext.RunAsNonRoot,
+		SELinuxOptions: func() map[string]string {
+			if pod.Spec.SecurityContext.SELinuxOptions != nil {
+				return map[string]string{
+					"user":  pod.Spec.SecurityContext.SELinuxOptions.User,
+					"role":  pod.Spec.SecurityContext.SELinuxOptions.Role,
+					"type":  pod.Spec.SecurityContext.SELinuxOptions.Type,
+					"level": pod.Spec.SecurityContext.SELinuxOptions.Level,
+				}
+			}
+			return nil
+		}(),
+	}
+}
+
+func (pc *PodCollector) collectQoSDetails(pod v1.Pod) *models.QoSMetrics {
+	qosMetrics := &models.QoSMetrics{
+		Class:            string(pod.Status.QOSClass),
+		CPUGuaranteed:    true,
+		MemoryGuaranteed: true,
+	}
+
+	for _, container := range pod.Spec.Containers {
+		if container.Resources.Requests.Cpu().IsZero() {
+			qosMetrics.CPUGuaranteed = false
+		}
+		if container.Resources.Requests.Memory().IsZero() {
+			qosMetrics.MemoryGuaranteed = false
+		}
+	}
+
+	return qosMetrics
+}
diff --git a/pkg/collectors/replicaset.go b/pkg/collectors/replicaset.go
index 360cff1..2b74028 100644
--- a/pkg/collectors/replicaset.go
+++ b/pkg/collectors/replicaset.go
@@ -7,6 +7,8 @@
 // the Business Source License, use of this software will be governed
 // by the Apache License, Version 2.0, included in the file
 // licenses/APL.txt.
+
+// Package collectors hosts the collection functions
 package collectors
 
 import (
@@ -30,12 +32,23 @@ type ReplicaSetCollector struct {
 
 // NewReplicaSetCollector creates a new ReplicaSetCollector instance
 func NewReplicaSetCollector(clientset *kubernetes.Clientset, cfg *config.Config) *ReplicaSetCollector {
-	collector := &ReplicaSetCollector{
+	// logrus.Debug("Starting ReplicaSetCollector")
+	// if token, err := os.ReadFile("/var/run/secrets/kubernetes.io/serviceaccount/token"); err == nil {
+	// 	clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport = &http.Transport{
+	// 		TLSClientConfig: &tls.Config{
+	// 			InsecureSkipVerify: cfg.VegaInsecure,
+	// 		},
+	// 	}
+	// 	clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport = transport.NewBearerAuthRoundTripper(
+	// 		string(token),
+	// 		clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport,
+	// 	)
+	// }
+	logrus.Debug("ReplicaSetCollector created successfully")
+	return &ReplicaSetCollector{
 		clientset: clientset,
 		config:    cfg,
 	}
-	logrus.Debug("ReplicaSetCollector created successfully")
-	return collector
 }
 
 // CollectMetrics collects metrics for all ReplicaSets in the cluster
@@ -71,12 +84,33 @@ func (rsc *ReplicaSetCollector) parseReplicaSetMetrics(rs appsv1.ReplicaSet) mod
 	if rs.Labels == nil {
 		rs.Labels = make(map[string]string)
 	}
+	if rs.Annotations == nil {
+		rs.Annotations = make(map[string]string)
+	}
+
+	conditions := make([]models.RSCondition, 0, len(rs.Status.Conditions))
+	for _, condition := range rs.Status.Conditions {
+		conditions = append(conditions, models.RSCondition{
+			Type:               string(condition.Type),
+			Status:             string(condition.Status),
+			LastTransitionTime: &condition.LastTransitionTime.Time,
+			Reason:             condition.Reason,
+			Message:            condition.Message,
+		})
+	}
+
 	return models.ReplicaSetMetrics{
-		Name:              rs.Name,
-		Namespace:         rs.Namespace,
-		Replicas:          *rs.Spec.Replicas,
-		ReadyReplicas:     rs.Status.ReadyReplicas,
-		AvailableReplicas: rs.Status.AvailableReplicas,
-		Labels:            rs.Labels,
+		Name:                 rs.Name,
+		Namespace:            rs.Namespace,
+		Replicas:             *rs.Spec.Replicas,
+		ReadyReplicas:        rs.Status.ReadyReplicas,
+		AvailableReplicas:    rs.Status.AvailableReplicas,
+		CurrentReplicas:      rs.Status.Replicas,
+		FullyLabeledReplicas: rs.Status.FullyLabeledReplicas,
+		ObservedGeneration:   rs.Status.ObservedGeneration,
+		Conditions:           conditions,
+		Labels:               rs.Labels,
+		Annotations:          rs.Annotations,
+		CreationTimestamp:    &rs.CreationTimestamp.Time,
 	}
 }
diff --git a/pkg/collectors/replicationController..go b/pkg/collectors/replicationController..go
index 2ce7899..6e247f5 100644
--- a/pkg/collectors/replicationController..go
+++ b/pkg/collectors/replicationController..go
@@ -7,6 +7,8 @@
 // the Business Source License, use of this software will be governed
 // by the Apache License, Version 2.0, included in the file
 // licenses/APL.txt.
+
+// Package collectors hosts the collection functions
 package collectors
 
 import (
@@ -22,21 +24,35 @@ import (
 	"github.com/vegacloud/kubernetes/metricsagent/pkg/models"
 )
 
+// ReplicationControllerCollector collects metrics from Kubernetes replication controllers.
 type ReplicationControllerCollector struct {
 	clientset *kubernetes.Clientset
 	config    *config.Config
 }
 
+// NewReplicationControllerCollector creates a new ReplicationControllerCollector.
 func NewReplicationControllerCollector(clientset *kubernetes.Clientset,
 	cfg *config.Config) *ReplicationControllerCollector {
-	collector := &ReplicationControllerCollector{
+	// logrus.Debug("Starting ReplicationControllerCollector")
+	// if token, err := os.ReadFile("/var/run/secrets/kubernetes.io/serviceaccount/token"); err == nil {
+	// 	clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport = &http.Transport{
+	// 		TLSClientConfig: &tls.Config{
+	// 			InsecureSkipVerify: cfg.VegaInsecure,
+	// 		},
+	// 	}
+	// 	clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport = transport.NewBearerAuthRoundTripper(
+	// 		string(token),
+	// 		clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport,
+	// 	)
+	// }
+	logrus.Debug("ReplicationControllerCollector created successfully")
+	return &ReplicationControllerCollector{
 		clientset: clientset,
 		config:    cfg,
 	}
-	logrus.Debug("ReplicationControllerCollector created successfully")
-	return collector
 }
 
+// CollectMetrics collects metrics from Kubernetes replication controllers.
 func (rcc *ReplicationControllerCollector) CollectMetrics(ctx context.Context) (interface{}, error) {
 	metrics, err := rcc.CollectReplicationControllerMetrics(ctx)
 	if err != nil {
@@ -46,6 +62,7 @@ func (rcc *ReplicationControllerCollector) CollectMetrics(ctx context.Context) (
 	return metrics, nil
 }
 
+// CollectReplicationControllerMetrics collects metrics from Kubernetes replication controllers.
 func (rcc *ReplicationControllerCollector) CollectReplicationControllerMetrics(
 	ctx context.Context) ([]models.ReplicationControllerMetrics, error) {
 	rcs, err := rcc.clientset.CoreV1().ReplicationControllers("").List(ctx, metav1.ListOptions{})
@@ -68,13 +85,33 @@ func (rcc *ReplicationControllerCollector) parseReplicationControllerMetrics(
 	if rc.Labels == nil {
 		rc.Labels = make(map[string]string)
 	}
+	if rc.Annotations == nil {
+		rc.Annotations = make(map[string]string)
+	}
+
+	conditions := make([]models.RCCondition, 0, len(rc.Status.Conditions))
+	for _, condition := range rc.Status.Conditions {
+		conditions = append(conditions, models.RCCondition{
+			Type:               string(condition.Type),
+			Status:             string(condition.Status),
+			LastTransitionTime: &condition.LastTransitionTime.Time,
+			Reason:             condition.Reason,
+			Message:            condition.Message,
+		})
+	}
+
 	metrics := models.ReplicationControllerMetrics{
-		Name:              rc.Name,
-		Namespace:         rc.Namespace,
-		Replicas:          rc.Status.Replicas,
-		ReadyReplicas:     rc.Status.ReadyReplicas,
-		AvailableReplicas: rc.Status.AvailableReplicas,
-		Labels:            rc.Labels,
+		Name:                 rc.Name,
+		Namespace:            rc.Namespace,
+		Replicas:             rc.Status.Replicas,
+		ReadyReplicas:        rc.Status.ReadyReplicas,
+		AvailableReplicas:    rc.Status.AvailableReplicas,
+		Labels:               rc.Labels,
+		ObservedGeneration:   rc.Status.ObservedGeneration,
+		FullyLabeledReplicas: rc.Status.FullyLabeledReplicas,
+		Conditions:           conditions,
+		Annotations:          rc.Annotations,
+		CreationTimestamp:    &rc.CreationTimestamp.Time,
 	}
 
 	logrus.Debugf("Parsed replication controller metrics for %s/%s", rc.Namespace, rc.Name)
diff --git a/pkg/collectors/storageclass.go b/pkg/collectors/storageclass.go
new file mode 100644
index 0000000..596b3c1
--- /dev/null
+++ b/pkg/collectors/storageclass.go
@@ -0,0 +1,242 @@
+// Copyright 2024 Vega Cloud, Inc.
+//
+// Use of this software is governed by the Business Source License
+// included in the file licenses/BSL.txt.
+//
+// As of the Change Date specified in that file, in accordance with
+// the Business Source License, use of this software will be governed
+// by the Apache License, Version 2.0, included in the file
+// licenses/APL.txt.
+
+package collectors
+
+import (
+	"context"
+	"fmt"
+	"strings"
+
+	"github.com/sirupsen/logrus"
+	v1 "k8s.io/api/core/v1"
+	storagev1 "k8s.io/api/storage/v1"
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	"k8s.io/client-go/kubernetes"
+
+	"github.com/vegacloud/kubernetes/metricsagent/pkg/config"
+	"github.com/vegacloud/kubernetes/metricsagent/pkg/models"
+	"github.com/vegacloud/kubernetes/metricsagent/pkg/utils"
+)
+
+// StorageClassCollector collects metrics from Kubernetes storage classes
+type StorageClassCollector struct {
+	clientset *kubernetes.Clientset
+	config    *config.Config
+}
+
+// NewStorageClassCollector creates a new StorageClassCollector
+func NewStorageClassCollector(clientset *kubernetes.Clientset, cfg *config.Config) *StorageClassCollector {
+	// logrus.Debug("Starting StorageClassCollector")
+	// if token, err := os.ReadFile("/var/run/secrets/kubernetes.io/serviceaccount/token"); err == nil {
+	// 	clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport = &http.Transport{
+	// 		TLSClientConfig: &tls.Config{
+	// 			InsecureSkipVerify: cfg.VegaInsecure,
+	// 		},
+	// 	}
+	// 	clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport = transport.NewBearerAuthRoundTripper(
+	// 		string(token),
+	// 		clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport,
+	// 	)
+	// }
+	logrus.Debug("StorageClassCollector created successfully")
+	return &StorageClassCollector{
+		clientset: clientset,
+		config:    cfg,
+	}
+
+}
+
+// CollectMetrics collects metrics from Kubernetes storage classes
+func (sc *StorageClassCollector) CollectMetrics(ctx context.Context) (interface{}, error) {
+	storageClasses, err := sc.clientset.StorageV1().StorageClasses().List(ctx, metav1.ListOptions{})
+	if err != nil {
+		return nil, fmt.Errorf("failed to list storage classes: %w", err)
+	}
+
+	metrics := make([]models.StorageClassMetrics, 0, len(storageClasses.Items))
+	for _, storageClass := range storageClasses.Items {
+		metrics = append(metrics, sc.parseStorageClassMetrics(ctx, storageClass))
+	}
+
+	logrus.Debugf("Successfully collected metrics for %d storage classes", len(metrics))
+	return metrics, nil
+}
+
+func (sc *StorageClassCollector) parseStorageClassMetrics(ctx context.Context, storageClass storagev1.StorageClass) models.StorageClassMetrics {
+	if storageClass.Labels == nil {
+		storageClass.Labels = make(map[string]string)
+	}
+	if storageClass.Annotations == nil {
+		storageClass.Annotations = make(map[string]string)
+	}
+
+	metrics := models.StorageClassMetrics{
+		Name:                 storageClass.Name,
+		Provisioner:          storageClass.Provisioner,
+		ReclaimPolicy:        string(*storageClass.ReclaimPolicy),
+		VolumeBindingMode:    string(*storageClass.VolumeBindingMode),
+		AllowVolumeExpansion: storageClass.AllowVolumeExpansion != nil && *storageClass.AllowVolumeExpansion,
+		Labels:               storageClass.Labels,
+		Annotations:          storageClass.Annotations,
+		Parameters:           storageClass.Parameters,
+		MountOptions:         storageClass.MountOptions,
+		CreationTimestamp:    &storageClass.CreationTimestamp.Time,
+	}
+
+	// Check if this is the default storage class
+	metrics.IsDefault = false
+	for key, value := range storageClass.Annotations {
+		if (key == "storageclass.kubernetes.io/is-default-class" ||
+			key == "storageclass.beta.kubernetes.io/is-default-class") &&
+			value == "true" {
+			metrics.IsDefault = true
+			break
+		}
+	}
+
+	// Collect CSI driver metrics - continue even if this fails
+	if csiDriver, err := sc.collectCSIDriverMetrics(ctx, storageClass.Provisioner); err != nil {
+		logrus.Debugf("Storage class %s: CSI driver metrics collection skipped: %v", storageClass.Name, err)
+	} else {
+		metrics.CSIDriver = csiDriver
+	}
+
+	// Collect storage pool metrics - continue even if this fails
+	if storagePools, err := sc.collectStoragePoolMetrics(ctx, storageClass.Name); err != nil {
+		logrus.Debugf("Storage class %s: Storage pool metrics collection skipped: %v", storageClass.Name, err)
+	} else {
+		metrics.StoragePools = storagePools
+	}
+
+	// Collect capacity metrics - continue even if this fails
+	if capacityMetrics, err := sc.collectCapacityMetrics(ctx, storageClass.Name); err != nil {
+		logrus.Debugf("Storage class %s: Capacity metrics collection skipped: %v", storageClass.Name, err)
+	} else {
+		metrics.TotalCapacity = capacityMetrics.TotalCapacity
+		metrics.AllocatedCapacity = capacityMetrics.AllocatedCapacity
+		metrics.AvailableCapacity = capacityMetrics.AvailableCapacity
+		metrics.CapacityUtilization = capacityMetrics.CapacityUtilization
+		metrics.ProvisionedPVCs = capacityMetrics.ProvisionedPVCs
+		metrics.ProvisioningRate = capacityMetrics.ProvisioningRate
+	}
+
+	logrus.Debugf("Parsed metrics for storage class %s", storageClass.Name)
+	return metrics
+}
+
+func (sc *StorageClassCollector) collectCSIDriverMetrics(ctx context.Context, provisioner string) (*models.CSIDriverMetrics, error) {
+	csiDrivers, err := sc.clientset.StorageV1().CSIDrivers().List(ctx, metav1.ListOptions{})
+	if err != nil {
+		return nil, fmt.Errorf("failed to list CSI drivers: %w", err)
+	}
+
+	for _, driver := range csiDrivers.Items {
+		if driver.Name == provisioner {
+			metrics := &models.CSIDriverMetrics{
+				Name:               driver.Name,
+				Available:          true,
+				VolumeSnapshotting: driver.Spec.VolumeLifecycleModes != nil,
+				VolumeCloning:      driver.Spec.VolumeLifecycleModes != nil,
+				VolumeExpansion:    driver.Spec.RequiresRepublish != nil && *driver.Spec.RequiresRepublish,
+				NodePluginPods:     make(map[string]string),
+				ControllerPods:     make(map[string]string),
+			}
+
+			// Try to get CSI pods, but continue if we can't
+			if pods, err := sc.clientset.CoreV1().Pods("").List(ctx, metav1.ListOptions{
+				LabelSelector: fmt.Sprintf("app=%s", driver.Name),
+			}); err == nil {
+				for _, pod := range pods.Items {
+					if strings.Contains(pod.Name, "node") {
+						metrics.NodePluginPods[pod.Spec.NodeName] = string(pod.Status.Phase)
+					} else if strings.Contains(pod.Name, "controller") {
+						metrics.ControllerPods[pod.Namespace] = string(pod.Status.Phase)
+					}
+				}
+			}
+
+			return metrics, nil
+		}
+	}
+
+	// Return empty metrics if CSI driver not found
+	return &models.CSIDriverMetrics{
+		Name:      provisioner,
+		Available: false,
+	}, nil
+}
+
+func (sc *StorageClassCollector) collectStoragePoolMetrics(ctx context.Context, storageClassName string) ([]models.StoragePoolMetrics, error) {
+	pvs, err := sc.clientset.CoreV1().PersistentVolumes().List(ctx, metav1.ListOptions{})
+	if err != nil {
+		return []models.StoragePoolMetrics{}, fmt.Errorf("failed to list PVs: %w", err)
+	}
+
+	poolMap := make(map[string]*models.StoragePoolMetrics)
+	for _, pv := range pvs.Items {
+		if pv.Spec.StorageClassName == storageClassName {
+			poolName := pv.Labels["storage-pool"] // Adjust label key based on your CSI driver
+			if poolName == "" {
+				poolName = "default"
+			}
+
+			if pool, exists := poolMap[poolName]; exists {
+				pool.UsedCapacity += pv.Spec.Capacity.Storage().Value()
+				pool.VolumeCount++
+			} else {
+				poolMap[poolName] = &models.StoragePoolMetrics{
+					Name:         poolName,
+					Provider:     pv.Spec.CSI.Driver,
+					StorageClass: storageClassName,
+					UsedCapacity: pv.Spec.Capacity.Storage().Value(),
+					VolumeCount:  1,
+				}
+			}
+		}
+	}
+
+	pools := make([]models.StoragePoolMetrics, 0, len(poolMap))
+	for _, pool := range poolMap {
+		if pool.TotalCapacity > 0 {
+			pool.UtilizationPct = float64(pool.UsedCapacity) / float64(pool.TotalCapacity) * 100
+		}
+		pool.AvailableSpace = pool.TotalCapacity - pool.UsedCapacity
+		pools = append(pools, *pool)
+	}
+
+	return pools, nil
+}
+
+func (sc *StorageClassCollector) collectCapacityMetrics(ctx context.Context, storageClassName string) (*models.StorageClassMetrics, error) {
+	metrics := &models.StorageClassMetrics{}
+
+	// Get PVCs using this storage class
+	pvcs, err := sc.clientset.CoreV1().PersistentVolumeClaims("").List(ctx, metav1.ListOptions{})
+	if err != nil {
+		return nil, fmt.Errorf("failed to list PVCs: %w", err)
+	}
+
+	var totalRequested int64
+	provisionedCount := 0
+	for _, pvc := range pvcs.Items {
+		if pvc.Spec.StorageClassName != nil && *pvc.Spec.StorageClassName == storageClassName {
+			totalRequested += pvc.Spec.Resources.Requests.Storage().Value()
+			if pvc.Status.Phase == v1.ClaimBound {
+				provisionedCount++
+			}
+		}
+	}
+
+	metrics.AllocatedCapacity = totalRequested
+	metrics.ProvisionedPVCs = utils.SafeInt32Conversion(provisionedCount)
+
+	return metrics, nil
+}
diff --git a/pkg/collectors/workload.go b/pkg/collectors/workload.go
index 82d0a76..f13d286 100644
--- a/pkg/collectors/workload.go
+++ b/pkg/collectors/workload.go
@@ -8,11 +8,14 @@
 // by the Apache License, Version 2.0, included in the file
 // licenses/APL.txt.
 // File: pkg/collectors/workload.go
+
+// Package collectors hosts the collection functions
 package collectors
 
 import (
 	"context"
 	"fmt"
+	"time"
 
 	"github.com/sirupsen/logrus"
 	appsv1 "k8s.io/api/apps/v1"
@@ -22,24 +25,42 @@ import (
 
 	"github.com/vegacloud/kubernetes/metricsagent/pkg/config"
 	"github.com/vegacloud/kubernetes/metricsagent/pkg/models"
+	v1 "k8s.io/api/core/v1"
 )
 
+// WorkloadCollector collects metrics from Kubernetes workloads.
 type WorkloadCollector struct {
 	clientset *kubernetes.Clientset
 	config    *config.Config
 }
 
+// NewWorkloadCollector creates a new WorkloadCollector.
 func NewWorkloadCollector(clientset *kubernetes.Clientset, cfg *config.Config) *WorkloadCollector {
+	// logrus.Debug("Starting WorkloadCollector")
+	// if token, err := os.ReadFile("/var/run/secrets/kubernetes.io/serviceaccount/token"); err == nil {
+	// 	clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport = &http.Transport{
+	// 		TLSClientConfig: &tls.Config{
+	// 			InsecureSkipVerify: cfg.VegaInsecure,
+	// 		},
+	// 	}
+	// 	clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport = transport.NewBearerAuthRoundTripper(
+	// 		string(token),
+	// 		clientset.CoreV1().RESTClient().(*rest.RESTClient).Client.Transport,
+	// 	)
+	// }
+	logrus.Debug("WorkloadCollector created successfully")
 	return &WorkloadCollector{
 		clientset: clientset,
 		config:    cfg,
 	}
 }
 
+// CollectMetrics collects metrics from Kubernetes workloads.
 func (wc *WorkloadCollector) CollectMetrics(ctx context.Context) (interface{}, error) {
 	return wc.CollectWorkloadMetrics(ctx)
 }
 
+// CollectWorkloadMetrics collects metrics from Kubernetes workloads.
 func (wc *WorkloadCollector) CollectWorkloadMetrics(ctx context.Context) (*models.WorkloadMetrics, error) {
 	metrics := &models.WorkloadMetrics{}
 
@@ -75,6 +96,7 @@ func (wc *WorkloadCollector) CollectWorkloadMetrics(ctx context.Context) (*model
 	return metrics, nil
 }
 
+// collectDeploymentMetrics collects metrics from Kubernetes deployments.
 func (wc *WorkloadCollector) collectDeploymentMetrics(ctx context.Context) ([]models.DeploymentMetrics, error) {
 	deployments, err := wc.clientset.AppsV1().Deployments("").List(ctx, metav1.ListOptions{})
 	if err != nil {
@@ -95,14 +117,24 @@ func (wc *WorkloadCollector) parseDeploymentMetrics(d appsv1.Deployment) models.
 	if d.Labels == nil {
 		d.Labels = make(map[string]string)
 	}
+
+	conditions := make([]string, 0)
+	for _, condition := range d.Status.Conditions {
+		conditions = append(conditions, string(condition.Type))
+	}
+
 	return models.DeploymentMetrics{
-		Name:              d.Name,
-		Namespace:         d.Namespace,
-		Replicas:          *d.Spec.Replicas,
-		ReadyReplicas:     d.Status.ReadyReplicas,
-		UpdatedReplicas:   d.Status.UpdatedReplicas,
-		AvailableReplicas: d.Status.AvailableReplicas,
-		Labels:            d.Labels,
+		Name:               d.Name,
+		Namespace:          d.Namespace,
+		Replicas:           *d.Spec.Replicas,
+		ReadyReplicas:      d.Status.ReadyReplicas,
+		UpdatedReplicas:    d.Status.UpdatedReplicas,
+		AvailableReplicas:  d.Status.AvailableReplicas,
+		Labels:             d.Labels,
+		CollisionCount:     d.Status.CollisionCount,
+		Conditions:         conditions,
+		Generation:         d.Generation,
+		ObservedGeneration: d.Status.ObservedGeneration,
 	}
 }
 
@@ -125,14 +157,25 @@ func (wc *WorkloadCollector) parseStatefulSetMetrics(s appsv1.StatefulSet) model
 	if s.Labels == nil {
 		s.Labels = make(map[string]string)
 	}
+
+	conditions := make([]string, 0)
+	for _, condition := range s.Status.Conditions {
+		conditions = append(conditions, string(condition.Type))
+	}
+
 	return models.StatefulSetMetrics{
-		Name:            s.Name,
-		Namespace:       s.Namespace,
-		Replicas:        *s.Spec.Replicas,
-		ReadyReplicas:   s.Status.ReadyReplicas,
-		CurrentReplicas: s.Status.CurrentReplicas,
-		UpdatedReplicas: s.Status.UpdatedReplicas,
-		Labels:          s.Labels,
+		Name:               s.Name,
+		Namespace:          s.Namespace,
+		Replicas:           *s.Spec.Replicas,
+		ReadyReplicas:      s.Status.ReadyReplicas,
+		CurrentReplicas:    s.Status.CurrentReplicas,
+		UpdatedReplicas:    s.Status.UpdatedReplicas,
+		AvailableReplicas:  s.Status.AvailableReplicas,
+		Labels:             s.Labels,
+		CollisionCount:     s.Status.CollisionCount,
+		Conditions:         conditions,
+		Generation:         s.Generation,
+		ObservedGeneration: s.Status.ObservedGeneration,
 	}
 }
 
@@ -155,6 +198,18 @@ func (wc *WorkloadCollector) parseDaemonSetMetrics(d appsv1.DaemonSet) models.Da
 	if d.Labels == nil {
 		d.Labels = make(map[string]string)
 	}
+
+	conditions := make([]models.DaemonSetCondition, 0)
+	for _, condition := range d.Status.Conditions {
+		conditions = append(conditions, models.DaemonSetCondition{
+			Type:               string(condition.Type),
+			Status:             string(condition.Status),
+			LastTransitionTime: &condition.LastTransitionTime.Time,
+			Reason:             condition.Reason,
+			Message:            condition.Message,
+		})
+	}
+
 	return models.DaemonSetMetrics{
 		Name:                   d.Name,
 		Namespace:              d.Namespace,
@@ -163,7 +218,12 @@ func (wc *WorkloadCollector) parseDaemonSetMetrics(d appsv1.DaemonSet) models.Da
 		NumberReady:            d.Status.NumberReady,
 		UpdatedNumberScheduled: d.Status.UpdatedNumberScheduled,
 		NumberAvailable:        d.Status.NumberAvailable,
+		NumberUnavailable:      d.Status.NumberUnavailable,
+		NumberMisscheduled:     d.Status.NumberMisscheduled,
 		Labels:                 d.Labels,
+		Generation:             d.Generation,
+		ObservedGeneration:     d.Status.ObservedGeneration,
+		Conditions:             conditions,
 	}
 }
 
@@ -186,33 +246,85 @@ func (wc *WorkloadCollector) parseJobMetrics(j batchv1.Job) models.JobMetrics {
 	if j.Labels == nil {
 		j.Labels = make(map[string]string)
 	}
+
+	conditions := make([]models.JobCondition, 0)
+	for _, condition := range j.Status.Conditions {
+		conditions = append(conditions, models.JobCondition{
+			Type:               string(condition.Type),
+			Status:             string(condition.Status),
+			LastProbeTime:      &condition.LastProbeTime.Time,
+			LastTransitionTime: &condition.LastTransitionTime.Time,
+			Reason:             condition.Reason,
+			Message:            condition.Message,
+		})
+	}
+
 	metrics := models.JobMetrics{
-		Name:      j.Name,
-		Namespace: j.Namespace,
-		Labels:    j.Labels,
-		Completions: func() *int32 {
-			if j.Spec.Completions != nil {
-				return j.Spec.Completions
-			}
-			return nil
-		}(),
-		Parallelism: func() *int32 {
-			if j.Spec.Parallelism != nil {
-				return j.Spec.Parallelism
-			}
-			return nil
-		}(),
-		Active:    j.Status.Active,
-		Succeeded: j.Status.Succeeded,
-		Failed:    j.Status.Failed,
+		Name:             j.Name,
+		Namespace:        j.Namespace,
+		Labels:           j.Labels,
+		Active:           j.Status.Active,
+		Succeeded:        j.Status.Succeeded,
+		Failed:           j.Status.Failed,
+		Status:           getJobStatus(j.Status),
+		CompletedIndexes: j.Status.CompletedIndexes,
+		Conditions:       conditions,
+		Generation:       j.Generation,
 	}
 
+	// Add existing time-related fields
 	if j.Status.StartTime != nil {
-		metrics.StartTime = j.Status.StartTime.Time
+		metrics.StartTime = &j.Status.StartTime.Time
 	}
-
 	if j.Status.CompletionTime != nil {
-		metrics.CompletionTime = j.Status.CompletionTime.Time
+		metrics.CompletionTime = &j.Status.CompletionTime.Time
+	}
+	if metrics.StartTime != nil {
+		endTime := time.Now()
+		if metrics.CompletionTime != nil {
+			endTime = *metrics.CompletionTime
+		}
+		duration := endTime.Sub(*metrics.StartTime)
+		metrics.Duration = &duration
+	}
+
+	metrics.ResourceMetrics = wc.getJobResourceMetrics(j)
+	return metrics
+}
+
+// Helper function to get job status
+func getJobStatus(status batchv1.JobStatus) string {
+	switch {
+	case status.Succeeded > 0:
+		return "Succeeded"
+	case status.Failed > 0:
+		return "Failed"
+	case status.Active > 0:
+		return "Active"
+	default:
+		return "Pending"
+	}
+}
+
+// Helper function to get resource metrics for a job
+func (wc *WorkloadCollector) getJobResourceMetrics(job batchv1.Job) models.ResourceMetrics {
+	metrics := models.ResourceMetrics{}
+
+	if job.Spec.Template.Spec.Containers == nil {
+		return metrics
+	}
+
+	for _, container := range job.Spec.Template.Spec.Containers {
+		if container.Resources.Requests != nil {
+			metrics.CPU += container.Resources.Requests.Cpu().MilliValue()
+			metrics.Memory += container.Resources.Requests.Memory().Value()
+			if storage := container.Resources.Requests.Storage(); storage != nil {
+				metrics.Storage += storage.Value()
+			}
+			if ephemeral, ok := container.Resources.Requests[v1.ResourceEphemeralStorage]; ok {
+				metrics.EphemeralStorage += ephemeral.Value()
+			}
+		}
 	}
 
 	return metrics
diff --git a/pkg/config/config.go b/pkg/config/config.go
index d43e632..f763239 100644
--- a/pkg/config/config.go
+++ b/pkg/config/config.go
@@ -7,6 +7,7 @@
 // the Business Source License, use of this software will be governed
 // by the Apache License, Version 2.0, included in the file
 // licenses/APL.txt.
+
 // Package config provides configuration parameters for the agent
 package config
 
diff --git a/pkg/config/config_loader.go b/pkg/config/config_loader.go
index 70cb7dd..de9f7dd 100644
--- a/pkg/config/config_loader.go
+++ b/pkg/config/config_loader.go
@@ -7,6 +7,7 @@
 // the Business Source License, use of this software will be governed
 // by the Apache License, Version 2.0, included in the file
 // licenses/APL.txt.
+
 // Package config handles loading configuration using Viper
 package config
 
diff --git a/pkg/models/metrics.go b/pkg/models/metrics.go
index 947ea97..f32d2d9 100644
--- a/pkg/models/metrics.go
+++ b/pkg/models/metrics.go
@@ -9,6 +9,8 @@
 // licenses/APL.txt.
 // Package models provides the data models for the metrics API
 // File: pkg/models/metrics.go
+
+// Package models provides the data models for the metrics API
 package models
 
 import (
@@ -17,40 +19,92 @@ import (
 
 // EnhancedNodeMetrics represents detailed metrics for a single node
 type EnhancedNodeMetrics struct {
-	Name        string            `json:"name"`
-	Capacity    ResourceMetrics   `json:"capacity"`
-	Allocatable ResourceMetrics   `json:"allocatable"`
-	Usage       ResourceMetrics   `json:"usage"`
-	Conditions  NodeConditions    `json:"conditions"`
-	Labels      map[string]string `json:"labels"`
+	Name              string                      `json:"name"`
+	Capacity          ResourceMetrics             `json:"capacity"`
+	Allocatable       ResourceMetrics             `json:"allocatable"`
+	Usage             ResourceMetrics             `json:"usage"`
+	Conditions        NodeConditions              `json:"conditions"`
+	Labels            map[string]string           `json:"labels"`
+	Annotations       map[string]string           `json:"annotations"`
+	DetailedMetrics   NodeDetailedMetrics         `json:"detailedMetrics"`
+	HardwareTopology  *HardwareTopology           `json:"hardwareTopology,omitempty"`
+	PowerMetrics      *PowerMetrics               `json:"powerMetrics,omitempty"`
+	Taints            []NodeTaint                 `json:"taints,omitempty"`
+	Lease             *NodeLease                  `json:"lease,omitempty"`
+	ExtendedResources map[string]ExtendedResource `json:"extendedResources,omitempty"`
+	NodeMetrics       NodeMetrics                 `json:"nodeMetrics"`
+	CPU               CPUMetrics                  `json:"cpu,omitempty"`
+	Memory            MemoryMetrics               `json:"memory,omitempty"`
+	Network           NetworkMetrics              `json:"network,omitempty"`
+	Disk              DiskMetrics                 `json:"disk,omitempty"`
+	Filesystem        FSMetrics                   `json:"filesystem,omitempty"`
+	VolumeHealth      []VolumeHealthMetrics       `json:"volumeHealth,omitempty"`
+	VolumeAttachments []VolumeAttachmentMetrics   `json:"volumeAttachments,omitempty"`
+	NodeInfo          NodeInfo                    `json:"nodeInfo"` // Add this line
 }
 
 // EnhancedPodMetrics represents detailed metrics for a single pod
 type EnhancedPodMetrics struct {
-	PodMetrics    PodMetrics         `json:"podMetrics"`
-	QoSClass      string             `json:"qosClass"`
-	Conditions    PodConditions      `json:"conditions"`
-	Requests      ResourceMetrics    `json:"requests"`
-	Limits        ResourceMetrics    `json:"limits"`
-	Containers    []ContainerMetrics `json:"containers"`
-	TotalRestarts int32              `json:"totalRestarts"`
+	PodMetrics          PodMetrics                  `json:"podMetrics"`
+	QoSClass            string                      `json:"qosClass"`
+	Conditions          PodConditions               `json:"conditions"`
+	Requests            ResourceMetrics             `json:"requests"`
+	Limits              ResourceMetrics             `json:"limits"`
+	Containers          []ContainerMetrics          `json:"containers"`
+	TotalRestarts       int32                       `json:"totalRestarts"`
+	StartTime           *time.Time                  `json:"startTime,omitempty"`
+	CompletionTime      *time.Time                  `json:"completionTime,omitempty"`
+	Priority            *int32                      `json:"priority,omitempty"`
+	PriorityClassName   string                      `json:"priorityClassName,omitempty"`
+	NodeName            string                      `json:"nodeName"`
+	HostIP              string                      `json:"hostIP"`
+	PodIPs              []string                    `json:"podIPs"`
+	NominatedNodeName   string                      `json:"nominatedNodeName,omitempty"`
+	ReadinessGates      []PodReadinessGate          `json:"readinessGates,omitempty"`
+	Annotations         map[string]string           `json:"annotations"`
+	VolumeMounts        []VolumeMountMetrics        `json:"volumeMounts"`
+	ImagePullPolicy     string                      `json:"imagePullPolicy"`
+	ServiceAccountName  string                      `json:"serviceAccountName"`
+	DisruptionBudget    *PodDisruptionBudgetMetrics `json:"disruptionBudget,omitempty"`
+	TopologySpread      []TopologySpreadConstraint  `json:"topologySpread,omitempty"`
+	Overhead            *PodOverheadMetrics         `json:"overhead,omitempty"`
+	SchedulingGates     []PodSchedulingGate         `json:"schedulingGates,omitempty"`
+	SecurityContext     *SecurityContextMetrics     `json:"securityContext,omitempty"`
+	Affinity            *AffinityMetrics            `json:"affinity,omitempty"`
+	InitContainers      []InitContainerMetrics      `json:"initContainers,omitempty"`
+	EphemeralContainers []EphemeralContainerMetrics `json:"ephemeralContainers,omitempty"`
+	QoSDetails          *QoSMetrics                 `json:"qosDetails,omitempty"`
 }
 
 // NodeMetrics represents the metrics for a single node
 type NodeMetrics struct {
-	Name        string          `json:"name"`
-	Capacity    ResourceMetrics `json:"capacity"`
-	Allocatable ResourceMetrics `json:"allocatable"`
-	Usage       ResourceMetrics `json:"usage"`
-	Conditions  NodeConditions  `json:"conditions"`
+	Name              string                    `json:"name"`
+	Capacity          ResourceMetrics           `json:"capacity"`
+	Allocatable       ResourceMetrics           `json:"allocatable"`
+	Usage             ResourceMetrics           `json:"usage"`
+	CPU               CPUMetrics                `json:"cpu,omitempty"`
+	Memory            MemoryMetrics             `json:"memory,omitempty"`
+	Network           NetworkMetrics            `json:"network,omitempty"`
+	Disk              DiskMetrics               `json:"disk,omitempty"`
+	Filesystem        FSMetrics                 `json:"filesystem,omitempty"`
+	VolumeHealth      []VolumeHealthMetrics     `json:"volumeHealth,omitempty"`
+	VolumeAttachments []VolumeAttachmentMetrics `json:"volumeAttachments,omitempty"`
 }
 
-// ResourceMetrics represents resource usage or capacity
+// ResourceMetrics represents resource usage or capacity based on kubelet /metrics/resource
 type ResourceMetrics struct {
-	CPU     int64 `json:"cpu"`    // in millicores
-	Memory  int64 `json:"memory"` // in bytes
-	Pods    int64 `json:"pods"`
-	Storage int64 `json:"storage"` // in bytes
+	// CPU usage in millicores (m)
+	CPU int64 `json:"cpu"`
+	// Memory usage in bytes
+	Memory int64 `json:"memory"`
+	// Storage usage in bytes
+	Storage int64 `json:"storage,omitempty"`
+	// Ephemeral storage usage in bytes
+	EphemeralStorage int64 `json:"ephemeralStorage,omitempty"`
+	// Number of pods
+	Pods int64 `json:"pods,omitempty"`
+	// GPU devices if available
+	GPUDevices []GPUMetrics `json:"gpuDevices,omitempty"`
 }
 
 // NodeConditions represents the conditions of a node
@@ -76,17 +130,6 @@ type PodMetrics struct {
 	Labels        map[string]string  `json:"labels"`
 }
 
-// ContainerMetrics represents the metrics for a single container
-type ContainerMetrics struct {
-	Name           string `json:"name"`
-	RestartCount   int32  `json:"restartCount"`
-	Ready          bool   `json:"ready"`
-	State          string `json:"state"`
-	LastTerminated string `json:"lastTerminated,omitempty"`
-	CPU            int64  `json:"cpu"`    // in millicores
-	Memory         int64  `json:"memory"` // in bytes
-}
-
 // PodConditions represents the conditions of a pod
 type PodConditions struct {
 	PodScheduled    bool `json:"podScheduled"`
@@ -112,46 +155,113 @@ type ClusterMetrics struct {
 }
 
 // PersistentVolumeMetrics represents metrics for persistent volumes
-type PersistentVolumeMetrics struct {
-	PVs  []PVMetric  `json:"pvs"`
-	PVCs []PVCMetric `json:"pvcs"`
-}
+// Deprecated: Use separate PVMetric and PVCMetric instead
+// type PersistentVolumeMetrics struct {
+//     PVs  []PVMetric  `json:"pvs"`
+//     PVCs []PVCMetric `json:"pvcs"`
+// }
 
 // PVMetric represents metrics for a single persistent volume
 type PVMetric struct {
-	Name         string            `json:"name"`
-	Capacity     int64             `json:"capacity"` // in bytes
-	Phase        string            `json:"phase"`
-	StorageClass string            `json:"storageClass"`
-	BoundPVC     string            `json:"boundPVC,omitempty"`
-	Labels       map[string]string `json:"labels"`
+	Name               string                  `json:"name"`
+	Capacity           int64                   `json:"capacity"` // in bytes
+	Phase              string                  `json:"phase"`
+	StorageClass       string                  `json:"storageClass"`
+	BoundPVC           string                  `json:"boundPVC,omitempty"`
+	Labels             map[string]string       `json:"labels"`
+	AccessModes        []string                `json:"accessModes"`
+	ReclaimPolicy      string                  `json:"reclaimPolicy"`
+	VolumeMode         string                  `json:"volumeMode"`
+	StorageProvisioner string                  `json:"storageProvisioner,omitempty"`
+	MountPoint         string                  `json:"mountPoint,omitempty"`
+	Status             PVStatus                `json:"status"`
+	MountOptions       []string                `json:"mountOptions,omitempty"`
+	VolumeBindingMode  string                  `json:"volumeBindingMode,omitempty"`
+	Snapshots          []VolumeSnapshotMetrics `json:"snapshots,omitempty"`
 }
 
 // PVCMetric represents metrics for a single persistent volume claim
 type PVCMetric struct {
-	Name         string            `json:"name"`
-	Namespace    string            `json:"namespace"`
-	Phase        string            `json:"phase"`
-	Capacity     int64             `json:"capacity"` // in bytes
-	StorageClass string            `json:"storageClass"`
-	BoundPV      string            `json:"boundPV,omitempty"`
-	Labels       map[string]string `json:"labels"`
+	Name               string                  `json:"name"`
+	Namespace          string                  `json:"namespace"`
+	Phase              string                  `json:"phase"`
+	Capacity           int64                   `json:"capacity"`         // in bytes
+	RequestedStorage   int64                   `json:"requestedStorage"` // in bytes
+	StorageClass       string                  `json:"storageClass"`
+	BoundPV            string                  `json:"boundPV,omitempty"`
+	Labels             map[string]string       `json:"labels"`
+	AccessModes        []string                `json:"accessModes"`
+	VolumeMode         string                  `json:"volumeMode"`
+	Status             PVCStatus               `json:"status"`
+	VolumeName         string                  `json:"volumeName,omitempty"`
+	StorageProvisioner string                  `json:"storageProvisioner,omitempty"`
+	MountOptions       []string                `json:"mountOptions,omitempty"`
+	VolumeBindingMode  string                  `json:"volumeBindingMode,omitempty"`
+	Expansion          *VolumeExpansionMetrics `json:"expansion,omitempty"`
+}
+
+// PVStatus represents the status of a persistent volume
+type PVStatus struct {
+	Phase               string     `json:"phase"`
+	Message             string     `json:"message,omitempty"`
+	Reason              string     `json:"reason,omitempty"`
+	LastPhaseTransition *time.Time `json:"lastPhaseTransition,omitempty"`
+}
+
+// PVCStatus represents the status of a persistent volume claim
+type PVCStatus struct {
+	Phase               string         `json:"phase"`
+	Message             string         `json:"message,omitempty"`
+	Reason              string         `json:"reason,omitempty"`
+	LastPhaseTransition *time.Time     `json:"lastPhaseTransition,omitempty"`
+	Conditions          []PVCCondition `json:"conditions"`
+}
+
+// PVCCondition represents a condition of a PVC
+type PVCCondition struct {
+	Type               string     `json:"type"`
+	Status             string     `json:"status"`
+	Reason             string     `json:"reason,omitempty"`
+	Message            string     `json:"message,omitempty"`
+	LastTransitionTime *time.Time `json:"lastTransitionTime,omitempty"`
 }
 
 // NamespaceMetrics represents metrics for a single namespace
 type NamespaceMetrics struct {
-	Name           string                 `json:"name"`
-	Status         string                 `json:"status"`
-	ResourceQuotas []ResourceQuotaMetrics `json:"resourceQuotas"`
-	LimitRanges    []LimitRangeMetrics    `json:"limitRanges"`
-	Usage          ResourceUsage          `json:"usage"`
-	Labels         map[string]string      `json:"labels"`
+	Name              string                 `json:"name"`
+	Status            string                 `json:"status"`
+	Phase             string                 `json:"phase"`
+	CreationTimestamp time.Time              `json:"creationTimestamp"`
+	DeletionTimestamp *time.Time             `json:"deletionTimestamp,omitempty"`
+	Finalizers        []string               `json:"finalizers"`
+	Conditions        []NamespaceCondition   `json:"conditions"`
+	ResourceQuotas    []ResourceQuotaMetrics `json:"resourceQuotas"`
+	LimitRanges       []LimitRangeMetrics    `json:"limitRanges"`
+	Usage             ResourceUsage          `json:"usage"`
+	Labels            map[string]string      `json:"labels"`
+	Annotations       map[string]string      `json:"annotations"`
+}
+
+// NamespaceCondition represents the current condition of a namespace
+type NamespaceCondition struct {
+	Type               string     `json:"type"`
+	Status             string     `json:"status"`
+	LastTransitionTime *time.Time `json:"lastTransitionTime,omitempty"`
+	Reason             string     `json:"reason,omitempty"`
+	Message            string     `json:"message,omitempty"`
 }
 
 // ResourceQuotaMetrics represents metrics for a resource quota
 type ResourceQuotaMetrics struct {
 	Name      string           `json:"name"`
 	Resources []ResourceMetric `json:"resources"`
+
+	// New detailed quota fields
+	Scopes          []QuotaScopeMetrics         `json:"scopes,omitempty"`
+	PriorityQuotas  []PriorityClassQuotaMetrics `json:"priorityQuotas,omitempty"`
+	StatusHistory   []QuotaStatusHistory        `json:"statusHistory,omitempty"`
+	LastUpdateTime  time.Time                   `json:"lastUpdateTime,omitempty"`
+	EnforcementMode string                      `json:"enforcementMode,omitempty"`
 }
 
 // ResourceMetric represents a single resource metric
@@ -192,72 +302,109 @@ type WorkloadMetrics struct {
 	Jobs         []JobMetrics         `json:"jobs"`
 }
 
-// DeploymentMetrics represents metrics for a deployment
+// DeploymentMetrics represents metrics for a Kubernetes Deployment
 type DeploymentMetrics struct {
-	Name              string            `json:"name"`
-	Namespace         string            `json:"namespace"`
-	Replicas          int32             `json:"replicas"`
-	ReadyReplicas     int32             `json:"readyReplicas"`
-	UpdatedReplicas   int32             `json:"updatedReplicas"`
-	AvailableReplicas int32             `json:"availableReplicas"`
-	Labels            map[string]string `json:"labels"`
+	Name               string            `json:"name"`
+	Namespace          string            `json:"namespace"`
+	Labels             map[string]string `json:"labels"`
+	Replicas           int32             `json:"replicas"`
+	ReadyReplicas      int32             `json:"readyReplicas"`
+	UpdatedReplicas    int32             `json:"updatedReplicas"`
+	AvailableReplicas  int32             `json:"availableReplicas"`
+	CollisionCount     *int32            `json:"collisionCount"`
+	Conditions         []string          `json:"conditions"`
+	Generation         int64             `json:"generation"`
+	ObservedGeneration int64             `json:"observedGeneration"`
 }
 
-// StatefulSetMetrics represents metrics for a stateful set
+// StatefulSetMetrics represents metrics for a Kubernetes StatefulSet
 type StatefulSetMetrics struct {
-	Name            string            `json:"name"`
-	Namespace       string            `json:"namespace"`
-	Replicas        int32             `json:"replicas"`
-	ReadyReplicas   int32             `json:"readyReplicas"`
-	CurrentReplicas int32             `json:"currentReplicas"`
-	UpdatedReplicas int32             `json:"updatedReplicas"`
-	Labels          map[string]string `json:"labels"`
+	Name               string            `json:"name"`
+	Namespace          string            `json:"namespace"`
+	Labels             map[string]string `json:"labels"`
+	Replicas           int32             `json:"replicas"`
+	ReadyReplicas      int32             `json:"readyReplicas"`
+	CurrentReplicas    int32             `json:"currentReplicas"`
+	UpdatedReplicas    int32             `json:"updatedReplicas"`
+	AvailableReplicas  int32             `json:"availableReplicas"`
+	CollisionCount     *int32            `json:"collisionCount"`
+	Conditions         []string          `json:"conditions"`
+	Generation         int64             `json:"generation"`
+	ObservedGeneration int64             `json:"observedGeneration"`
 }
 
-// DaemonSetMetrics represents metrics for a daemon set
+// DaemonSetMetrics represents metrics for a Kubernetes DaemonSet
 type DaemonSetMetrics struct {
-	Name                   string            `json:"name"`
-	Namespace              string            `json:"namespace"`
-	DesiredNumberScheduled int32             `json:"desiredNumberScheduled"`
-	CurrentNumberScheduled int32             `json:"currentNumberScheduled"`
-	NumberReady            int32             `json:"numberReady"`
-	UpdatedNumberScheduled int32             `json:"updatedNumberScheduled"`
-	NumberAvailable        int32             `json:"numberAvailable"`
-	Labels                 map[string]string `json:"labels"`
+	Name                   string               `json:"name"`
+	Namespace              string               `json:"namespace"`
+	Labels                 map[string]string    `json:"labels"`
+	DesiredNumberScheduled int32                `json:"desiredNumberScheduled"`
+	CurrentNumberScheduled int32                `json:"currentNumberScheduled"`
+	NumberReady            int32                `json:"numberReady"`
+	UpdatedNumberScheduled int32                `json:"updatedNumberScheduled"`
+	NumberAvailable        int32                `json:"numberAvailable"`
+	NumberUnavailable      int32                `json:"numberUnavailable"`
+	NumberMisscheduled     int32                `json:"numberMisscheduled"`
+	Generation             int64                `json:"generation"`
+	ObservedGeneration     int64                `json:"observedGeneration"`
+	Conditions             []DaemonSetCondition `json:"conditions"`
+	Status                 DaemonSetStatus      `json:"status"`
+	CreationTimestamp      *time.Time           `json:"creationTimestamp"`
+	CollisionCount         *int32               `json:"collisionCount"`
+	Annotations            map[string]string    `json:"annotations"`
 }
 
-// JobMetrics represents metrics for a job
+// JobMetrics represents metrics for a Kubernetes Job
 type JobMetrics struct {
-	Name           string            `json:"name"`
-	Namespace      string            `json:"namespace"`
-	Completions    *int32            `json:"completions,omitempty"`
-	Parallelism    *int32            `json:"parallelism,omitempty"`
-	Active         int32             `json:"active"`
-	Succeeded      int32             `json:"succeeded"`
-	Failed         int32             `json:"failed"`
-	StartTime      time.Time         `json:"startTime,omitempty"`
-	CompletionTime time.Time         `json:"completionTime,omitempty"`
-	Labels         map[string]string `json:"labels"`
+	Name               string            `json:"name"`
+	Namespace          string            `json:"namespace"`
+	Labels             map[string]string `json:"labels"`
+	Active             int32             `json:"active"`
+	Succeeded          int32             `json:"succeeded"`
+	Failed             int32             `json:"failed"`
+	Status             string            `json:"status"`
+	StartTime          *time.Time        `json:"startTime,omitempty"`
+	CompletionTime     *time.Time        `json:"completionTime,omitempty"`
+	Duration           *time.Duration    `json:"duration,omitempty"`
+	ResourceMetrics    ResourceMetrics   `json:"resourceMetrics"`
+	CompletedIndexes   string            `json:"completedIndexes,omitempty"`
+	Conditions         []JobCondition    `json:"conditions"`
+	Generation         int64             `json:"generation"`
+	ObservedGeneration int64             `json:"observedGeneration"`
+	Suspended          bool              `json:"suspended"`
+	CreationTime       *time.Time        `json:"creationTime"`
+	Parallelism        *int32            `json:"parallelism,omitempty"`
+	Completions        *int32            `json:"completions,omitempty"`
+	BackoffLimit       *int32            `json:"backoffLimit,omitempty"`
+	Resources          ResourceMetrics   `json:"resources,omitempty"`
 }
 
 // NetworkingMetrics represents metrics for networking resources
 type NetworkingMetrics struct {
-	Services  []ServiceMetrics `json:"services"`
-	Ingresses []IngressMetrics `json:"ingresses"`
+	Services        []ServiceMetrics       `json:"services"`
+	Ingresses       []IngressMetrics       `json:"ingresses"`
+	NetworkPolicies []NetworkPolicyMetrics `json:"networkPolicies"`
 }
 
 // ServiceMetrics represents metrics for a service
 type ServiceMetrics struct {
-	Name                string            `json:"name"`
-	Namespace           string            `json:"namespace"`
-	Type                string            `json:"type"`
-	ClusterIP           string            `json:"clusterIP"`
-	ExternalIP          []string          `json:"externalIP,omitempty"`
-	LoadBalancerIP      string            `json:"loadBalancerIP,omitempty"`
-	LoadBalancerIngress []string          `json:"loadBalancerIngress,omitempty"`
-	Ports               []ServicePort     `json:"ports"`
-	Selector            map[string]string `json:"selector,omitempty"`
-	Labels              map[string]string `json:"labels"`
+	Name                  string            `json:"name"`
+	Namespace             string            `json:"namespace"`
+	Type                  string            `json:"type"`
+	ClusterIP             string            `json:"clusterIP"`
+	ExternalIP            []string          `json:"externalIP,omitempty"`
+	LoadBalancerIP        string            `json:"loadBalancerIP,omitempty"`
+	LoadBalancerIngress   []string          `json:"loadBalancerIngress,omitempty"`
+	Ports                 []ServicePort     `json:"ports"`
+	Selector              map[string]string `json:"selector,omitempty"`
+	Labels                map[string]string `json:"labels"`
+	SessionAffinity       string            `json:"sessionAffinity,omitempty"`
+	ExternalTrafficPolicy string            `json:"externalTrafficPolicy,omitempty"`
+	HealthCheckNodePort   int32             `json:"healthCheckNodePort,omitempty"`
+	IPFamilies            []string          `json:"ipFamilies,omitempty"`
+	IPFamilyPolicy        string            `json:"ipFamilyPolicy,omitempty"`
+	Status                ServiceStatus     `json:"status"`
+	Annotations           map[string]string `json:"annotations"`
 }
 
 // ServicePort represents a port exposed by a service
@@ -278,6 +425,9 @@ type IngressMetrics struct {
 	TLS                 []IngressTLS      `json:"tls,omitempty"`
 	LoadBalancerIngress []string          `json:"loadBalancerIngress,omitempty"`
 	Labels              map[string]string `json:"labels"`
+	Annotations         map[string]string `json:"annotations"`
+	Status              IngressStatus     `json:"status"`
+	CreationTimestamp   *time.Time        `json:"creationTimestamp"`
 }
 
 // IngressRule represents a rule in an ingress
@@ -310,36 +460,91 @@ type IngressTLS struct {
 	SecretName string   `json:"secretName"`
 }
 
-// CronJobMetrics represents metrics for a cron job
+// CronJobMetrics represents metrics for a CronJob
 type CronJobMetrics struct {
-	Name             string            `json:"name"`
-	Namespace        string            `json:"namespace"`
-	Schedule         string            `json:"schedule"`
-	Suspend          bool              `json:"suspend"`
-	ActiveJobs       int               `json:"activeJobs"`
-	LastScheduleTime *time.Time        `json:"lastScheduleTime,omitempty"`
-	Labels           map[string]string `json:"labels"`
+	Name       string            `json:"name"`
+	Namespace  string            `json:"namespace"`
+	Schedule   string            `json:"schedule"`
+	Labels     map[string]string `json:"labels"`
+	Status     CronJobStatus     `json:"status"`
+	Spec       CronJobSpec       `json:"spec"`
+	JobMetrics []JobMetrics      `json:"jobs,omitempty"`
+}
+
+// CronJobStatus represents the status of a CronJob
+type CronJobStatus struct {
+	LastScheduleTime   *time.Time `json:"lastScheduleTime,omitempty"`
+	LastSuccessfulTime *time.Time `json:"lastSuccessfulTime,omitempty"`
+	NextScheduledTime  *time.Time `json:"nextScheduledTime,omitempty"`
+	Active             int        `json:"active"`
+	SuccessRate        float64    `json:"successRate"`
+}
+
+// CronJobSpec represents the specification of a CronJob
+type CronJobSpec struct {
+	Suspend                    bool   `json:"suspend"`
+	Concurrency                string `json:"concurrencyPolicy"`
+	StartingDeadlineSeconds    int64  `json:"startingDeadlineSeconds"`
+	SuccessfulJobsHistoryLimit int32  `json:"successfulJobsHistoryLimit"`
+	FailedJobsHistoryLimit     int32  `json:"failedJobsHistoryLimit"`
 }
 
 // HPAMetrics represents metrics for a horizontal pod autoscaler
 type HPAMetrics struct {
-	Name                            string            // Name of the HPA
-	Namespace                       string            // Namespace of the HPA
-	CurrentReplicas                 int32             // Current number of replicas
-	DesiredReplicas                 int32             // Desired number of replicas
-	CurrentCPUUtilizationPercentage *int32            // Current CPU utilization percentage, if available
-	TargetCPUUtilizationPercentage  *int32            // Target CPU utilization percentage, if specified
+	Name                            string            `json:"name"`
+	Namespace                       string            `json:"namespace"`
+	ScaleTargetRef                  ScaleTargetRef    `json:"scaleTargetRef"`
+	MinReplicas                     *int32            `json:"minReplicas,omitempty"`
+	MaxReplicas                     int32             `json:"maxReplicas"`
+	CurrentReplicas                 int32             `json:"currentReplicas"`
+	DesiredReplicas                 int32             `json:"desiredReplicas"`
+	CurrentCPUUtilizationPercentage *int32            `json:"currentCPUUtilizationPercentage,omitempty"`
+	TargetCPUUtilizationPercentage  *int32            `json:"targetCPUUtilizationPercentage,omitempty"`
+	LastScaleTime                   *time.Time        `json:"lastScaleTime,omitempty"`
+	ObservedGeneration              *int64            `json:"observedGeneration,omitempty"`
+	Status                          HPAStatus         `json:"status"`
 	Labels                          map[string]string `json:"labels"`
+	Annotations                     map[string]string `json:"annotations"`
+}
+
+// ScaleTargetRef represents the target reference for scaling
+type ScaleTargetRef struct {
+	Kind       string `json:"kind"`
+	Name       string `json:"name"`
+	APIVersion string `json:"apiVersion"`
+}
+
+// HPAStatus represents the status of the HPA
+type HPAStatus struct {
+	CurrentReplicas                 int32      `json:"currentReplicas"`
+	DesiredReplicas                 int32      `json:"desiredReplicas"`
+	CurrentCPUUtilizationPercentage *int32     `json:"currentCPUUtilizationPercentage,omitempty"`
+	LastScaleTime                   *time.Time `json:"lastScaleTime,omitempty"`
 }
 
 // ReplicaSetMetrics represents metrics for a replica set
 type ReplicaSetMetrics struct {
-	Name              string            `json:"name"`
-	Namespace         string            `json:"namespace"`
-	Replicas          int32             `json:"replicas"`
-	ReadyReplicas     int32             `json:"readyReplicas"`
-	AvailableReplicas int32             `json:"availableReplicas"`
-	Labels            map[string]string `json:"labels"`
+	Name                 string            `json:"name"`
+	Namespace            string            `json:"namespace"`
+	Replicas             int32             `json:"replicas"`
+	ReadyReplicas        int32             `json:"readyReplicas"`
+	AvailableReplicas    int32             `json:"availableReplicas"`
+	CurrentReplicas      int32             `json:"currentReplicas"`
+	FullyLabeledReplicas int32             `json:"fullyLabeledReplicas"`
+	ObservedGeneration   int64             `json:"observedGeneration"`
+	Conditions           []RSCondition     `json:"conditions"`
+	Labels               map[string]string `json:"labels"`
+	Annotations          map[string]string `json:"annotations"`
+	CreationTimestamp    *time.Time        `json:"creationTimestamp"`
+}
+
+// RSCondition represents a condition of a ReplicaSet
+type RSCondition struct {
+	Type               string     `json:"type"`
+	Status             string     `json:"status"`
+	LastTransitionTime *time.Time `json:"lastTransitionTime,omitempty"`
+	Reason             string     `json:"reason,omitempty"`
+	Message            string     `json:"message,omitempty"`
 }
 
 // ReplicationControllerMetrics represents metrics for a replication controller
@@ -350,4 +555,801 @@ type ReplicationControllerMetrics struct {
 	ReadyReplicas     int32             `json:"readyReplicas"`
 	AvailableReplicas int32             `json:"availableReplicas"`
 	Labels            map[string]string `json:"labels"`
+	// Adding missing fields
+	ObservedGeneration   int64             `json:"observedGeneration"`
+	FullyLabeledReplicas int32             `json:"fullyLabeledReplicas"`
+	Conditions           []RCCondition     `json:"conditions"`
+	Annotations          map[string]string `json:"annotations"`
+	CreationTimestamp    *time.Time        `json:"creationTimestamp"`
+}
+
+// RCCondition represents a condition of a ReplicationController
+type RCCondition struct {
+	Type               string     `json:"type"`
+	Status             string     `json:"status"`
+	LastTransitionTime *time.Time `json:"lastTransitionTime,omitempty"`
+	Reason             string     `json:"reason,omitempty"`
+	Message            string     `json:"message,omitempty"`
+}
+
+// NodeDetailedMetrics represents detailed metrics collected for a node
+type NodeDetailedMetrics struct {
+	CPU        CPUMetrics     `json:"cpu"`
+	Memory     MemoryMetrics  `json:"memory"`
+	Network    NetworkMetrics `json:"network"`
+	DiskIO     DiskMetrics    `json:"diskIO"`
+	Filesystem FSMetrics      `json:"filesystem"`
+	Process    ProcessMetrics `json:"process"`
+	Runtime    RuntimeMetrics `json:"runtime"`
+	System     SystemMetrics  `json:"system"`
+	Volume     VolumeMetrics  `json:"volume"`
+	Container  ContainerStats `json:"container"`
+}
+
+// CPUMetrics represents CPU metrics from cadvisor
+type CPUMetrics struct {
+	// Usage values in nanoseconds
+	UsageTotal     uint64            `json:"usageTotal"`
+	UsageUser      uint64            `json:"usageUser"`
+	UsageSystem    uint64            `json:"usageSystem"`
+	SystemTime     uint64            `json:"systemTime"`
+	UserTime       uint64            `json:"userTime"`
+	LoadAverage    LoadAverageStats  `json:"loadAverage"`
+	SchedulerStats SchedulerMetrics  `json:"schedulerStats"`
+	Throttling     ThrottlingMetrics `json:"throttling"`
+	// Per-core CPU metrics
+	PerCoreUsage map[string]uint64 `json:"perCoreUsage,omitempty"`
+
+	// Add new fields for detailed CPU metrics
+	CPUPressure float64    `json:"cpuPressure,omitempty"`
+	SchedStats  SchedStats `json:"schedStats,omitempty"`
+}
+
+// LoadAverageStats represents CPU load averages
+type LoadAverageStats struct {
+	Load1  float64 `json:"load1"`
+	Load5  float64 `json:"load5"`
+	Load15 float64 `json:"load15"`
+}
+
+// SchedulerMetrics represents CPU scheduler statistics
+type SchedulerMetrics struct {
+	RunQueueLength   uint64 `json:"runQueueLength"`
+	ContextSwitches  uint64 `json:"contextSwitches"`
+	ProcessesCreated uint64 `json:"processesCreated"`
+	ProcessesRunning uint64 `json:"processesRunning"`
+	ProcessesBlocked uint64 `json:"processesBlocked"`
+}
+
+// ThrottlingMetrics represents CPU throttling information
+type ThrottlingMetrics struct {
+	Periods          uint64 `json:"periods"`
+	ThrottledPeriods uint64 `json:"throttledPeriods"`
+	ThrottledTime    uint64 `json:"throttledTime"`
+}
+
+// MemoryMetrics represents memory statistics
+type MemoryMetrics struct {
+	// All values in bytes
+	Total           uint64  `json:"total"`
+	Available       uint64  `json:"available"`
+	Used            uint64  `json:"used"`
+	WorkingSet      uint64  `json:"workingSet"`
+	RSS             uint64  `json:"rss"`
+	Cache           uint64  `json:"cache"`
+	Swap            uint64  `json:"swap"`
+	PageFaults      uint64  `json:"pageFaults"`
+	KernelUsage     uint64  `json:"kernelUsage,omitempty"`
+	OOMEvents       uint64  `json:"oomEvents"`
+	OOMKills        uint64  `json:"oomKills"`
+	PressureLevel   float64 `json:"pressureLevel"`
+	SwapInBytes     uint64  `json:"swapInBytes"`
+	SwapOutBytes    uint64  `json:"swapOutBytes"`
+	MajorPageFaults uint64  `json:"majorPageFaults"`
+	MinorPageFaults uint64  `json:"minorPageFaults"`
+}
+
+// NetworkMetrics represents network statistics
+// Note: Metric availability depends on CNI plugin and network configuration
+type NetworkMetrics struct {
+	// Basic network metrics - generally available
+	Summary NetworkSummary `json:"summary"`
+
+	// Detailed interface metrics - availability varies by platform and CNI
+	Interfaces []InterfaceStats `json:"interfaces,omitempty"`
+}
+
+// InterfaceStats represents per-interface network statistics
+type InterfaceStats struct {
+	InterfaceName string `json:"interfaceName"`
+	RxBytes       uint64 `json:"rxBytes"`
+	TxBytes       uint64 `json:"txBytes"`
+	RxPackets     uint64 `json:"rxPackets"`
+	TxPackets     uint64 `json:"txPackets"`
+	RxErrors      uint64 `json:"rxErrors"`
+	TxErrors      uint64 `json:"txErrors"`
+	RxDropped     uint64 `json:"rxDropped"`
+	TxDropped     uint64 `json:"txDropped"`
+}
+
+// NetworkSummary represents aggregated network statistics
+type NetworkSummary struct {
+	RxBytesTotal          uint64
+	TxBytesTotal          uint64
+	ContainerRxBytesTotal uint64
+	ContainerTxBytesTotal uint64
+}
+
+// FSMetrics represents filesystem metrics
+type FSMetrics struct {
+	// Capacity and usage information
+	TotalBytes     uint64 `json:"totalBytes"`
+	UsedBytes      uint64 `json:"usedBytes"`
+	AvailableBytes uint64 `json:"availableBytes"`
+	// Inode information
+	TotalInodes uint64 `json:"totalInodes"`
+	UsedInodes  uint64 `json:"usedInodes"`
+	FreeInodes  uint64 `json:"freeInodes"`
+	// Per-device statistics
+	DeviceStats map[string]FSStats `json:"deviceStats"`
+	// Add inode metrics
+	Inodes InodeStats `json:"inodes"`
+}
+
+// FSStats represents per-device filesystem statistics
+type FSStats struct {
+	Major       uint64 `json:"major"`
+	Minor       uint64 `json:"minor"`
+	ReadOps     uint64 `json:"readOps"`
+	WriteOps    uint64 `json:"writeOps"`
+	ReadBytes   uint64 `json:"readBytes"`
+	WriteBytes  uint64 `json:"writeBytes"`
+	ReadTimeMs  uint64 `json:"readTimeMs"`
+	WriteTimeMs uint64 `json:"writeTimeMs"`
+	IoTimeMs    uint64 `json:"ioTimeMs"`
+}
+
+// ContainerStats represents container-specific metrics
+type ContainerStats struct {
+	RunningCount int                         `json:"runningCount"`
+	TotalCount   int                         `json:"totalCount"`
+	PerContainer map[string]ContainerMetrics `json:"perContainer"`
+}
+
+// GPUMetrics represents GPU device metrics if available
+type GPUMetrics struct {
+	DeviceID    string          `json:"deviceId"`
+	MemoryTotal uint64          `json:"memoryTotal"`
+	MemoryUsed  uint64          `json:"memoryUsed"`
+	OptMetrics  OptionalMetrics `json:"optionalMetrics,omitempty"`
+}
+
+// OptionalMetrics represents optional metrics for GPU devices
+type OptionalMetrics struct {
+	DutyCycle   float64 `json:"dutyCycle"`
+	Temperature float64 `json:"temperature"`
+	PowerUsage  float64 `json:"powerUsage"`
+}
+
+// ProcessMetrics represents process-related metrics
+type ProcessMetrics struct {
+	ProcessCount int `json:"processCount"`
+	ThreadCount  int `json:"threadCount,omitempty"`
+	FDCount      int `json:"fdCount,omitempty"`
+}
+
+// RuntimeMetrics represents container runtime metrics
+type RuntimeMetrics struct {
+	Operations map[string]RuntimeOperation `json:"operations"`
+}
+
+// RuntimeOperation represents a runtime operation metric
+type RuntimeOperation struct {
+	Type   string `json:"type"`
+	Count  uint64 `json:"count"`
+	Errors uint64 `json:"errors"`
+}
+
+// ImageStats represents container image statistics
+type ImageStats struct {
+	// Total number of images
+	TotalCount int `json:"totalCount"`
+	// Total size of all images in bytes
+	TotalSizeBytes uint64 `json:"totalSizeBytes"`
+}
+
+// ContainerMetrics represents metrics for a single container
+// Note: Some metrics may not be available depending on the container runtime
+type ContainerMetrics struct {
+	// Required fields - always available
+	Name      string    `json:"name"`
+	ID        string    `json:"id"`
+	State     string    `json:"state"`
+	StartTime time.Time `json:"startTime"`
+
+	// Basic resource usage - generally available
+	UsageNanos int64 `json:"usageNanos"` // CPU usage in nanoseconds
+	UsageBytes int64 `json:"usageBytes"` // Memory usage in bytes
+
+	// Detailed metrics - availability varies by runtime and platform
+	CPU struct {
+		UsageTotal  uint64 `json:"usageTotal"`
+		UsageUser   uint64 `json:"usageUser,omitempty"`
+		UsageSystem uint64 `json:"usageSystem,omitempty"`
+		Throttling  *struct {
+			Periods          uint64 `json:"periods"`
+			ThrottledPeriods uint64 `json:"throttledPeriods"`
+
+			ThrottledTime uint64 `json:"throttledTime"`
+		} `json:"throttling,omitempty"`
+	} `json:"cpu"`
+
+	// Optional fields - may not be available in all environments
+	LastTerminationReason string          `json:"lastTerminationReason,omitempty"`
+	BlockIO               *BlockIOMetrics `json:"blockIO,omitempty"`
+	RestartCount          int32           `json:"restartCount"`
+	Ready                 bool            `json:"ready"`
+	Memory                MemoryMetrics   `json:"memory"`
+}
+
+// BlockIOMetrics represents block I/O metrics
+type BlockIOMetrics struct {
+	// Read operations
+	ReadOps uint64 `json:"readOps"`
+	// Write operations
+	WriteOps uint64 `json:"writeOps"`
+	// Bytes read
+	ReadBytes uint64 `json:"readBytes"`
+	// Bytes written
+	WriteBytes uint64 `json:"writeBytes"`
+}
+
+// DiskMetrics represents disk I/O metrics
+type DiskMetrics struct {
+	ReadBytes  int64           // Total bytes read across all devices
+	WriteBytes int64           // Total bytes written across all devices
+	Devices    []NodeDiskStats // Per-device statistics
+}
+
+// ServiceStatus represents the status of a Kubernetes service
+type ServiceStatus struct {
+	LoadBalancer LoadBalancerStatus `json:"loadBalancer"`
+	Conditions   []ServiceCondition `json:"conditions"`
+}
+
+// ServiceCondition represents the current condition of a Kubernetes service
+type ServiceCondition struct {
+	Type               string     `json:"type"`
+	Status             string     `json:"status"`
+	LastTransitionTime *time.Time `json:"lastTransitionTime,omitempty"`
+	Reason             string     `json:"reason,omitempty"`
+	Message            string     `json:"message,omitempty"`
+}
+
+// LoadBalancerStatus represents the status of a Kubernetes load balancer
+type LoadBalancerStatus struct {
+	Ingress []LoadBalancerIngress `json:"ingress,omitempty"`
+}
+
+// LoadBalancerIngress represents the ingress point for a Kubernetes load balancer
+type LoadBalancerIngress struct {
+	IP       string `json:"ip,omitempty"`
+	Hostname string `json:"hostname,omitempty"`
+}
+
+// IngressStatus represents the status of a Kubernetes ingress
+type IngressStatus struct {
+	LoadBalancer LoadBalancerStatus `json:"loadBalancer"`
+}
+
+// IngressCondition represents the current condition of a Kubernetes ingress
+type IngressCondition struct {
+	Type               string     `json:"type"`
+	Status             string     `json:"status"`
+	LastTransitionTime *time.Time `json:"lastTransitionTime,omitempty"`
+	Reason             string     `json:"reason,omitempty"`
+	Message            string     `json:"message,omitempty"`
+}
+
+// SystemMetrics represents system-level metrics
+// Note: Availability of these metrics depends on the host system configuration
+type SystemMetrics struct {
+	UptimeSeconds   float64 `json:"uptimeSeconds"`
+	NumCPUs         int64   `json:"numCPUs"`
+	NumProcs        uint64  `json:"numProcs"`
+	BootTimeSeconds float64 `json:"bootTimeSeconds"`
+
+	// Platform-specific fields - may not be available in all environments
+	KernelVersion string `json:"kernelVersion,omitempty"`
+	OSVersion     string `json:"osVersion,omitempty"`
+	// These IDs may not be available on all platforms
+	MachineID  string `json:"machineID,omitempty"`
+	SystemUUID string `json:"systemUUID,omitempty"`
+	BootID     string `json:"bootID,omitempty"`
+}
+
+// KubeletMetrics represents kubelet-related metrics
+// Note: Metric availability depends on Kubelet version and configuration
+type KubeletMetrics struct {
+	// Basic metrics - generally available
+	PodStartLatency LatencyMetric `json:"podStartLatency"`
+
+	// Advanced metrics - may not be available in all configurations
+	OperationLatency  map[string]LatencyMetric   `json:"operationLatency,omitempty"`
+	CGroupManager     *CGroupManagerMetrics      `json:"cgroupManager,omitempty"`
+	PLEGLatency       *LatencyMetric             `json:"plegLatency,omitempty"`
+	RuntimeOperations map[string]RuntimeOpMetric `json:"runtimeOperations,omitempty"`
+	EvictionStats     *EvictionStats             `json:"evictionStats,omitempty"`
+}
+
+// LatencyMetric represents latency metrics
+type LatencyMetric struct {
+	P50   float64 `json:"p50"`
+	P90   float64 `json:"p90"`
+	P99   float64 `json:"p99"`
+	Count int64   `json:"count"`
+}
+
+// CGroupManagerMetrics represents cgroup manager metrics
+type CGroupManagerMetrics struct {
+	Operations uint64        `json:"operations"`
+	Errors     uint64        `json:"errors"`
+	Latency    LatencyMetric `json:"latency"`
+}
+
+// RuntimeOpMetric represents runtime operation metrics
+type RuntimeOpMetric struct {
+	Count    uint64  `json:"count"`
+	Errors   uint64  `json:"errors"`
+	Duration float64 `json:"duration"`
+}
+
+// EvictionStats represents eviction statistics
+type EvictionStats struct {
+	ThresholdMet      bool       `json:"thresholdMet"`
+	HardEvictionCount uint64     `json:"hardEvictionCount"`
+	SoftEvictionCount uint64     `json:"softEvictionCount"`
+	LastEvictionTime  *time.Time `json:"lastEvictionTime,omitempty"`
+}
+
+// VolumeMetrics represents volume-related metrics
+type VolumeMetrics struct {
+	AttachDetachCount  uint64                   `json:"attachDetachCount"`
+	InUseCount         uint64                   `json:"inUseCount"`
+	OperationLatency   map[string]LatencyMetric `json:"operationLatency"`
+	TotalCapacityBytes uint64                   `json:"totalCapacityBytes"`
+	AvailableBytes     uint64                   `json:"availableBytes"`
+	UsedBytes          uint64                   `json:"usedBytes"`
+}
+
+// PodReadinessGate represents a readiness gate for a pod
+type PodReadinessGate struct {
+	ConditionType string `json:"conditionType"`
+	Status        bool   `json:"status"`
+}
+
+// JobCondition represents a condition of a Job
+type JobCondition struct {
+	Type               string     `json:"type"`
+	Status             string     `json:"status"`
+	LastProbeTime      *time.Time `json:"lastProbeTime,omitempty"`
+	LastTransitionTime *time.Time `json:"lastTransitionTime,omitempty"`
+	Reason             string     `json:"reason,omitempty"`
+	Message            string     `json:"message,omitempty"`
+}
+
+// DaemonSetStatus represents the status of a DaemonSet
+type DaemonSetStatus struct {
+	ObservedGeneration int64 `json:"observedGeneration"`
+}
+
+// DaemonSetCondition represents a condition of a DaemonSet
+type DaemonSetCondition struct {
+	Type               string     `json:"type"`
+	Status             string     `json:"status"`
+	LastTransitionTime *time.Time `json:"lastTransitionTime,omitempty"`
+	Reason             string     `json:"reason,omitempty"`
+	Message            string     `json:"message,omitempty"`
+}
+
+// NodeInfo represents node information
+type NodeInfo struct {
+	Architecture            string `json:"architecture"`
+	ContainerRuntimeVersion string `json:"containerRuntimeVersion"`
+	KernelVersion           string `json:"kernelVersion"`
+	OSImage                 string `json:"osImage"`
+	KubeletVersion          string `json:"kubeletVersion"`
+}
+
+// NodeNetworkStats represents network metrics for a node
+type NodeNetworkStats struct {
+	InterfaceName string `json:"interfaceName"`
+	RxBytes       uint64 `json:"rxBytes"`
+	RxPackets     uint64 `json:"rxPackets"`
+	RxErrors      uint64 `json:"rxErrors"`
+	RxDropped     uint64 `json:"rxDropped"`
+	TxBytes       uint64 `json:"txBytes"`
+	TxPackets     uint64 `json:"txPackets"`
+	TxErrors      uint64 `json:"txErrors"`
+	TxDropped     uint64 `json:"txDropped"`
+}
+
+// NodeDiskStats represents disk metrics for a node
+type NodeDiskStats struct {
+	Device         string `json:"device"`
+	ReadOps        uint64 `json:"readOps"`
+	WriteOps       uint64 `json:"writeOps"`
+	ReadBytes      uint64 `json:"readBytes"`
+	WriteBytes     uint64 `json:"writeBytes"`
+	ReadLatency    uint64 `json:"readLatency"`
+	WriteLatency   uint64 `json:"writeLatency"`
+	IoInProgress   uint64 `json:"ioInProgress"`
+	IoTime         uint64 `json:"ioTime"`
+	WeightedIoTime uint64 `json:"weightedIoTime"`
+}
+
+// InodeStats represents inode metrics
+type InodeStats struct {
+	Total uint64 `json:"total"`
+	Used  uint64 `json:"used"`
+	Free  uint64 `json:"free"`
+}
+
+// VolumeMountMetrics represents volume mount metrics
+type VolumeMountMetrics struct {
+	Name             string `json:"name"`
+	MountPath        string `json:"mountPath"`
+	ReadOnly         bool   `json:"readOnly,omitempty"`
+	SubPath          string `json:"subPath,omitempty"`
+	SubPathExpr      string `json:"subPathExpr,omitempty"`
+	MountPropagation string `json:"mountPropagation,omitempty"`
+}
+
+// NetworkPolicyMetrics represents metrics for networking resources
+type NetworkPolicyMetrics struct {
+	Name        string                     `json:"name"`
+	Namespace   string                     `json:"namespace"`
+	Labels      map[string]string          `json:"labels"`
+	Annotations map[string]string          `json:"annotations"`
+	PodSelector map[string]string          `json:"podSelector"`
+	Ingress     []NetworkPolicyIngressRule `json:"ingress"`
+	Egress      []NetworkPolicyEgressRule  `json:"egress"`
+	PolicyTypes []string                   `json:"policyTypes"`
+}
+
+// NetworkPolicyIngressRule represents an ingress rule in a network policy
+type NetworkPolicyIngressRule struct {
+	Ports []NetworkPolicyPort `json:"ports"`
+	From  []NetworkPolicyPeer `json:"from"`
+}
+
+// NetworkPolicyEgressRule represents an egress rule in a network policy
+type NetworkPolicyEgressRule struct {
+	Ports []NetworkPolicyPort `json:"ports"`
+	To    []NetworkPolicyPeer `json:"to"`
+}
+
+// NetworkPolicyPort represents a port in a network policy
+type NetworkPolicyPort struct {
+	Protocol string `json:"protocol"`
+	Port     int32  `json:"port"`
+}
+
+// NetworkPolicyPeer represents a peer in a network policy
+type NetworkPolicyPeer struct {
+	PodSelector       map[string]string `json:"podSelector,omitempty"`
+	NamespaceSelector map[string]string `json:"namespaceSelector,omitempty"`
+	IPBlock           *IPBlock          `json:"ipBlock,omitempty"`
+}
+
+// IPBlock represents an IP block in a network policy
+type IPBlock struct {
+	CIDR   string   `json:"cidr"`
+	Except []string `json:"except,omitempty"`
+}
+
+// StorageClassMetrics represents metrics for storage classes
+type StorageClassMetrics struct {
+	Name                 string            `json:"name"`
+	Provisioner          string            `json:"provisioner"`
+	ReclaimPolicy        string            `json:"reclaimPolicy"`
+	VolumeBindingMode    string            `json:"volumeBindingMode"`
+	AllowVolumeExpansion bool              `json:"allowVolumeExpansion"`
+	Labels               map[string]string `json:"labels"`
+	Annotations          map[string]string `json:"annotations"`
+	IsDefault            bool              `json:"isDefault"`
+	Parameters           map[string]string `json:"parameters"`
+	MountOptions         []string          `json:"mountOptions,omitempty"`
+	CreationTimestamp    *time.Time        `json:"creationTimestamp"`
+
+	// New fields for CSI and capacity metrics
+	CSIDriver           *CSIDriverMetrics    `json:"csiDriver,omitempty"`
+	StoragePools        []StoragePoolMetrics `json:"storagePools,omitempty"`
+	TotalCapacity       int64                `json:"totalCapacity,omitempty"`
+	AllocatedCapacity   int64                `json:"allocatedCapacity,omitempty"`
+	AvailableCapacity   int64                `json:"availableCapacity,omitempty"`
+	CapacityUtilization float64              `json:"capacityUtilization,omitempty"`
+	ProvisionedPVCs     int32                `json:"provisionedPVCs,omitempty"`
+	ProvisioningRate    float64              `json:"provisioningRate,omitempty"`
+}
+
+// HardwareTopology represents the hardware topology of a node
+type HardwareTopology struct {
+	Sockets int64 `json:"sockets,omitempty"`
+	Cores   int64 `json:"cores,omitempty"`
+	Threads int64 `json:"threads,omitempty"`
+	// NUMA topology
+	NUMANodes []NUMANode `json:"numaNodes,omitempty"`
+}
+
+// NUMANode represents NUMA node information
+type NUMANode struct {
+	ID       int32    `json:"id"`
+	Memory   uint64   `json:"memory,omitempty"`   // Memory in bytes
+	CPUList  []int32  `json:"cpuList,omitempty"`  // List of CPU IDs
+	Distance []uint32 `json:"distance,omitempty"` // NUMA distance array
+}
+
+// PowerMetrics represents power management information
+type PowerMetrics struct {
+	CurrentWatts float64 `json:"currentWatts,omitempty"`
+	MaxWatts     float64 `json:"maxWatts,omitempty"`
+	// Power capping information
+	PowerCap     *float64 `json:"powerCap,omitempty"`
+	PowerProfile string   `json:"powerProfile,omitempty"`
+}
+
+// NodeTaint represents a Kubernetes node taint
+type NodeTaint struct {
+	Key       string     `json:"key"`
+	Value     string     `json:"value,omitempty"`
+	Effect    string     `json:"effect"`
+	TimeAdded *time.Time `json:"timeAdded,omitempty"`
+}
+
+// NodeLease represents node lease information
+type NodeLease struct {
+	HolderIdentity       string     `json:"holderIdentity"`
+	LeaseDurationSeconds int32      `json:"leaseDurationSeconds"`
+	AcquireTime          *time.Time `json:"acquireTime,omitempty"`
+	RenewTime            *time.Time `json:"renewTime,omitempty"`
+}
+
+// ExtendedResource represents custom resource information
+type ExtendedResource struct {
+	Name        string `json:"name"`
+	Capacity    string `json:"capacity"`
+	Allocatable string `json:"allocatable"`
+	Used        string `json:"used,omitempty"`
+}
+
+// PodDisruptionBudgetMetrics represents PDB metrics
+type PodDisruptionBudgetMetrics struct {
+	MinAvailable       string `json:"minAvailable,omitempty"`
+	MaxUnavailable     string `json:"maxUnavailable,omitempty"`
+	CurrentHealthy     int32  `json:"currentHealthy"`
+	DesiredHealthy     int32  `json:"desiredHealthy"`
+	DisruptionsAllowed int32  `json:"disruptionsAllowed"`
+	ExpectedPods       int32  `json:"expectedPods"`
+}
+
+// TopologySpreadConstraint represents pod topology spread constraints
+type TopologySpreadConstraint struct {
+	MaxSkew           int32  `json:"maxSkew"`
+	TopologyKey       string `json:"topologyKey"`
+	WhenUnsatisfiable string `json:"whenUnsatisfiable"`
+	LabelSelector     string `json:"labelSelector,omitempty"`
+	MinDomains        *int32 `json:"minDomains,omitempty"`
+}
+
+// PodOverheadMetrics represents pod overhead metrics
+type PodOverheadMetrics struct {
+	CPU    string `json:"cpu,omitempty"`
+	Memory string `json:"memory,omitempty"`
+}
+
+// PodSchedulingGate represents pod scheduling gates
+type PodSchedulingGate struct {
+	Name   string     `json:"name"`
+	Active bool       `json:"active"`
+	Since  *time.Time `json:"since,omitempty"`
+}
+
+// SecurityContextMetrics represents security context information
+type SecurityContextMetrics struct {
+	RunAsUser      *int64            `json:"runAsUser,omitempty"`
+	RunAsGroup     *int64            `json:"runAsGroup,omitempty"`
+	FSGroup        *int64            `json:"fsGroup,omitempty"`
+	RunAsNonRoot   *bool             `json:"runAsNonRoot,omitempty"`
+	SELinuxOptions map[string]string `json:"seLinuxOptions,omitempty"`
+	Sysctls        []string          `json:"sysctls,omitempty"`
+	SeccompProfile string            `json:"seccompProfile,omitempty"`
+}
+
+// AffinityMetrics represents pod affinity/anti-affinity rules
+type AffinityMetrics struct {
+	NodeAffinity    []NodeAffinityTerm `json:"nodeAffinity,omitempty"`
+	PodAffinity     []PodAffinityTerm  `json:"podAffinity,omitempty"`
+	PodAntiAffinity []PodAffinityTerm  `json:"podAntiAffinity,omitempty"`
+}
+
+// NodeAffinityTerm represents a node affinity requirement
+type NodeAffinityTerm struct {
+	Key      string   `json:"key"`
+	Operator string   `json:"operator"`
+	Values   []string `json:"values,omitempty"`
+}
+
+// PodAffinityTerm represents a pod affinity/anti-affinity requirement
+type PodAffinityTerm struct {
+	LabelSelector string   `json:"labelSelector"`
+	Namespaces    []string `json:"namespaces,omitempty"`
+	TopologyKey   string   `json:"topologyKey"`
+}
+
+// InitContainerMetrics represents init container specific metrics
+type InitContainerMetrics struct {
+	ContainerMetrics
+	CompletionTime *time.Time `json:"completionTime,omitempty"`
+	Duration       float64    `json:"duration,omitempty"`
+}
+
+// EphemeralContainerMetrics represents ephemeral container metrics
+type EphemeralContainerMetrics struct {
+	ContainerMetrics
+	TargetContainerName string     `json:"targetContainerName"`
+	StartTime           *time.Time `json:"startTime,omitempty"`
+}
+
+// QoSMetrics represents detailed QoS metrics
+type QoSMetrics struct {
+	Class            string  `json:"class"`
+	CPUGuaranteed    bool    `json:"cpuGuaranteed"`
+	MemoryGuaranteed bool    `json:"memoryGuaranteed"`
+	BurstableLimit   float64 `json:"burstableLimit,omitempty"`
+}
+
+// CSIDriverMetrics represents CSI driver metrics
+type CSIDriverMetrics struct {
+	Name               string            `json:"name"`
+	Version            string            `json:"version,omitempty"`
+	Available          bool              `json:"available"`
+	VolumeSnapshotting bool              `json:"volumeSnapshotting,omitempty"`
+	VolumeCloning      bool              `json:"volumeCloning,omitempty"`
+	VolumeExpansion    bool              `json:"volumeExpansion,omitempty"`
+	NodePluginPods     map[string]string `json:"nodePluginPods,omitempty"`
+	ControllerPods     map[string]string `json:"controllerPods,omitempty"`
+	OperationStats     map[string]int64  `json:"operationStats,omitempty"`
+}
+
+// StoragePoolMetrics represents storage pool metrics
+type StoragePoolMetrics struct {
+	Name           string  `json:"name"`
+	Provider       string  `json:"provider"`
+	TotalCapacity  int64   `json:"totalCapacity"`
+	UsedCapacity   int64   `json:"usedCapacity"`
+	AvailableSpace int64   `json:"availableSpace"`
+	VolumeCount    int32   `json:"volumeCount,omitempty"`
+	UtilizationPct float64 `json:"utilizationPct,omitempty"`
+	Health         string  `json:"health,omitempty"`
+	StorageClass   string  `json:"storageClass"`
+}
+
+// VolumeHealthMetrics represents volume health status
+type VolumeHealthMetrics struct {
+	VolumeName     string    `json:"volumeName"`
+	PodName        string    `json:"podName,omitempty"`
+	Namespace      string    `json:"namespace,omitempty"`
+	State          string    `json:"state"`
+	LastCheckTime  time.Time `json:"lastCheckTime"`
+	ErrorMessage   string    `json:"errorMessage,omitempty"`
+	RepairRequired bool      `json:"repairRequired,omitempty"`
+	IoPerformance  string    `json:"ioPerformance,omitempty"`
+	FsIntegrity    bool      `json:"fsIntegrity,omitempty"`
+}
+
+// VolumeAttachmentMetrics represents volume attachment status
+type VolumeAttachmentMetrics struct {
+	VolumeName      string    `json:"volumeName"`
+	PodName         string    `json:"podName,omitempty"`
+	Namespace       string    `json:"namespace,omitempty"`
+	AttachTime      time.Time `json:"attachTime"`
+	DevicePath      string    `json:"devicePath,omitempty"`
+	AttachmentState string    `json:"attachmentState"`
+	ReadOnly        bool      `json:"readOnly"`
+	MountPoint      string    `json:"mountPoint,omitempty"`
+	AttachError     string    `json:"attachError,omitempty"`
+}
+
+// NodeVolumeMetrics represents node volume metrics
+type NodeVolumeMetrics struct {
+	// Existing fields
+	InUseCount       uint64                   `json:"inUseCount"`
+	OperationLatency map[string]LatencyMetric `json:"operationLatency,omitempty"`
+
+	// New fields
+	VolumeHealth      []VolumeHealthMetrics     `json:"volumeHealth,omitempty"`
+	VolumeAttachments []VolumeAttachmentMetrics `json:"volumeAttachments,omitempty"`
+	AttachLimit       int32                     `json:"attachLimit,omitempty"`
+	AttachCount       int32                     `json:"attachCount"`
+	AttachErrors      uint64                    `json:"attachErrors,omitempty"`
+	DetachErrors      uint64                    `json:"detachErrors,omitempty"`
+}
+
+// VolumeSnapshotMetrics represents snapshot information for a volume
+type VolumeSnapshotMetrics struct {
+	Name         string     `json:"name"`
+	Namespace    string     `json:"namespace,omitempty"`
+	SourcePVName string     `json:"sourcePVName"`
+	CreationTime time.Time  `json:"creationTime"`
+	Size         int64      `json:"size"`
+	ReadyToUse   bool       `json:"readyToUse"`
+	RestoreSize  int64      `json:"restoreSize,omitempty"`
+	DeletionTime *time.Time `json:"deletionTime,omitempty"`
+	Error        string     `json:"error,omitempty"`
+}
+
+// VolumeExpansionMetrics represents volume expansion metrics
+type VolumeExpansionMetrics struct {
+	CurrentSize    int64      `json:"currentSize"`
+	RequestedSize  int64      `json:"requestedSize"`
+	InProgress     bool       `json:"inProgress"`
+	LastResizeTime *time.Time `json:"lastResizeTime,omitempty"`
+	ResizeStatus   string     `json:"resizeStatus,omitempty"`
+	FailureMessage string     `json:"failureMessage,omitempty"`
+}
+
+// QuotaScopeMetrics represents quota scope information
+type QuotaScopeMetrics struct {
+	ScopeName   string            `json:"scopeName"`
+	Resources   []string          `json:"resources,omitempty"`
+	MatchLabels map[string]string `json:"matchLabels,omitempty"`
+	MatchScopes []string          `json:"matchScopes,omitempty"`
+}
+
+// PriorityClassQuotaMetrics represents quota metrics for priority classes
+type PriorityClassQuotaMetrics struct {
+	PriorityClass string            `json:"priorityClass"`
+	Hard          map[string]string `json:"hard,omitempty"`
+	Used          map[string]string `json:"used,omitempty"`
+}
+
+// QuotaStatusHistory represents historical quota status
+type QuotaStatusHistory struct {
+	Timestamp time.Time `json:"timestamp"`
+	Resource  string    `json:"resource"`
+	Hard      string    `json:"hard"`
+	Used      string    `json:"used"`
+}
+
+// CustomResourceQuota represents quotas for custom resources
+// type CustomResourceQuota struct {
+// 	Group    string `json:"group"`
+// 	Resource string `json:"resource"`
+// 	Version  string `json:"version"`
+// 	Hard     string `json:"hard,omitempty"`
+// 	Used     string `json:"used,omitempty"`
+// }
+
+// SchedStats represents scheduler statistics
+type SchedStats struct {
+	RunQueueLength   uint64 `json:"runQueueLength"`
+	WaitingProcesses uint64 `json:"waitingProcesses"`
+	SleepingTasks    uint64 `json:"sleepingTasks"`
+}
+
+// PSIMetrics represents metrics for the container runtime
+type PSIMetrics struct {
+	CPU struct {
+		Some float64 `json:"some"`
+		Full float64 `json:"full"`
+	} `json:"cpu"`
+	Memory struct {
+		Some float64 `json:"some"`
+		Full float64 `json:"full"`
+	} `json:"memory"`
+	IO struct {
+		Some float64 `json:"some"`
+		Full float64 `json:"full"`
+	} `json:"io"`
 }
diff --git a/pkg/utils/k8sconfig.go b/pkg/utils/k8sconfig.go
index b1ec2b9..538f82f 100644
--- a/pkg/utils/k8sconfig.go
+++ b/pkg/utils/k8sconfig.go
@@ -15,9 +15,13 @@ import (
 	"fmt"
 	"os"
 	"path/filepath"
+	"strings"
+	"sync"
 
 	"github.com/sirupsen/logrus"
 	"github.com/vegacloud/kubernetes/metricsagent/pkg/config"
+	authenticationv1 "k8s.io/api/authentication/v1"
+	authorizationv1 "k8s.io/api/authorization/v1"
 	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
 	"k8s.io/client-go/kubernetes"
 	"k8s.io/client-go/rest"
@@ -35,18 +39,91 @@ type K8sClientConfig struct {
 	ClusterVersion     string
 }
 
-// GetClientConfig returns a Kubernetes ClientConfig using in-cluster or kubeconfig
-func GetClientConfig(ctx context.Context, cfg *config.Config) (*K8sClientConfig, error) {
-	logrus.Debug("GetClientConfig: Attempting to get Kubernetes client configuration")
+var (
+	instance *K8sClientConfig
+	once     sync.Once
+)
 
-	inClusterConfig, err := rest.InClusterConfig()
+// logCurrentIdentity attempts to get and log the current service account identity
+func logCurrentIdentity(ctx context.Context, clientset *kubernetes.Clientset) {
+	logger := logrus.WithField("function", "logCurrentIdentity")
+
+	// Try to get self review
+	selfReview := &authenticationv1.SelfSubjectReview{}
+	result, err := clientset.AuthenticationV1().SelfSubjectReviews().Create(ctx, selfReview, metav1.CreateOptions{})
 	if err != nil {
-		logrus.Debug("GetClientConfig: Not in cluster, attempting to use kubeconfig")
-		return getOutOfClusterConfig(ctx, cfg)
+		logger.WithError(err).Error("Failed to get self subject review")
+		return
 	}
 
-	logrus.Info("GetClientConfig: Using in-cluster configuration")
-	return createClientConfig(ctx, inClusterConfig, true, cfg)
+	logger.WithFields(logrus.Fields{
+		"username": result.Status.UserInfo.Username,
+		"uid":      result.Status.UserInfo.UID,
+		"groups":   result.Status.UserInfo.Groups,
+	}).Info("Current identity")
+
+	// Try to get self access review
+	accessReview := &authorizationv1.SelfSubjectRulesReview{
+		Spec: authorizationv1.SelfSubjectRulesReviewSpec{
+			Namespace: "default",
+		},
+	}
+	accessResult, err := clientset.AuthorizationV1().SelfSubjectRulesReviews().Create(ctx, accessReview, metav1.CreateOptions{})
+	if err != nil {
+		logger.WithError(err).Error("Failed to get self subject rules review")
+		return
+	}
+
+	logger.WithField("rules", accessResult.Status.ResourceRules).Debug("Current permissions")
+}
+
+// GetClientConfig returns a Kubernetes ClientConfig using in-cluster or kubeconfig
+func GetClientConfig(ctx context.Context, cfg *config.Config) (*K8sClientConfig, error) {
+	var err error
+	once.Do(func() {
+		logrus.Debug("GetClientConfig: Attempting to get Kubernetes client configuration")
+
+		inClusterConfig, inClusterErr := rest.InClusterConfig()
+		if inClusterErr != nil {
+			logrus.Debug("GetClientConfig: Not in cluster, attempting to use kubeconfig")
+			instance, err = getOutOfClusterConfig(ctx, cfg)
+			return
+		}
+
+		// Explicitly set the bearer token from the specified path
+		token, tokenErr := os.ReadFile(cfg.VegaBearerTokenPath)
+		if tokenErr != nil {
+			err = fmt.Errorf("failed to read service account token from %s: %w", cfg.VegaBearerTokenPath, tokenErr)
+			return
+		}
+
+		// Override the token in the config
+		inClusterConfig.BearerToken = string(token)
+		inClusterConfig.BearerTokenFile = cfg.VegaBearerTokenPath
+
+		logrus.WithFields(logrus.Fields{
+			"token_path": cfg.VegaBearerTokenPath,
+			"token_len":  len(string(token)),
+		}).Debug("Using explicit service account token")
+
+		instance, err = createClientConfig(ctx, inClusterConfig, true, cfg)
+		if err != nil {
+			return
+		}
+
+		// Log the current identity
+		logCurrentIdentity(ctx, instance.Clientset)
+	})
+
+	return instance, nil
+}
+
+// GetExistingClientConfig returns the existing client config without creating a new one
+func GetExistingClientConfig() (*K8sClientConfig, error) {
+	if instance == nil {
+		return nil, fmt.Errorf("kubernetes client configuration has not been initialized")
+	}
+	return instance, nil
 }
 
 func getOutOfClusterConfig(ctx context.Context, cfg *config.Config) (*K8sClientConfig, error) {
@@ -108,6 +185,7 @@ func createClientConfig(
 }
 
 func enrichClientConfig(ctx context.Context, config *K8sClientConfig) error {
+
 	var err error
 
 	// Get cluster UID
@@ -127,6 +205,15 @@ func enrichClientConfig(ctx context.Context, config *K8sClientConfig) error {
 
 // getNamespaceUID retrieves the UID of a given namespace
 func getNamespaceUID(ctx context.Context, clientset *kubernetes.Clientset, namespace string) (string, error) {
+	logrus.Debugf("Attempting to get namespace %s", namespace)
+
+	_, err := os.ReadFile("/var/run/secrets/kubernetes.io/serviceaccount/token")
+	if err != nil {
+		logrus.Debugf("Error reading service account token: %v", err)
+	} else {
+		logrus.Debugf("Service account token exists and is readable")
+	}
+
 	ns, err := clientset.CoreV1().Namespaces().Get(ctx, namespace, metav1.GetOptions{})
 	if err != nil {
 		return "", fmt.Errorf("getNamespaceUID: failed to get namespace %s: %v", namespace, err)
@@ -142,3 +229,28 @@ func getClusterVersion(clientset *kubernetes.Clientset) (string, error) {
 	}
 	return version.String(), nil
 }
+
+// VerifyClientIdentity checks if the client is using the expected service account
+func VerifyClientIdentity(ctx context.Context, clientset *kubernetes.Clientset, expectedNamespace string) error {
+	logger := logrus.WithField("function", "VerifyClientIdentity")
+
+	selfReview := &authenticationv1.SelfSubjectReview{}
+	result, err := clientset.AuthenticationV1().SelfSubjectReviews().Create(ctx, selfReview, metav1.CreateOptions{})
+	if err != nil {
+		return fmt.Errorf("failed to get self subject review: %w", err)
+	}
+
+	// Log the identity information
+	logger.WithFields(logrus.Fields{
+		"username": result.Status.UserInfo.Username,
+		"uid":      result.Status.UserInfo.UID,
+		"groups":   result.Status.UserInfo.Groups,
+	}).Info("Current client identity")
+
+	// Verify the service account namespace
+	if !strings.Contains(result.Status.UserInfo.Username, expectedNamespace) {
+		return fmt.Errorf("client is not using the expected service account in namespace %s", expectedNamespace)
+	}
+
+	return nil
+}
diff --git a/pkg/utils/safememory.go b/pkg/utils/safememory.go
new file mode 100644
index 0000000..0f49cd5
--- /dev/null
+++ b/pkg/utils/safememory.go
@@ -0,0 +1,45 @@
+// Package utils provides utility functions for the metrics agent
+package utils
+
+import "github.com/sirupsen/logrus"
+
+// SafeGPUMemory converts GPU memory value to bytes safely
+func SafeGPUMemory(value int64) uint64 {
+	const (
+		GB        = uint64(1024 * 1024 * 1024)
+		maxUint64 = ^uint64(0)
+	)
+
+	// First convert the value to uint64 safely
+	if value < 0 {
+		logrus.Warnf("Negative GPU memory value %d, using 0", value)
+		return 0
+	}
+
+	uvalue := uint64(value)
+
+	// Check if multiplication would overflow
+	if uvalue > maxUint64/GB {
+		logrus.Warnf("GPU memory value %d would overflow, capping at maximum", value)
+		return maxUint64
+	}
+
+	return uvalue * GB
+}
+
+// SafeInt32Conversion converts int to int32 safely
+func SafeInt32Conversion(value int) int32 {
+	const maxInt32 = 1<<31 - 1
+	const minInt32 = -1 << 31
+
+	if value > maxInt32 {
+		logrus.Warnf("Value %d exceeds maximum int32, capping at maximum", value)
+		return maxInt32
+	}
+	if value < minInt32 {
+		logrus.Warnf("Value %d is below minimum int32, capping at minimum", value)
+		return minInt32
+	}
+
+	return int32(value)
+}
diff --git a/test/resources/buildkindtestcluster.sh b/test/resources/buildkindtestcluster.sh
new file mode 100644
index 0000000..1a06da1
--- /dev/null
+++ b/test/resources/buildkindtestcluster.sh
@@ -0,0 +1,6 @@
+curl -Lo ./kind https://kind.sigs.k8s.io/dl/latest/kind-linux-amd64
+chmod +x ./kind
+./kind create cluster --name test-cluster --config kind-config.yaml --image=kindest/node:v1.30.0
+echo "Sleeping 20 seconds to let things settle down)
+sleep(20)
+kubectl apply -f ./test-resources.yaml
diff --git a/test/resources/kind-config.yaml b/test/resources/kind-config.yaml
new file mode 100644
index 0000000..f7620b8
--- /dev/null
+++ b/test/resources/kind-config.yaml
@@ -0,0 +1,8 @@
+# kind-config.yaml
+kind: Cluster
+apiVersion: kind.x-k8s.io/v1alpha4
+nodes:
+  - role: control-plane
+  - role: worker
+  - role: worker
+  - role: worker
diff --git a/test/resources/test-resources.yaml b/test/resources/test-resources.yaml
new file mode 100644
index 0000000..250d8f7
--- /dev/null
+++ b/test/resources/test-resources.yaml
@@ -0,0 +1,395 @@
+---
+apiVersion: v1
+kind: Namespace
+metadata:
+  name: test-metrics
+  labels:
+    environment: test
+    purpose: metrics-collection
+---
+# A ResourceQuota to test namespace-level quota metrics
+apiVersion: v1
+kind: ResourceQuota
+metadata:
+  name: test-quota
+  namespace: test-metrics
+spec:
+  hard:
+    pods: "10"
+    requests.cpu: "2"
+    requests.memory: "1Gi"
+    limits.cpu: "4"
+    limits.memory: "2Gi"
+---
+# A LimitRange to test namespace-level limit metrics
+apiVersion: v1
+kind: LimitRange
+metadata:
+  name: test-limitrange
+  namespace: test-metrics
+spec:
+  limits:
+    - type: Container
+      defaultRequest:
+        cpu: "100m"
+        memory: "128Mi"
+      default:
+        cpu: "200m"
+        memory: "256Mi"
+      max:
+        cpu: "500m"
+        memory: "512Mi"
+---
+# A ConfigMap to test presence of config-related metrics (labels/annotations, etc.)
+apiVersion: v1
+kind: ConfigMap
+metadata:
+  name: test-config
+  namespace: test-metrics
+  labels:
+    app: test-app
+data:
+  example-key: "example-value"
+---
+# A StorageClass to test storage metrics
+apiVersion: storage.k8s.io/v1
+kind: StorageClass
+metadata:
+  name: test-storageclass
+  labels:
+    environment: test
+provisioner: kubernetes.io/no-provisioner
+volumeBindingMode: WaitForFirstConsumer
+---
+# A PersistentVolume for testing PV metrics
+apiVersion: v1
+kind: PersistentVolume
+metadata:
+  name: test-pv
+  labels:
+    environment: test
+spec:
+  capacity:
+    storage: 1Gi
+  accessModes:
+    - ReadWriteOnce
+  persistentVolumeReclaimPolicy: Retain
+  storageClassName: test-storageclass
+  hostPath:
+    path: /tmp/test-pv
+---
+# A PersistentVolumeClaim for testing PVC metrics
+apiVersion: v1
+kind: PersistentVolumeClaim
+metadata:
+  name: test-pvc
+  namespace: test-metrics
+  labels:
+    environment: test
+spec:
+  accessModes:
+    - ReadWriteOnce
+  storageClassName: test-storageclass
+  resources:
+    requests:
+      storage: 500Mi
+---
+# A simple Pod that uses the PVC, has resource requests/limits, annotations, ephemeral containers, etc.
+apiVersion: v1
+kind: Pod
+metadata:
+  name: test-pod
+  namespace: test-metrics
+  labels:
+    app: test-pod
+  annotations:
+    test-annotation: "true"
+spec:
+  readinessGates:
+    - conditionType: "test.scheduling.k8s.io/ready" # non-existent, simulating conditions
+  securityContext:
+    runAsUser: 1000
+  containers:
+    - name: main-container
+      image: nginx:stable
+      resources:
+        requests:
+          cpu: "100m"
+          memory: "128Mi"
+        limits:
+          cpu: "200m"
+          memory: "256Mi"
+      volumeMounts:
+        - name: test-volume
+          mountPath: /data
+  initContainers:
+    - name: init-test
+      image: busybox
+      command: ["sh", "-c", "echo 'init done'"]
+  volumes:
+    - name: test-volume
+      persistentVolumeClaim:
+        claimName: test-pvc
+  # Ephemeral containers (requires feature gate + kubectl debug)
+  # Just an example; may not run as-is.
+  ephemeralContainers:
+    - name: debugger
+      image: busybox
+      command: ["sh", "-c", "sleep 3600"]
+      stdin: true
+      tty: true
+---
+# A Deployment for testing deployment metrics
+apiVersion: apps/v1
+kind: Deployment
+metadata:
+  name: test-deployment
+  namespace: test-metrics
+  labels:
+    app: test-deployment
+spec:
+  replicas: 2
+  selector:
+    matchLabels:
+      app: test-deployment
+  template:
+    metadata:
+      labels:
+        app: test-deployment
+    spec:
+      containers:
+        - name: web
+          image: nginx:stable
+          resources:
+            requests:
+              cpu: "100m"
+              memory: "128Mi"
+---
+# A DaemonSet to test daemonset metrics
+apiVersion: apps/v1
+kind: DaemonSet
+metadata:
+  name: test-daemonset
+  namespace: test-metrics
+  labels:
+    app: test-daemonset
+spec:
+  selector:
+    matchLabels:
+      app: test-daemonset
+  template:
+    metadata:
+      labels:
+        app: test-daemonset
+    spec:
+      containers:
+        - name: daemon
+          image: nginx:stable
+---
+# A StatefulSet to test statefulset metrics
+apiVersion: apps/v1
+kind: StatefulSet
+metadata:
+  name: test-statefulset
+  namespace: test-metrics
+  labels:
+    app: test-statefulset
+spec:
+  serviceName: "test-statefulset-service"
+  replicas: 1
+  selector:
+    matchLabels:
+      app: test-statefulset
+  template:
+    metadata:
+      labels:
+        app: test-statefulset
+    spec:
+      containers:
+        - name: stateful
+          image: nginx:stable
+---
+# A Job to test job metrics
+apiVersion: batch/v1
+kind: Job
+metadata:
+  name: test-job
+  namespace: test-metrics
+  labels:
+    app: test-job
+spec:
+  completions: 1
+  parallelism: 1
+  template:
+    metadata:
+      labels:
+        app: test-job
+    spec:
+      restartPolicy: Never
+      containers:
+        - name: job
+          image: busybox
+          command: ["sh", "-c", "echo Job Completed; sleep 2"]
+---
+# A CronJob to test cronjob metrics
+apiVersion: batch/v1
+kind: CronJob
+metadata:
+  name: test-cronjob
+  namespace: test-metrics
+  labels:
+    app: test-cronjob
+spec:
+  schedule: "*/5 * * * *"
+  jobTemplate:
+    spec:
+      template:
+        metadata:
+          labels:
+            app: test-cronjob
+        spec:
+          restartPolicy: Never
+          containers:
+            - name: cron
+              image: busybox
+              command: ["sh", "-c", "echo CronJob Run; sleep 1"]
+---
+# A ReplicationController for replicationcontroller metrics
+apiVersion: v1
+kind: ReplicationController
+metadata:
+  name: test-rc
+  namespace: test-metrics
+  labels:
+    app: test-rc
+spec:
+  replicas: 1
+  selector:
+    app: test-rc
+  template:
+    metadata:
+      labels:
+        app: test-rc
+    spec:
+      containers:
+        - name: rc-container
+          image: nginx:stable
+---
+# A Service to test service metrics
+apiVersion: v1
+kind: Service
+metadata:
+  name: test-service
+  namespace: test-metrics
+  labels:
+    app: test-service
+spec:
+  selector:
+    app: test-deployment
+  ports:
+    - protocol: TCP
+      port: 80
+      targetPort: 80
+---
+# An Ingress to test ingress metrics (assuming ingress controller present)
+apiVersion: networking.k8s.io/v1
+kind: Ingress
+metadata:
+  name: test-ingress
+  namespace: test-metrics
+  labels:
+    app: test-ingress
+  annotations:
+    nginx.ingress.kubernetes.io/rewrite-target: /
+spec:
+  rules:
+    - host: "test.example.com"
+      http:
+        paths:
+          - path: "/"
+            pathType: Prefix
+            backend:
+              service:
+                name: test-service
+                port:
+                  number: 80
+---
+# A NetworkPolicy to test network policy metrics
+apiVersion: networking.k8s.io/v1
+kind: NetworkPolicy
+metadata:
+  name: test-networkpolicy
+  namespace: test-metrics
+  labels:
+    app: test-networkpolicy
+spec:
+  podSelector:
+    matchLabels:
+      app: test-pod
+  policyTypes:
+    - Ingress
+  ingress:
+    - from:
+        - namespaceSelector:
+            matchLabels:
+              environment: test
+      ports:
+        - protocol: TCP
+          port: 80
+---
+# A HorizontalPodAutoscaler to test autoscaling metrics
+apiVersion: autoscaling/v2
+kind: HorizontalPodAutoscaler
+metadata:
+  name: test-hpa
+  namespace: test-metrics
+  labels:
+    app: test-hpa
+spec:
+  scaleTargetRef:
+    apiVersion: apps/v1
+    kind: Deployment
+    name: test-deployment
+  minReplicas: 1
+  maxReplicas: 5
+  metrics:
+    - type: Resource
+      resource:
+        name: cpu
+        target:
+          type: Utilization
+          averageUtilization: 50
+---
+# A PodDisruptionBudget to test PDB metrics
+apiVersion: policy/v1
+kind: PodDisruptionBudget
+metadata:
+  name: test-pdb
+  namespace: test-metrics
+  labels:
+    app: test-pdb
+spec:
+  minAvailable: 1
+  selector:
+    matchLabels:
+      app: test-deployment
+---
+# If VolumeSnapshots are supported, test volume snapshot metrics
+# Requires CRDs from snapshot.storage.k8s.io
+# Uncomment if you have VolumeSnapshot feature enabled.
+# apiVersion: snapshot.storage.k8s.io/v1
+# kind: VolumeSnapshotClass
+# metadata:
+#   name: test-volumesnapshotclass
+# driver: hostpath.csi.k8s.io
+# deletionPolicy: Retain
+#
+# apiVersion: snapshot.storage.k8s.io/v1
+# kind: VolumeSnapshot
+# metadata:
+#   name: test-volumesnapshot
+#   namespace: test-metrics
+# spec:
+#   source:
+#     persistentVolumeClaimName: test-pvc
+#   volumeSnapshotClassName: test-volumesnapshotclass
